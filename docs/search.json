[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Multilinear Algebra",
    "section": "",
    "text": "Part I. Orientation & Motivation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-1.-what-is-multilinear",
    "href": "index.html#chapter-1.-what-is-multilinear",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 1. What is Multilinear?",
    "text": "Chapter 1. What is Multilinear?\n\n1.1 Linear vs. Multilinear: From Lines to Volumes\nWhen you first meet linear algebra, the word “linear” carries a specific flavor: we study maps that preserve straightness and scaling. A linear map \\(f: V \\to W\\) satisfies\n\\[\nf(a v_1 + b v_2) = a f(v_1) + b f(v_2),\n\\]\nfor scalars \\(a, b\\) and vectors \\(v_1, v_2 \\in V\\). The picture is of functions that take lines to lines, planes to planes, preserving the essential structure of addition and scaling.\n\nFrom Linear to Multilinear\nA multilinear map involves several vector inputs at once. For instance, a bilinear map takes two vectors:\n\\[\nB : V \\times W \\to \\mathbb{R}, \\quad B(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w),\n\\]\nand is linear in each argument separately. More generally, a \\(k\\)-linear map eats \\(k\\) vectors-each input behaves linearly while the others are held fixed.\n\n\nGeometry of the Shift\n\nLinear (1-input): Think of scaling a line, stretching it along one axis.\nBilinear (2-input): Now imagine two directions at once. The determinant of a \\(2 \\times 2\\) matrix is bilinear: it measures the signed *area- spanned by two vectors.\nTrilinear (3-input): With three inputs, multilinearity measures a volume. The scalar triple product \\(u \\cdot (v \\times w)\\) is trilinear, giving the volume of the parallelepiped formed by \\(u, v, w\\).\nHigher multilinearity: Beyond three inputs, we can measure 4D hyper-volumes and higher-dimensional “content.”\n\n\n\nIntuition: From 1D to Higher Dimensions\n\n1D (linear): One direction → length.\n2D (bilinear): Two directions → area.\n3D (trilinear): Three directions → volume.\nkD (multilinear): \\(k\\) directions → hyper-volume.\n\nEach new input adds a new dimension of measurement. In this sense, multilinear algebra is the natural extension of linear algebra: instead of studying transformations of single vectors, we study functions that combine several vectors in structured ways.\n\n\nEveryday Examples\n\nDot product: Bilinear form producing a scalar from two vectors.\nMatrix multiplication: Bilinear in its row and column inputs.\nDeterminant: Multilinear in its columns (or rows), encoding volume.\nCross product: Bilinear but antisymmetric, giving a vector orthogonal to two inputs.\nNeural networks: Convolutions and tensor contractions are deeply multilinear operations, reshaped for computation.\n\nLinear algebra captures what happens when you act on one direction at a time. Multilinear algebra generalizes this, letting us measure, transform, and compute with several directions simultaneously. It is the leap from lines to volumes, from single-vector transformations to the rich geometry of many interacting vectors.\n\n\nExercises\n\nLinearity Check: Let \\(f:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) be defined by \\(f(x,y) = (2x, 3y)\\). Show that \\(f\\) is linear.\nBilinear Dot Product: Verify that the dot product \\(\\langle u, v \\rangle = u_1 v_1 + u_2 v_2 + u_3 v_3\\) is bilinear by checking linearity in each argument separately.\nArea via Determinant: For vectors \\(u = (1,0)\\) and \\(v = (1,2)\\) in \\(\\mathbb{R}^2\\), compute the determinant of the \\(2 \\times 2\\) matrix with columns \\(u\\) and \\(v\\). Interpret the result geometrically.\nVolume via Scalar Triple Product: Compute the scalar triple product \\(u \\cdot (v \\times w)\\) for \\(u = (1,0,0)\\), \\(v = (0,1,0)\\), and \\(w = (0,0,1)\\). Explain why the result makes sense in terms of volume.\nGeneralization Thought Experiment: Imagine you have four independent vectors in \\(\\mathbb{R}^4\\). What kind of geometric quantity does a multilinear map on these four inputs measure? (Hint: think of the analogy with length, area, and volume.)\n\n\n\n\n1.2 Three Faces of a Tensor: Array, Map, and Element of a Product Space\nA tensor can feel slippery at first, because it wears different “faces” depending on how you meet it. Each face is valid and useful. Together they form the three standard viewpoints:\n\n1. Array View: Numbers in a Box\nAt the simplest level, a tensor looks like a multidimensional array of numbers.\n\nA vector is a 1-dimensional array: \\([v_i]\\).\nA matrix is a 2-dimensional array: \\([a_{ij}]\\).\nA general tensor of order \\(k\\) is like a \\(k\\)-dimensional grid of numbers: \\([t_{i_1 i_2 \\dots i_k}]\\).\n\nThis viewpoint is intuitive for computation, storage, and indexing. For example, in machine learning an image is a 3rd-order tensor: height × width × color channels.\n\n\n2. Map View: Multilinear Functions\nAnother way to see tensors is as multilinear maps.\n\nA bilinear form takes two vectors and outputs a number, e.g., the dot product.\nA trilinear form takes three vectors and outputs a number, e.g., the scalar triple product.\nIn general, a tensor \\(T\\) of type \\((0,k)\\) is a map\n\\[\nT: V \\times V \\times \\cdots \\times V \\to \\mathbb{R},\n\\]\nlinear in each slot separately.\n\nThis viewpoint highlights behavior: how the tensor interacts with vectors. It is central in geometry and physics, where tensors encode measurable relationships.\n\n\n3. Product Space View: Elements of \\(V \\otimes W \\otimes \\cdots\\)\nThe third viewpoint places tensors as elements of tensor product spaces.\n\nGiven vector spaces \\(V\\) and \\(W\\), their tensor product \\(V \\otimes W\\) is a new space built to capture bilinear maps.\nA simple (decomposable) tensor looks like \\(v \\otimes w\\).\nGeneral tensors are linear combinations of these simple pieces.\n\nThis is the most abstract but also the most powerful perspective. It makes precise statements about dimension, bases, and transformation laws. It also unifies the array and map viewpoints: the array entries are just the coordinates of a tensor element with respect to a basis, and the map behavior is encoded in how the element acts under evaluation.\n\n\nReconciling the Views\n\nArray: concrete for computation.\nMap: functional and geometric meaning.\nProduct space: rigorous foundation.\n\nA beginner should be comfortable switching between them: “the same object, three different languages.”\n\n\nExercises\n\nArray Identification: Write down a 3rd-order tensor \\(T\\) with entries \\(t_{ijk}\\) where \\(i,j,k \\in \\{1,2\\}\\). How many numbers are needed to specify \\(T\\)?\nDot Product as a Tensor: Show that the dot product on \\(\\mathbb{R}^3\\) can be viewed both as a bilinear map and as an array (matrix).\nSimple Tensor Construction: Given \\(u=(1,2)\\in \\mathbb{R}^2\\) and \\(v=(3,4)\\in \\mathbb{R}^2\\), form the tensor \\(u \\otimes v\\). Write its coordinates as a \\(2 \\times 2\\) array.\nSwitching Perspectives: Consider the determinant of a \\(2\\times2\\) matrix: \\(\\det([a_{ij}]) = a_{11}a_{22} - a_{12}a_{21}\\). Explain how this determinant can be seen as a bilinear map of the two column vectors of the matrix.\nThought Experiment: Suppose you store an RGB image of size \\(100 \\times 200\\).\n\nIn the array view, what is the order (number of indices) of the tensor representing the image?\nIn the product space view, which vector spaces might this tensor live in?\n\n\n\n\n\n1.3 Why It Matters: Graphics, Physics, ML, Data Compression\nTensors may sound abstract, but they appear everywhere in modern science and technology. Understanding *why- they matter helps motivate the study of multilinear algebra.\n\nIn Computer Graphics\n\nTransformations: 3D graphics use matrices (2nd-order tensors) to rotate, scale, and project objects.\nLighting Models: Many shading equations combine vectors of light, normal directions, and material properties in bilinear or trilinear ways.\nAnimation: Deformations of 3D models often rely on multilinear blending of control parameters.\n\nTensors let us express geometry and transformations in compact formulas that computers can process efficiently.\n\n\nIn Physics and Engineering\n\nStress and Strain: The stress tensor (2nd-order) relates internal forces to orientations in a material. The elasticity tensor (4th-order) relates stress and strain in solids.\nElectromagnetism: The electromagnetic field tensor encodes electric and magnetic fields in a relativistic framework.\nInertia Tensor: Describes how an object resists rotational acceleration depending on its mass distribution.\n\nTensors naturally capture “laws that hold in every direction,” which is why they dominate in mechanics and field theories.\n\n\nIn Machine Learning\n\nNeural Networks: Input data (images, videos, audio) are tensors of order 3, 4, or higher.\nConvolutions: The convolution kernel is a small tensor sliding across a larger tensor, producing a new one.\nAttention Mechanisms: Core operations in transformers are tensor contractions that combine multiple input arrays.\n\nUnderstanding tensor decompositions helps compress models, speed up computation, and reveal hidden structures in data.\n\n\nIn Data Compression and Signal Processing\n\nPCA and SVD: Classic matrix decompositions are 2D tensor methods.\nTensor Decompositions (CP, Tucker, TT): These extend compression ideas to multidimensional data, useful for video, hyperspectral imaging, and big-data analysis.\nMultiway Data Analysis: Tensors allow us to uncover patterns across several “modes” simultaneously-like user × time × product in recommendation systems.\n\n\n\nThe Big Picture\nLinear algebra lets us describe single-direction transformations (lines). Multilinear algebra extends this to multiple interacting directions (areas, volumes, hyper-volumes). These structures appear whenever we handle multi-dimensional data or physical laws.\nLearning to *think tensorially- is the key to navigating modern applied mathematics, science, and AI.\n\n\nExercises\n\nGraphics Example: A 3D rotation matrix is a 2nd-order tensor. Explain why it is linear in its input vector but not multilinear.\nPhysics Example: The stress tensor \\(\\sigma\\) maps a direction vector \\(n\\) to a force vector \\(\\sigma n\\). Why is this operation linear in \\(n\\)?\nMachine Learning Example: An RGB image of size \\(32 \\times 32\\) has 3 color channels.\n\nWhat is the order of the tensor representing this image?\nHow many entries does it have in total?\n\nData Compression Example: PCA reduces a data matrix (2nd-order tensor) to a low-rank approximation. Suggest what a “low-rank” tensor decomposition might achieve for video data (3rd-order tensor: frame × width × height).\nThought Experiment: Suppose you could only use vectors and matrices, not higher-order tensors. Which of the following applications would be impossible or very awkward:\n\nRepresenting the interaction of three forces at once.\nCompressing a color video.\nEncoding the stress-strain relationship in 3D materials. ### 1.4 A First Walk-Through: Color Images and 3-Way Arrays\n\n\nLet’s make tensors concrete with an everyday example: digital images.\n\n\nFrom Grayscale to Color\n\nA grayscale image of size \\(100 \\times 200\\) can be seen as a matrix (2nd-order tensor). Each entry stores the brightness of a pixel.\nA color image has three channels: red, green, and blue (RGB). Now every pixel carries three values. This naturally forms a 3rd-order tensor:\n\\[\nI \\in \\mathbb{R}^{100 \\times 200 \\times 3}.\n\\]\nThe three indices correspond to row, column, color channel.\n\n\n\nThe Three Index Roles\n\nRow (height): vertical position in the image.\nColumn (width): horizontal position in the image.\nChannel: one of the RGB color intensities.\n\nTogether, \\((i,j,k)\\) points to a single number: the intensity of color \\(k\\) at pixel \\((i,j)\\).\n\n\nOperations as Tensor Manipulations\n\nFlattening: We can reshape the 3D tensor into a 2D matrix, useful for feeding into algorithms that expect vectors or matrices.\nContraction (summing over an index): If we sum over the color channel, we turn an RGB image into a grayscale image.\nOuter products: A colored checkerboard pattern can be constructed by taking tensor products of row and column vectors, then adding a color channel vector.\n\n\n\nWhy This Example Matters\n\nIt shows how natural data can have more than two indices.\nIt illustrates why matrices (2D tensors) are not enough for modern problems.\nIt connects tensor operations with practical tasks: filtering, compression, feature extraction.\n\nIn fact, video data adds one more index: time. A video is a 4th-order tensor: frame × height × width × channel.\n\n\nExercises\n\nCounting Entries: How many numbers are required to store a color image of size \\(64 \\times 64\\)?\nSlicing: For a color image tensor \\(I \\in \\mathbb{R}^{100 \\times 200 \\times 3}\\), what is the shape of:\n\na single row across all columns and channels?\na single color channel across all pixels?\n\nFlattening Practice: A \\(32 \\times 32 \\times 3\\) image is flattened into a vector. What is the length of this vector?\nGrayscale Conversion: Define a grayscale image \\(G(i,j)\\) from a color image tensor \\(I(i,j,k)\\) by averaging across channels:\n\\[\nG(i,j) = \\frac{1}{3} \\sum_{k=1}^3 I(i,j,k).\n\\]\nWhy is this operation an example of contraction?\nVideo as Tensor: Suppose you have a 10-second video at 30 frames per second, each frame \\(128 \\times 128\\) with 3 color channels.\n\nWhat is the order of the video tensor?\nHow many entries does it contain in total?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-2.-minimal-prerequisites",
    "href": "index.html#chapter-2.-minimal-prerequisites",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 2. Minimal Prerequisites",
    "text": "Chapter 2. Minimal Prerequisites\n\n2.1 Vector Spaces, Bases, Dimension\nBefore diving deeper into multilinear algebra, we need a short refresher on the basic building blocks of linear algebra: vector spaces.\n\nWhat is a Vector Space?\nA vector space is a collection of objects (called vectors) that can be:\n\nAdded: \\(u + v\\) is again a vector.\nScaled: \\(a v\\) (where \\(a\\) is a scalar) is again a vector.\n\nThe rules of addition and scaling follow natural laws: associativity, commutativity, distributivity, and the existence of a zero vector.\nExamples:\n\n\\(\\mathbb{R}^n\\): all \\(n\\)-tuples of real numbers.\nPolynomials of degree ≤ \\(d\\).\nContinuous functions on an interval.\n\n\n\nBases and Coordinates\nA basis of a vector space is a set of vectors that:\n\nAre linearly independent (no one is a linear combination of the others).\nSpan the entire space (every vector can be expressed as a linear combination of them).\n\nFor \\(\\mathbb{R}^3\\), the standard basis is:\n\\[\ne_1 = (1,0,0), \\quad e_2 = (0,1,0), \\quad e_3 = (0,0,1).\n\\]\nEvery vector \\(v \\in \\mathbb{R}^3\\) can be uniquely written as:\n\\[\nv = x e_1 + y e_2 + z e_3,\n\\]\nwith coordinates \\((x,y,z)\\).\n\n\nDimension\nThe dimension of a vector space is the number of vectors in any basis.\n\n\\(\\mathbb{R}^n\\) has dimension \\(n\\).\nPolynomials of degree ≤ \\(d\\) form a vector space of dimension \\(d+1\\).\nA trivial space \\(\\{0\\}\\) has dimension 0.\n\nDimension gives the “number of independent directions” in the space.\n\n\nWhy This Matters for Multilinear Algebra\n\nTensors live in spaces built from vector spaces (tensor products).\nUnderstanding bases and dimensions is crucial for counting entries of tensors.\nCoordinates provide the link between abstract definitions and concrete arrays.\n\n\n\nExercises\n\nChecking Vector Spaces: Decide whether each of the following is a vector space over \\(\\mathbb{R}\\):\n\n\nAll \\(2 \\times 2\\) real matrices.\n\n\nAll positive real numbers.\n\n\nAll polynomials with real coefficients.\n\n\nBasis in \\(\\mathbb{R}^2\\): Show that \\((1,1)\\) and \\((1,-1)\\) form a basis for \\(\\mathbb{R}^2\\). Express the vector \\((3,2)\\) in this basis.\nCounting Dimension: What is the dimension of the space of all real polynomials of degree ≤ 4? Suggest a natural basis.\nUniqueness of Representation: In \\(\\mathbb{R}^3\\), write \\((2,3,5)\\) as a combination of \\(e_1, e_2, e_3\\). Why is this representation unique?\nApplication to Tensors: If \\(V = \\mathbb{R}^2\\) and \\(W = \\mathbb{R}^3\\), what is the dimension of the product space \\(V \\otimes W\\)?\n\n\n\n\n2.2 Linear Maps, Matrices, Change of Basis\nHaving reviewed vector spaces, we now turn to linear maps-the main actors in linear algebra.\n\nLinear Maps\nA linear map \\(T: V \\to W\\) between vector spaces satisfies:\n\\[\nT(av + bw) = aT(v) + bT(w),\n\\]\nfor all scalars \\(a,b\\) and vectors \\(v,w \\in V\\).\nExamples:\n\nScaling: \\(T(x) = 3x\\).\nRotation: \\(T:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) rotates vectors by 90°.\nDerivative: \\(D: P_3 \\to P_2\\) (maps a polynomial of degree ≤ 3 to its derivative).\n\nLinear maps preserve the structure of vector spaces.\n\n\nMatrices as Representations\nGiven bases of \\(V\\) and \\(W\\), a linear map \\(T: V \\to W\\) can be represented by a matrix.\n\nColumns of the matrix are just the images of the basis vectors of \\(V\\).\nIf \\(T(e_i) = \\sum_j a_{ji} f_j\\), then the matrix entries are \\(a_{ji}\\).\n\nThus, matrices are coordinate-based representations of abstract linear maps.\n\n\nComposition and Matrix Multiplication\n\nComposing linear maps corresponds to multiplying their matrices.\nThe identity map corresponds to the identity matrix.\nInverse maps correspond to inverse matrices (when they exist).\n\nThis makes linear maps concrete and computable.\n\n\nChange of Basis\nSuppose we change basis in a vector space \\(V\\):\n\nOld basis: \\(\\{e_1, \\dots, e_n\\}\\).\nNew basis: \\(\\{e'_1, \\dots, e'_n\\}\\).\n\nThe change-of-basis matrix \\(P\\) expresses each new basis vector as a combination of the old ones.\nFor a linear operator \\(T: V \\to V\\):\n\\[\n[T]_{new} = P^{-1} [T]_{old} P.\n\\]\nThis formula is fundamental for tensors, since tensors must transform consistently under basis changes.\n\n\nWhy This Matters for Multilinear Algebra\n\nLinear maps are 2nd-order tensors.\nUnderstanding how matrices change under new coordinates sets the stage for how higher-order tensors transform.\nThe concept of basis change ensures that tensors encode *intrinsic- information, not just numbers in an array.\n\n\n\n\nExercises\n\nLinear or Not? Decide whether each map is linear:\n\n\n\\(T(x,y) = (2x,3y)\\).\n\n\n\\(S(x,y) = (x^2,y)\\).\n\n\n\\(R(x,y) = (y,x)\\).\n\n\nMatrix Representation: Let \\(T:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) be defined by \\(T(x,y) = (x+2y,3x+y)\\). Find the matrix of \\(T\\) with respect to the standard basis.\nComposition Practice: If \\(A = \\begin{bmatrix}1 & 2\\\\0 & 1\\end{bmatrix}\\) and \\(B = \\begin{bmatrix}0 & 1\\\\1 & 0\\end{bmatrix}\\), compute \\(AB\\). Interpret the action of \\(AB\\) as a linear map.\nChange of Basis: In \\(\\mathbb{R}^2\\), let the old basis be \\(e_1 = (1,0), e_2=(0,1)\\). The new basis is \\(e'_1=(1,1), e'_2=(1,-1)\\).\n\nFind the change-of-basis matrix \\(P\\).\nVerify that \\(P^{-1}\\) transforms coordinates back to the old basis.\n\nTensor Connection: Explain why a linear map \\(T: V \\to W\\) can be viewed as an element of \\(V^- \\otimes W\\). (Hint: it eats a vector and produces another vector, which can be encoded by pairing with covectors.)\n\n\n\n2.3 Inner Products, Dual Spaces, Adjoint\nLinear maps and vector spaces give the structure. To measure angles, lengths, and projections, we need inner products. To generalize “coordinates” beyond a chosen basis, we need dual spaces. These two ideas connect directly in multilinear algebra.\n\nInner Products\nAn inner product on a real vector space \\(V\\) is a function\n\\[\n\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\n\\]\nsatisfying:\n\nLinearity in each slot: \\(\\langle av+ bw, u\\rangle = a\\langle v,u\\rangle + b\\langle w,u\\rangle\\).\nSymmetry: \\(\\langle v, w \\rangle = \\langle w, v \\rangle\\).\nPositive definiteness: \\(\\langle v,v\\rangle \\geq 0\\), with equality only when \\(v=0\\).\n\nThis structure gives:\n\nLength: \\(\\|v\\| = \\sqrt{\\langle v,v\\rangle}\\).\nAngle: \\(\\cos\\theta = \\frac{\\langle v,w\\rangle}{\\|v\\|\\|w\\|}\\).\nOrthogonality: \\(\\langle v,w\\rangle=0\\).\n\nExample: the dot product in \\(\\mathbb{R}^n\\).\n\n\nDual Spaces\nThe dual space \\(V^*\\) is the set of all linear functionals \\(f: V \\to \\mathbb{R}\\).\n\nElements of \\(V^*\\) are called covectors.\nIf \\(V=\\mathbb{R}^n\\), then \\(V^- \\cong \\mathbb{R}^n\\), but conceptually they are different:\n\nVectors: “arrows” in space.\nCovectors: “measuring devices” that output numbers when fed a vector.\n\n\nThe dual basis:\n\nIf \\(\\{e_1,\\dots,e_n\\}\\) is a basis of \\(V\\), then there is a unique dual basis \\(\\{e^1,\\dots,e^n\\}\\) in \\(V^*\\) with\n\\[\ne^i(e_j) = \\delta^i_j.\n\\]\n\nThis duality underpins how tensor indices “live up or down” (contravariant vs. covariant).\n\n\nAdjoint of a Linear Map\nGiven a linear map \\(T: V \\to V\\) on an inner product space, the adjoint \\(T^*\\) is defined by:\n\\[\n\\langle Tv, w \\rangle = \\langle v, T^*w \\rangle \\quad \\forall v,w \\in V.\n\\]\n\nIf \\(T\\) is represented by a matrix \\(A\\) in an orthonormal basis, then \\(T^*\\) corresponds to the transpose \\(A^\\top\\).\nAdjoint maps generalize the idea of “transpose” to arbitrary inner product spaces.\n\n\n\nWhy This Matters for Multilinear Algebra\n\nInner products allow us to raise or lower indices (switch between vectors and covectors).\nDual spaces are essential for defining general tensors (mixing vectors and covectors).\nAdjoint operators appear everywhere in applications: projections, least squares, and symmetry in physical laws.\n\n\n\nExercises\n\nInner Product Verification: Show that \\(\\langle (x_1,y_1),(x_2,y_2)\\rangle = 2x_1x_2 + y_1y_2\\) defines an inner product on \\(\\mathbb{R}^2\\).\nLength and Angle: For \\(u=(1,2,2)\\) and \\(v=(2,0,1)\\) in \\(\\mathbb{R}^3\\), compute:\n\n\\(\\|u\\|\\), \\(\\|v\\|\\).\nThe cosine of the angle between them.\n\nDual Basis: Let \\(V=\\mathbb{R}^2\\) with basis \\(e_1=(1,0), e_2=(0,1)\\).\n\nWrite the dual basis \\(e^1, e^2\\).\nCompute \\(e^1(3,4)\\) and \\(e^2(3,4)\\).\n\nAdjoint Map: Let \\(T:\\mathbb{R}^2 \\to \\mathbb{R}^2\\) with matrix\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 0 & 1\\end{bmatrix}.\n\\]\nFind the matrix of \\(T^*\\) under the standard dot product.\nTensor Connection: Explain why an element of \\(V^- \\otimes V\\) can be interpreted as a matrix, and why adjointness naturally appears when working with inner products.\n\n\n\n\n2.4 Bilinear Forms and Quadratic Forms\nNow that we have inner products and dual spaces, we can introduce two important types of multilinear maps that already appear in basic linear algebra: bilinear forms and quadratic forms.\n\nBilinear Forms\nA bilinear form on a vector space \\(V\\) is a function\n\\[\nB: V \\times V \\to \\mathbb{R}\n\\]\nthat is linear in each argument separately:\n\n\\(B(av_1 + bv_2, w) = aB(v_1, w) + bB(v_2, w)\\),\n\\(B(v, aw_1 + bw_2) = aB(v, w_1) + bB(v, w_2)\\).\n\nExamples:\n\nDot product: \\(\\langle v,w\\rangle\\) is symmetric and bilinear.\nMatrix form: Any matrix \\(A\\) defines a bilinear form by\n\\[\nB(v,w) = v^\\top A w.\n\\]\n\nProperties:\n\nSymmetric: if \\(B(v,w) = B(w,v)\\).\nSkew-symmetric: if \\(B(v,w) = -B(w,v)\\).\n\n\n\nQuadratic Forms\nA quadratic form is a special case obtained by feeding the *same- vector into both slots of a bilinear form:\n\\[\nQ(v) = B(v,v).\n\\]\nIn coordinates, with a matrix \\(A\\):\n\\[\nQ(v) = v^\\top A v.\n\\]\nExamples:\n\nIn \\(\\mathbb{R}^2\\), \\(Q(x,y) = x^2 + y^2\\) corresponds to the identity matrix.\nIn optimization, quadratic forms represent energy functions, error functions, or cost functions.\n\n\n\nGeometric Meaning\n\nQuadratic forms define conic sections (ellipses, hyperbolas, parabolas) in 2D and quadric surfaces in higher dimensions.\nThey also measure “curvature” locally in multivariable functions (via the Hessian matrix).\n\n\n\nWhy This Matters for Multilinear Algebra\n\nBilinear forms are 2nd-order tensors: one covector for each input.\nQuadratic forms connect multilinearity with geometry (shapes, volumes, energy).\nThe distinction between symmetric and skew-symmetric bilinear forms leads to exterior algebra and inner product structures later.\n\n\n\nExercises\n\nMatrix Bilinear Form: Let\n\\[\nA = \\begin{bmatrix}2 & 1 \\\\ 1 & 3\\end{bmatrix}.\n\\]\nDefine \\(B(v,w) = v^\\top A w\\).\n\nCompute \\(B((1,0),(0,1))\\).\nCompute \\(B((1,2),(3,4))\\).\n\nSymmetry Check: For the above \\(B\\), show that \\(B(v,w) = B(w,v)\\).\nQuadratic Form: Compute \\(Q(x,y) = [x \\; y] A [x \\; y]^\\top\\) for the same matrix \\(A\\). Write the explicit formula.\nGeometric Interpretation: Consider \\(Q(x,y) = 4x^2 + y^2\\). Sketch (or describe) the curve \\(Q(x,y)=1\\). What kind of conic section is it?\nTensor Connection: Explain why a bilinear form \\(B: V \\times V \\to \\mathbb{R}\\) can be seen as an element of \\(V^- \\otimes V^*\\). What does this mean in terms of indices (covariant slots)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-3.-tensors-as-indexed-arrays",
    "href": "index.html#chapter-3.-tensors-as-indexed-arrays",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 3. Tensors as Indexed Arrays",
    "text": "Chapter 3. Tensors as Indexed Arrays\n\n3.1 Order (Arity), Shape, and Indices\nWe now begin exploring tensors directly, starting with the array viewpoint. This perspective treats tensors as multi-dimensional generalizations of matrices.\n\nOrder (Arity) of a Tensor\nThe order (or arity) of a tensor is the number of indices needed to locate one of its entries.\n\n0th-order: A scalar, e.g. \\(5\\).\n1st-order: A vector \\(v_i\\), with one index.\n2nd-order: A matrix \\(A_{ij}\\), with two indices.\n3rd-order: A block of numbers \\(T_{ijk}\\).\nkth-order: An array with \\(k\\) indices.\n\nThus, the order tells us how many “directions” or “modes” the tensor has.\n\n\nShape (Dimensions of Each Mode)\nEach index ranges over some set of values, defining the shape of the tensor.\n\nA vector in \\(\\mathbb{R}^n\\) has shape \\((n)\\).\nA matrix of size \\(m \\times n\\) has shape \\((m,n)\\).\nA color image of size \\(100 \\times 200\\) with 3 channels has shape \\((100,200,3)\\).\n\nThe shape is just the list of sizes of each mode.\n\n\nIndices and Notation\nIndices label positions in the tensor:\n\\[\nT_{i_1 i_2 \\dots i_k}.\n\\]\nExample: A 3rd-order tensor \\(T_{ijk}\\) with shape \\((2,3,4)\\) has:\n\n\\(i \\in \\{1,2\\}\\),\n\\(j \\in \\{1,2,3\\}\\),\n\\(k \\in \\{1,2,3,4\\}\\).\n\nSo it contains \\(2 \\times 3 \\times 4 = 24\\) entries.\n\n\nVisual Intuition\n\nScalars are points.\nVectors are arrows (1D arrays).\nMatrices are grids (2D arrays).\nHigher-order tensors are cubes, hypercubes, or higher-dimensional arrays, which we can’t fully draw but can still manipulate symbolically.\n\n\n\nWhy This Matters\n\nThe array view is the most concrete: you can store tensors in memory, index them, and manipulate them in code.\nIt provides the language of shapes that data science, ML, and physics use constantly.\nLater, we will see that these indices correspond to *slots- in multilinear maps and tensor product spaces.\n\n\n\nExercises\n\nCounting Entries: How many entries does a tensor of shape \\((3,4,5)\\) have?\nOrder Identification: Identify the order and shape of each object:\n\n\nA grayscale image \\(64 \\times 64\\).\n\n\nA video: 30 frames, each \\(128 \\times 128\\) RGB.\n\n\nA dataset with 1000 samples, each a \\(20 \\times 20\\) grayscale image.\n\n\nIndex Practice: For a tensor \\(T_{ijk}\\) with shape \\((2,2,2)\\), list explicitly all the index triples \\((i,j,k)\\).\nShape Transformation: Flatten a tensor with shape \\((5,4,3)\\) into a matrix. What two possible shapes could the matrix have (depending on how you group the indices)?\nThought Experiment: Why is a scalar sometimes called a “0th-order tensor”? How does this viewpoint help unify the hierarchy of scalars, vectors, matrices, and higher-order tensors?\n\n\n\n\n3.2 Covariant vs. Contravariant Indices\nSo far, we’ve treated indices as simple “positions in an array.” But in multilinear algebra, indices carry roles. Some belong to vectors (contravariant), others to covectors (covariant). Understanding this distinction is crucial for how tensors behave under change of basis.\n\nVectors vs. Covectors\n\nVectors are elements of a space \\(V\\). They transform with the basis.\nCovectors (linear functionals) are elements of the dual space \\(V^*\\). They transform with the *inverse transpose- of the basis change.\n\nThis leads to two kinds of indices:\n\nContravariant indices (upper): \\(v^i\\), coordinates of a vector.\nCovariant indices (lower): \\(\\omega_j\\), coordinates of a covector.\n\n\n\nTensors Mixing Both\nA tensor may have both types of indices:\n\\[\nT^{i_1 i_2 \\dots i_p}_{j_1 j_2 \\dots j_q},\n\\]\nwhich means it accepts \\(q\\) vectors and \\(p\\) covectors as inputs (or outputs, depending on interpretation).\nExamples:\n\nA vector \\(v^i\\) → contravariant (upper index).\nA covector \\(\\omega_j\\) → covariant (lower index).\nA bilinear form \\(B_{ij}\\) → two covariant indices.\nA linear map \\(A^i{}_j\\) → one up and one down (it eats a vector, gives back a vector).\n\n\n\nWhy Two Types?\nThis distinction is not cosmetic:\n\nWhen we change basis, vectors and covectors transform in “opposite” ways.\nHaving both ensures that tensor equations describe intrinsic relationships, independent of coordinates.\n\nExample: Inner product\n\\[\n\\langle v, w \\rangle = g_{ij} v^i w^j.\n\\]\nHere \\(g_{ij}\\) is a metric tensor (covariant), combining two contravariant vectors into a scalar.\n\n\nPictures Before Symbols\n\nContravariant: arrows pointing “outward” (directions in space).\nCovariant: measuring devices pointing “inward” (hyperplanes that assign numbers to arrows).\nTensors: diagrams with arrows in and out, representing how they connect inputs to outputs.\n\nThis picture-based intuition helps prevent index mistakes when writing formulas.\n\n\nWhy This Matters\n\nCovariant vs. contravariant indices explain the geometry of tensors, not just their array form.\nIt prepares us for raising and lowering indices with inner products.\nIt ensures we can handle basis changes correctly (Chapter 12 will revisit this in detail).\n\n\n\nExercises\n\nIdentify Index Type: For each object, say whether its indices are covariant, contravariant, or mixed:\n\n\n\\(v^i\\).\n\n\n\\(\\omega_j\\).\n\n\n\\(A^i{}_j\\).\n\n\n\\(B_{ij}\\).\n\n\nDual Basis Practice: In \\(\\mathbb{R}^2\\) with basis \\(e_1, e_2\\) and dual basis \\(e^1, e^2\\):\n\nWrite a vector \\(v = 3e_1 + 4e_2\\) in coordinates \\(v^i\\).\nEvaluate \\(\\omega(v)\\) for \\(\\omega = 2e^1 - e^2\\).\n\nBasis Change Intuition: Suppose we scale the basis of \\(\\mathbb{R}^2\\) by 2: \\(e'_i = 2 e_i\\).\n\nHow do the contravariant coordinates \\(v^i\\) of a vector change?\nHow do the covariant coordinates \\(\\omega_i\\) of a covector change?\n\nMixed Tensor Example: Interpret the meaning of a tensor \\(T^i{}_j\\) acting on a vector \\(v^j\\). What kind of object is the result?\nThought Experiment: Why do we need both contravariant and covariant indices to describe something like the dot product? What would go wrong if we only allowed one type?\n\n\n\n\n3.3 Change of Basis Rules in Coordinates\nSo far we have seen that indices can be contravariant (upper) or covariant (lower). The key difference appears when we change basis. Multilinear algebra is all about writing rules that remain valid regardless of coordinates, and basis transformations reveal why the distinction is essential.\n\nVectors Under Change of Basis\nLet \\(V = \\mathbb{R}^n\\). Suppose we change from an old basis \\(\\{e_i\\}\\) to a new basis \\(\\{e'_i\\}\\):\n\\[\ne'_i = P^j{}_i e_j,\n\\]\nwhere \\(P\\) is the change-of-basis matrix.\n\nA vector \\(v\\) has coordinates \\(v^i\\) in the old basis and \\(v'^i\\) in the new basis.\nThe relation is:\n\n\\[\nv'^i = (P^{-1})^i{}_j v^j.\n\\]\nThus, contravariant components transform with the inverse of the basis change.\n\n\nCovectors Under Change of Basis\nNow consider a covector \\(\\omega \\in V^*\\), expressed in the old basis as \\(\\omega_i\\). Under the same change of basis:\n\\[\n\\omega'_i = P^j{}_i \\, \\omega_j.\n\\]\nThus, covariant components transform directly with the basis change matrix.\n\n\nGeneral Tensor Transformation\nA tensor with both covariant and contravariant indices transforms by applying these rules to each index:\n\\[\nT'^{i_1 \\dots i_p}{}_{j_1 \\dots j_q} =\n(P^{-1})^{i_1}{}_{k_1} \\cdots (P^{-1})^{i_p}{}_{k_p}\n\\, P^{\\ell_1}{}_{j_1} \\cdots P^{\\ell_q}{}_{j_q}\n\\, T^{k_1 \\dots k_p}{}_{\\ell_1 \\dots \\ell_q}.\n\\]\n\nUpper indices get \\(P^{-1}\\).\nLower indices get \\(P\\).\nThis ensures tensor equations remain coordinate-independent.\n\n\n\nSimple Example: Linear Maps\nA linear map \\(A: V \\to V\\) has components \\(A^i{}_j\\). Under change of basis:\n\\[\nA'^i{}_j = (P^{-1})^i{}_k \\, A^k{}_\\ell \\, P^\\ell{}_j.\n\\]\nThis is the familiar similarity transformation:\n\\[\n[A]_{new} = P^{-1} [A]_{old} P.\n\\]\n\n\nWhy This Matters\n\nBasis changes test whether your formulas are intrinsic or just coordinate artifacts.\nThe distinction between covariant and contravariant is precisely what makes tensor equations survive basis changes intact.\nIn physics, this explains why laws (like Maxwell’s equations or stress-strain relations) remain valid no matter what coordinates we choose.\n\n\n\nExercises\n\nVector Transformation: Let \\(P = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\).\n\nIf \\(v = (4,6)\\) in the old basis, what are its coordinates in the new basis?\n\nCovector Transformation: With the same \\(P\\), let \\(\\omega = (1,2)\\) in the old basis. What are its coordinates in the new basis?\nMatrix Transformation: Let\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 0 & 1\\end{bmatrix}, \\quad\nP = \\begin{bmatrix}1 & 1 \\\\ 0 & 1\\end{bmatrix}.\n\\]\nCompute \\(A' = P^{-1} A P\\).\nTensor Component Count: How many transformation matrices \\(P\\) and \\(P^{-1}\\) appear in the formula for a tensor of type \\((2,1)\\)?\nThought Experiment: Why would formulas break if we treated all indices as the same type (ignoring covariant vs. contravariant)? Consider the dot product as an example.\n\n\n\n\n3.4 Einstein Summation and Index Hygiene\nWhen working with tensors, writing every summation explicitly quickly becomes messy. To keep formulas clean, mathematicians and physicists use the Einstein summation convention and a set of informal “index hygiene” rules.\n\nEinstein Summation Convention\nThe rule is simple: whenever an index appears once up and once down, you sum over it.\nExample in \\(\\mathbb{R}^3\\):\n\\[\ny^i = A^i{}_j x^j\n\\]\nmeans\n\\[\ny^i = \\sum_{j=1}^3 A^i{}_j x^j.\n\\]\nThis compact notation hides the summation symbol but makes multilinear expressions much easier to read.\n\n\nFree vs. Dummy Indices\n\nFree indices: appear only once in a term; they label the components of the result.\nDummy indices: appear exactly twice (once up, once down); they are summed over and can be renamed arbitrarily.\n\nExample:\n\\[\nz^i = B^i{}_j x^j\n\\]\nHere \\(i\\) is free (labels components of \\(z\\)), and \\(j\\) is a dummy index (summed).\n\n\nIndex Hygiene Rules\n\nNever use the same index more than twice in a term.\nNever mix up free and dummy indices.\nRename dummy indices freely if it helps clarity.\n\nExample of bad hygiene:\n\\[\nA^i{}_i \\, x^i\n\\]\nThis is ambiguous, because \\(i\\) appears three times. Correct it by renaming:\n\\[\n(A^i{}_i) \\, x^j.\n\\]\n\n\nExamples of Einstein Notation\n\nDot product: \\(\\langle v,w \\rangle = v^i w_i\\).\nMatrix-vector product: \\(y^i = A^i{}_j x^j\\).\nBilinear form: \\(B(v,w) = B_{ij} v^i w^j\\).\nTrace of a matrix: \\(\\mathrm{tr}(A) = A^i{}_i\\).\n\n\n\nWhy This Matters\n\nEinstein summation is the language of tensors: concise, unambiguous, and basis-independent.\nIt allows formulas to be read structurally, focusing on how indices connect rather than on summation symbols.\nPracticing good index hygiene prevents mistakes when manipulating complicated expressions.\n\n\n\nExercises\n\nSummation Practice: Expand the Einstein-summed expression\n\\[\ny^i = A^i{}_j x^j\n\\]\nexplicitly for \\(i=1,2\\) when\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}, \\quad x=(5,6).\n\\]\nDot Product Check: Show that \\(v^i w_i\\) is invariant under a change of dummy index name (e.g. rewrite with \\(j\\) instead of \\(i\\)).\nTrace Calculation: For\n\\[\nA = \\begin{bmatrix}2 & 1 \\\\ 0 & -3\\end{bmatrix},\n\\]\ncompute \\(\\mathrm{tr}(A)\\) using Einstein notation.\nIndex Hygiene: Identify the mistake in the following expression and correct it:\n\\[\nC^i = A^i{}_j B^j{}_j x^j.\n\\]\nThought Experiment: Why does Einstein notation require one index up and one down to imply summation? What would go wrong if we summed whenever indices simply repeated, regardless of position?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-4.-tensors-as-multilinear-maps",
    "href": "index.html#chapter-4.-tensors-as-multilinear-maps",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 4. Tensors as Multilinear Maps",
    "text": "Chapter 4. Tensors as Multilinear Maps\n\n4.1 Multilinearity and Currying\nSo far, we treated tensors as arrays of numbers. Now we switch to the map viewpoint: tensors as functions that are linear in each argument separately. This is the most natural way to see how tensors act.\n\nWhat Does Multilinear Mean?\nA map \\(T: V_1 \\times V_2 \\times \\cdots \\times V_k \\to \\mathbb{R}\\) (or to another vector space) is multilinear if it is linear in each slot, while the others are fixed.\nExample (bilinear):\n\\[\nB(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w).\n\\]\nThe same rule holds in each argument.\n\nLinear → 1 slot.\nBilinear → 2 slots.\nTrilinear → 3 slots.\nk-linear → k slots.\n\n\n\nExamples of Multilinear Maps\n\nDot product: \\(\\langle v,w\\rangle\\), bilinear.\nMatrix-vector action: \\(A(v)\\), linear (1 slot).\nDeterminant in \\(\\mathbb{R}^2\\): \\(\\det(u,v) = u_1 v_2 - u_2 v_1\\), bilinear.\nScalar triple product: \\(u \\cdot (v \\times w)\\), trilinear.\n\nThese all satisfy linearity in each argument separately.\n\n\nCurrying Viewpoint\nAnother way to think about multilinear maps is through currying:\n\nA bilinear map \\(B: V \\times W \\to \\mathbb{R}\\) can be seen as a function\n\\[\nB(v,-): W \\to \\mathbb{R}, \\quad w \\mapsto B(v,w).\n\\]\nSo, fixing one input gives a linear map in the remaining argument.\nIn general, a \\(k\\)-linear map can be seen as a nested sequence of linear maps, each taking one input at a time.\n\nThis perspective helps connect multilinear maps with ordinary linear maps, by viewing them as “linear maps into linear maps.”\n\n\nWhy This Matters\n\nThis viewpoint clarifies how tensors can be “evaluated” by plugging in vectors.\nIt connects with functional programming ideas (currying and partial application).\nIt bridges between arrays (coordinates) and abstract multilinear functionals.\n\n\n\nExercises\n\nBilinearity Check: Verify directly that the dot product \\(\\langle (x_1,y_1),(x_2,y_2)\\rangle = x_1x_2 + y_1y_2\\) is bilinear.\nCurrying Example: Let \\(B(u,v) = u_1v_1 + 2u_2v_2\\). For a fixed \\(u=(1,2)\\), write the resulting linear functional on \\(v\\).\nDeterminant as Bilinear Form: Show that \\(\\det(u,v) = u_1v_2 - u_2v_1\\) is bilinear in \\(\\mathbb{R}^2\\).\nTrilinear Example: Prove that the scalar triple product \\(u \\cdot (v \\times w)\\) is trilinear by checking linearity in \\(u\\).\nThought Experiment: Why might it be useful to think of a bilinear map as a linear map into the dual space, i.e. \\(B: V \\to W^*\\)?\n\n\n\n\n4.2 Evaluation with Vectors and Covectors\nIn the map viewpoint, tensors are best understood by how they act on inputs. Depending on their type (covariant or contravariant indices), tensors expect vectors, covectors, or both.\n\nFeeding Vectors into Covariant Slots\nA purely covariant tensor \\(T_{ij}\\) is a multilinear map\n\\[\nT: V \\times V \\to \\mathbb{R}.\n\\]\n\nYou feed in two vectors \\(u, v \\in V\\).\nThe output is a scalar: \\(T(u,v) = T_{ij} u^i v^j\\).\n\nExample:\n\nA bilinear form (like an inner product) is a covariant 2-tensor.\n\n\n\nFeeding Covectors into Contravariant Slots\nA purely contravariant tensor \\(T^{ij}\\) is a multilinear map\n\\[\nT: V^- \\times V^- \\to \\mathbb{R}.\n\\]\n\nYou feed in two covectors \\(\\alpha, \\beta \\in V^*\\).\nThe output is a scalar: \\(T(\\alpha,\\beta) = T^{ij}\\alpha_i \\beta_j\\).\n\n\n\nMixed Tensors: Both Types of Inputs\nA mixed tensor \\(T^i{}_j\\) acts as a map:\n\\[\nT: V \\times V^- \\to \\mathbb{R}.\n\\]\nBut more naturally, it can be seen as:\n\ntaking a vector and giving back a vector,\nor taking a covector and giving back a covector.\n\nExample:\n\nA linear operator \\(A: V \\to V\\) has components \\(A^i{}_j\\). Given \\(v^j\\), it produces another vector \\(w^i = A^i{}_j v^j\\).\n\n\n\nEvaluating Step by Step\n\nPick a tensor.\nPlug in the right kind of inputs (vector or covector) into the right slots.\nContract the matching indices (up with down).\nThe result is either a scalar, a vector, or another tensor (depending on how many free indices remain).\n\n\n\nWhy This Matters\n\nThis clarifies the action of tensors, not just their coordinates.\nEvaluation explains why indices are placed up or down: they indicate what kind of input the tensor expects.\nIt connects naturally with Einstein summation: every evaluation is just an index contraction.\n\n\n\nExercises\n\nEvaluation of Bilinear Form: Let \\(B_{ij} = \\begin{bmatrix}1 & 2 \\\\ 2 & 3\\end{bmatrix}\\). Compute \\(B(u,v)\\) for \\(u=(1,1), v=(2,3)\\).\nLinear Operator Action: Let \\(A^i{}_j = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}\\).\n\nApply \\(A\\) to \\(v=(4,5)\\).\nInterpret the result.\n\nCovector Evaluation: If \\(\\omega = (2, -1)\\) and \\(v=(3,4)\\), compute \\(\\omega(v)\\).\nMixed Tensor Evaluation: For \\(T^i{}_j = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\), find the output vector when applied to \\(v=(1,2)\\).\nThought Experiment: Why does the distinction between covariant and contravariant slots matter when evaluating tensors? What would go wrong if we treated them the same?\n\n\n\n\n4.3 From Maps to Arrays (and Back) via a Basis\nSo far, we’ve looked at tensors as multilinear maps (abstract) and as arrays of numbers (concrete). The bridge between these viewpoints is a choice of basis.\n\nStep 1: Start with a Multilinear Map\nSuppose \\(T: V \\times W \\to \\mathbb{R}\\) is bilinear.\n\nOn its own, \\(T\\) is just a rule for combining vectors into a number.\nExample: in \\(\\mathbb{R}^2\\), \\(T((x_1,x_2),(y_1,y_2)) = x_1 y_1 + 2x_2 y_2\\).\n\n\n\nStep 2: Choose Bases\nLet \\(\\{e_i\\}\\) be a basis for \\(V\\) and \\(\\{f_j\\}\\) a basis for \\(W\\).\n\nWe can evaluate \\(T(e_i,f_j)\\) for each pair of basis vectors.\nThese numbers form an array of components \\(T_{ij}\\).\n\nSo in a basis, the multilinear map has a coordinate table.\n\n\nStep 3: Using the Array to Reconstruct the Map\nFor general vectors \\(v = v^i e_i\\) and \\(w = w^j f_j\\):\n\\[\nT(v,w) = T_{ij} v^i w^j.\n\\]\n\nHere, the coefficients \\(v^i, w^j\\) are the coordinates of \\(v, w\\).\nThe array entries \\(T_{ij}\\) tell us how the map acts on basis vectors.\n\nThus:\n\nAbstract definition: multilinear rule.\nConcrete representation: an array of numbers (depends on basis).\n\n\n\nStep 4: General Tensors\nFor a tensor of type \\((p,q)\\):\n\nChoose a basis for \\(V\\).\nEvaluate the tensor on all combinations of \\(q\\) basis vectors and \\(p\\) dual basis covectors.\nThe results form a multidimensional array \\(T^{i_1 \\dots i_p}{}_{j_1 \\dots j_q}\\).\n\nIn coordinates:\n\\[\nT(v_1, \\dots, v_q, \\omega^1, \\dots, \\omega^p) =\nT^{i_1 \\dots i_p}{}_{j_1 \\dots j_q}\n\\, v_1^{j_1} \\cdots v_q^{j_q} \\, \\omega^1_{i_1} \\cdots \\omega^p_{i_p}.\n\\]\n\n\nWhy This Matters\n\nIt explains why tensors can be stored as arrays of numbers: those numbers are just evaluations on basis elements.\nIt shows why basis choice matters for components but not for the tensor itself.\nIt unifies the map viewpoint and the array viewpoint into one consistent framework.\n\n\n\nExercises\n\nMatrix from Bilinear Form: Let \\(T((x_1,x_2),(y_1,y_2)) = 3x_1y_1 + 2x_1y_2 + x_2y_1\\).\n\nWrite down the matrix \\([T_{ij}]\\).\nCompute \\(T((1,2),(3,4))\\) using both the formula and the matrix.\n\nBasis Choice Effect: In \\(\\mathbb{R}^2\\), let \\(T\\) be the dot product.\n\nWhat is the matrix of \\(T\\) in the standard basis?\nWhat is the matrix of \\(T\\) in the basis \\((1,1),(1,-1)\\)?\n\nCoordinate Reconstruction: Let \\(T_{ij} = \\begin{bmatrix}1 & 2 \\\\ 0 & 3\\end{bmatrix}\\). Compute \\(T(v,w)\\) for \\(v=(2,1), w=(1,4)\\).\nHigher Order Example: Suppose \\(T\\) is a 3rd-order tensor with components \\(T_{ijk}\\). Write the formula for \\(T(u,v,w)\\) in terms of \\(T_{ijk}\\), \\(u^i\\), \\(v^j\\), and \\(w^k\\).\nThought Experiment: Why can the same abstract tensor have different component arrays depending on the basis? What stays the same across all choices?\n\n\n\n\n4.4 Universal Examples: Bilinear Forms, Trilinear Mixing\nTo make the map viewpoint concrete, let’s look at a few universal examples. These are classic multilinear maps that keep reappearing in mathematics, physics, and data science.\n\nExample 1: Bilinear Forms\nA bilinear form is a covariant 2-tensor:\n\\[\nB: V \\times V \\to \\mathbb{R}, \\quad B(u,v) = B_{ij} u^i v^j.\n\\]\n\nDot product: \\(\\langle u,v \\rangle = \\delta_{ij} u^i v^j\\).\nGeneral quadratic form: \\(Q(v) = v^\\top A v\\).\nApplications: measuring angles, energy, distance.\n\n\n\nExample 2: Determinant (Alternating Multilinear Form)\nThe determinant in \\(\\mathbb{R}^n\\) is an n-linear map of the column vectors:\n\\[\n\\det(v_1,\\dots,v_n).\n\\]\nIt is:\n\nMultilinear: linear in each column separately.\nAlternating: if two inputs are the same, the determinant vanishes.\nGeometric meaning: volume of the parallelepiped spanned by the vectors.\n\n\n\nExample 3: Scalar Triple Product (Trilinear)\nIn \\(\\mathbb{R}^3\\):\n\\[\n[u,v,w] = u \\cdot (v \\times w).\n\\]\n\nTrilinear: linear in each of \\(u,v,w\\).\nGeometric meaning: signed volume of the parallelepiped formed by \\(u,v,w\\).\nAppears in mechanics (torques, volumes, orientation tests).\n\n\n\nExample 4: Tensor Contraction in Applications\nSuppose \\(T: V \\times W \\times X \\to \\mathbb{R}\\) with components \\(T_{ijk}\\).\n\nFixing one input reduces \\(T\\) to a bilinear form.\nFixing two inputs reduces \\(T\\) to a linear functional.\n\nThis is how general multilinear maps can be “partially evaluated.”\n\n\nExample 5: Mixing Signals (Trilinear Mixing)\nIn signal processing and ML, a trilinear map appears naturally:\n\\[\nM(u,v,w) = \\sum_{i,j,k} T_{ijk} u^i v^j w^k.\n\\]\n\nExample: a 3D convolution kernel.\nApplications: image processing, tensor regression, multiway data analysis.\n\n\n\nWhy These Examples Matter\n\nThey show how multilinear maps generalize familiar ideas (dot product → determinant → triple product).\nThey illustrate how multilinearity encodes geometry (lengths, areas, volumes).\nThey connect abstract tensor notation to real applications (ML, physics, graphics).\n\n\n\nExercises\n\nDot Product as Bilinear Form: Show that the dot product \\(\\langle u,v \\rangle = u^i v^i\\) is bilinear.\nDeterminant in 2D: For \\(u=(1,0), v=(1,2)\\), compute \\(\\det(u,v)\\). Interpret geometrically.\nTriple Product: Compute \\([u,v,w]\\) for \\(u=(1,0,0), v=(0,1,0), w=(0,0,1)\\).\nSignal Mixing Example: Let \\(T_{ijk} = 1\\) if \\(i=j=k\\), and \\(0\\) otherwise, for indices \\(1 \\leq i,j,k \\leq 2\\). Compute \\(M(u,v,w)\\) for \\(u=(1,2), v=(3,4), w=(5,6)\\).\nThought Experiment: Why do determinants and triple products count as multilinear maps, not just algebraic formulas?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-5.-tensors-as-elements-of-tensor-products",
    "href": "index.html#chapter-5.-tensors-as-elements-of-tensor-products",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 5. Tensors as Elements of Tensor Products",
    "text": "Chapter 5. Tensors as Elements of Tensor Products\n\n5.1 Constructing \\(V \\otimes W\\): Intuition and Goals\nSo far, we’ve looked at tensors as arrays and as multilinear maps. The third viewpoint places them as elements of tensor product spaces. This perspective is abstract, but it unifies everything: it explains why- multilinear maps correspond to arrays and how- they transform consistently.\n\nMotivation\nSuppose you want to encode a bilinear map \\(B: V \\times W \\to \\mathbb{R}\\).\n\nYou could record all values of \\(B(e_i, f_j)\\) on basis elements.\nYou could write it as an array \\(B_{ij}\\).\nOr you could find a new space where a single element encodes all the bilinear behavior.\n\nThat new space is the tensor product space \\(V \\otimes W\\).\n\n\nIntuition: Building Blocks\n\nGiven \\(v \\in V\\) and \\(w \\in W\\), we form a simple tensor \\(v \\otimes w\\).\nThink of \\(v \\otimes w\\) as a “formal symbol” that remembers both vectors at once.\nThen we allow linear combinations:\n\\[\nu = \\sum_k v_k \\otimes w_k.\n\\]\nThese combinations form a new vector space: \\(V \\otimes W\\).\n\n\n\nUniversal Property (informal)\nThe tensor product is defined so that:\n\nEvery bilinear map \\(B: V \\times W \\to U\\) factors uniquely through a linear map \\(\\tilde{B}: V \\otimes W \\to U\\).\nIn plain words: “Bilinear maps are the same thing as linear maps out of a tensor product.”\n\nThis property makes \\(V \\otimes W\\) the *right- space to house tensors.\n\n\nExample\nIf \\(V = \\mathbb{R}^2, W = \\mathbb{R}^3\\), then \\(V \\otimes W\\) has dimension \\(2 \\times 3 = 6\\).\n\nBasis: \\(e_i \\otimes f_j\\) with \\(i=1,2; j=1,2,3\\).\nA general element:\n\\[\nu = \\sum_{i=1}^2 \\sum_{j=1}^3 c_{ij} (e_i \\otimes f_j).\n\\]\nCoordinates \\(c_{ij}\\) are exactly the array components of a bilinear map.\n\n\n\nGoals of This Viewpoint\n\nProvide a basis-independent foundation for tensors.\nUnify maps, arrays, and algebra into one framework.\nGeneralize easily: \\(V_1 \\otimes \\cdots \\otimes V_k\\) encodes \\(k\\)-linear maps.\n\n\n\nExercises\n\nDimension Count: If \\(\\dim V = 3\\) and \\(\\dim W = 4\\), what is \\(\\dim(V \\otimes W)\\)?\nSimple Tensor Example: In \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^2\\), write the simple tensor \\((1,2) \\otimes (3,4)\\) as a linear combination of basis elements \\(e_i \\otimes e_j\\).\nArray to Tensor: For the bilinear form \\(B((x_1,x_2),(y_1,y_2)) = 2x_1y_1 + 3x_2y_2\\), find the corresponding tensor in \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^2\\).\nUniversal Property Check: Suppose \\(B: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}\\) is given by \\(B(u,v) = u^\\top v\\).\n\nShow how \\(B\\) factors through a linear map \\(\\tilde{B}: \\mathbb{R}^2 \\otimes \\mathbb{R}^2 \\to \\mathbb{R}\\).\n\nThought Experiment: Why is it useful to replace “bilinear maps” with “linear maps from a bigger space”? How does this simplify reasoning and computation?\n\n\n\n\n5.2 Simple vs. General Tensors\nNow that we’ve seen how to build the tensor product space, we need to distinguish between its basic “atoms” and more complicated elements.\n\nSimple (Decomposable) Tensors\nA simple tensor (also called pure or decomposable) is one that can be written as a single tensor product:\n\\[\nu = v \\otimes w.\n\\]\nExamples:\n\nIn \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^3\\), \\((1,2) \\otimes (0,1,1)\\) is simple.\nIts array of components is an outer product:\n\\[\n\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\n\\otimes\n\\begin{bmatrix}0 & 1 & 1\\end{bmatrix}\n=\n\\begin{bmatrix}0 & 1 & 1 \\\\ 0 & 2 & 2\\end{bmatrix}.\n\\]\n\nSo simple tensors correspond to rank-one arrays (outer products).\n\n\nGeneral Tensors\nA general tensor in \\(V \\otimes W\\) is a linear combination of simple tensors:\n\\[\nT = \\sum_{k=1}^m v_k \\otimes w_k.\n\\]\n\nSome tensors can be expressed as a single simple tensor.\nMost cannot; they require a sum of several simple tensors.\nThe minimal number of terms in such a sum is called the tensor rank (analogous to matrix rank for order-2 tensors).\n\n\n\nMatrix Analogy\n\nSimple tensors ↔︎ rank-one matrices (outer products of a column and a row).\nGeneral tensors ↔︎ arbitrary matrices (sums of rank-one pieces).\n\nFor higher-order tensors, the situation is the same:\n\nSimple tensor = one outer product of vectors.\nGeneral tensor = sum of several such products.\n\n\n\nWhy This Matters\n\nThe distinction underlies tensor decomposition methods (CP, Tucker, TT).\nSimple tensors capture the “building blocks” of tensor spaces.\nGeneral tensors encode richer structure, but often we approximate them by a small number of simple ones.\n\n\n\nExercises\n\nSimple or Not? In \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^2\\), determine whether\n\\[\nT = e_1 \\otimes e_1 + e_2 \\otimes e_2\n\\]\nis a simple tensor.\nOuter Product Calculation: Compute the outer product of \\(u=(1,2)\\) and \\(v=(3,4,5)\\). Write the resulting \\(2 \\times 3\\) matrix.\nRank-One Check: Given the matrix\n\\[\nM = \\begin{bmatrix}2 & 4 \\\\ 3 & 6\\end{bmatrix},\n\\]\nshow that \\(M\\) is a simple tensor (rank one) by writing it as \\(u \\otimes v\\).\nGeneral Tensor Example: Express\n\\[\n\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\n\\]\nas a sum of two simple tensors in \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^2\\).\nThought Experiment: Why are most tensors not simple? What does this imply about the usefulness of tensor decompositions in applications like machine learning?\n\n\n\n\n5.3 Dimension and Bases of Tensor Products\nHaving distinguished simple tensors from general ones, let’s now describe the structure of a tensor product space: its dimension and basis.\n\nDimension Formula\nIf \\(V\\) and \\(W\\) are finite-dimensional vector spaces, then:\n\\[\n\\dim(V \\otimes W) = \\dim(V) \\cdot \\dim(W).\n\\]\nMore generally, for \\(V_1, V_2, \\dots, V_k\\):\n\\[\n\\dim(V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_k) = \\prod_{i=1}^k \\dim(V_i).\n\\]\nThis matches our intuition that each index multiplies the number of “slots” in the array representation.\n\n\nBasis of a Tensor Product\nSuppose \\(\\{e_i\\}\\) is a basis for \\(V\\) and \\(\\{f_j\\}\\) is a basis for \\(W\\).\n\nThen the set\n\\[\n\\{e_i \\otimes f_j : 1 \\leq i \\leq \\dim V, \\, 1 \\leq j \\leq \\dim W \\}\n\\]\nforms a basis for \\(V \\otimes W\\).\n\nExample:\n\nIf \\(\\dim V = 2, \\dim W = 3\\): \\(\\{e_1 \\otimes f_1, e_1 \\otimes f_2, e_1 \\otimes f_3, e_2 \\otimes f_1, e_2 \\otimes f_2, e_2 \\otimes f_3\\}\\) is a basis (6 elements).\n\n\n\nCoordinates in the Basis\nA general tensor can be expressed as:\n\\[\nT = \\sum_{i,j} c_{ij} \\, (e_i \\otimes f_j),\n\\]\nwhere the coefficients \\(c_{ij}\\) form the familiar matrix (array) representation.\nFor higher-order tensor products, the same logic applies:\n\\[\nT = \\sum_{i,j,k} c_{ijk} \\, (e_i \\otimes f_j \\otimes g_k).\n\\]\n\n\nAnalogy with Arrays\n\nBasis elements correspond to array positions.\nCoefficients are the entries.\nThe dimension formula tells us the total number of independent entries.\n\nThus, the product-space viewpoint and the array viewpoint are perfectly aligned.\n\n\nWhy This Matters\n\nThe basis description explains why tensor product spaces have the same “shape” as arrays.\nIt provides a rigorous foundation for the component representation of tensors.\nIt prepares us to reconcile all three viewpoints (array, map, product space).\n\n\n\nExercises\n\nDimension Count: Compute \\(\\dim(\\mathbb{R}^2 \\otimes \\mathbb{R}^3)\\). List a basis explicitly.\nBasis Expansion: Express \\(T = (1,2) \\otimes (3,4,5)\\) in terms of the standard basis \\(e_i \\otimes f_j\\).\nHigher-Order Example: If \\(\\dim U = 2, \\dim V = 2, \\dim W = 2\\), what is \\(\\dim(U \\otimes V \\otimes W)\\)?\nCoefficient Identification: Let \\(T = e_1 \\otimes f_1 + 2 e_2 \\otimes f_3\\). What are the nonzero coefficients \\(c_{ij}\\)?\nThought Experiment: Why does the formula \\(\\dim(V \\otimes W) = \\dim V \\cdot \\dim W\\) make sense if you think of arrays? How does this match the earlier idea of “shape”?\n\n\n\n\n5.4 Three Viewpoints Reconciled\nWe now have three different ways to understand tensors: as arrays, as multilinear maps, and as elements of tensor product spaces. Each looks different, but they are all equivalent perspectives on the same object.\n\n1. Array View (Concrete)\n\nA tensor is a multi-dimensional array of numbers.\nIts entries depend on a choice of basis.\nOperations are done by indexing and summing (Einstein notation).\nExample: a \\(2 \\times 3 \\times 4\\) tensor is just a 3D block of \\(24\\) numbers.\n\nStrengths: intuitive, easy to compute. Limitations: depends heavily on coordinates.\n\n\n2. Map View (Functional)\n\nA tensor is a multilinear function that takes vectors and covectors as inputs and produces scalars or other tensors.\nExample:\n\nDot product: bilinear map.\nDeterminant: alternating multilinear map.\nLinear operators: mixed tensors.\n\n\nStrengths: coordinate-free, emphasizes action. Limitations: abstract, less computational.\n\n\n3. Product Space View (Foundational)\n\nA tensor is an element of a tensor product space \\(V^{\\otimes p} \\otimes (V^*)^{\\otimes q}\\).\nBasis choice identifies this element with an array of components.\nUniversal property: multilinear maps from vectors ↔︎ linear maps out of tensor products.\n\nStrengths: rigorous, unifying, handles all cases. Limitations: abstract at first, harder to visualize.\n\n\nReconciling Them\n\nArray = product-space + basis.\nMap = product-space + evaluation on inputs.\nProduct-space = abstract “home” that makes the other two consistent.\n\nSo:\n\nIf you want computation, use arrays.\nIf you want geometry or physics intuition, use maps.\nIf you want rigor and generality, use tensor products.\n\nThey are not rivals but complementary languages describing the same object.\n\n\nWhy This Matters\nUnderstanding all three viewpoints and moving fluidly between them is the hallmark of real mastery. It’s like being fluent in three languages: you choose the one that fits the context, but you know they describe the same reality.\n\n\n\nExercises\n\nArray ↔︎ Map: Let \\(T_{ij} = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\).\n\nWrite \\(T(u,v) = T_{ij} u^i v^j\\).\nEvaluate for \\(u=(1,0), v=(0,1)\\).\n\nMap ↔︎ Product-Space: Show that the bilinear form \\(B(u,v) = u_1v_1 + 2u_2v_2\\) corresponds to an element of \\(\\mathbb{R}^2 \\otimes \\mathbb{R}^2\\).\nArray ↔︎ Product-Space: If \\(T = 3 e_1 \\otimes f_2 + 5 e_2 \\otimes f_3\\), what are the nonzero array components \\(T_{ij}\\)?\nThree Languages: Write the dot product of \\(u,v \\in \\mathbb{R}^2\\) in each of the three viewpoints:\n\nArray form\nMultilinear map\nTensor product element\n\nThought Experiment: Why is it valuable to know all three viewpoints instead of just one? Give an application where each viewpoint is the most natural.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-6.-building-blocks",
    "href": "index.html#chapter-6.-building-blocks",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 6. Building Blocks",
    "text": "Chapter 6. Building Blocks\n\n6.1 Tensor (Outer) Product\nNow that we’ve reconciled the three viewpoints, we can explore the core operations on tensors. The first and most fundamental is the tensor product itself, also known as the outer product in the array viewpoint.\n\nDefinition\nGiven two vectors \\(u \\in V\\) and \\(v \\in W\\), their tensor product is\n\\[\nu \\otimes v \\in V \\otimes W.\n\\]\n\nIt is bilinear in \\(u,v\\).\nIn a basis, it looks like an array of rank one (outer product).\n\n\n\nExample with Vectors\nIf \\(u = (1,2)\\) and \\(v = (3,4,5)\\):\n\\[\nu \\otimes v =\n\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}\n\\otimes\n\\begin{bmatrix}3 & 4 & 5\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & 4 & 5 \\\\\n6 & 8 & 10\n\\end{bmatrix}.\n\\]\nThis is a \\(2 \\times 3\\) array - exactly the outer product.\n\n\nHigher-Order Outer Products\nWe can extend this:\n\\[\nu \\otimes v \\otimes w,\n\\]\nproduces a 3rd-order tensor, with entries\n\\[\n(u \\otimes v \\otimes w)_{ijk} = u_i v_j w_k.\n\\]\nIn general, the outer product of \\(k\\) vectors is a simple tensor of order \\(k\\).\n\n\nProperties\n\nBilinearity: \\((au + bu') \\otimes v = a(u \\otimes v) + b(u' \\otimes v)\\).\nNoncommutativity: \\(u \\otimes v \\neq v \\otimes u\\) (unless dimensions align and we impose symmetry).\nRank-One Structure: Outer products produce the simplest tensors, forming the building blocks of general tensors.\n\n\n\nWhy This Matters\n\nThe tensor product (outer product) is the construction rule for higher-order tensors.\nIt connects vector multiplication with multidimensional arrays.\nMany decompositions (CP, Tucker, tensor train) rely on sums of outer products.\n\n\n\nExercises\n\nBasic Outer Product: Compute \\((2,1) \\otimes (1,3)\\). Write it as a \\(2 \\times 2\\) matrix.\nHigher Order: Compute \\((1,2) \\otimes (0,1) \\otimes (3,4)\\). What is the shape of the resulting tensor, and how many entries does it have?\nLinearity Check: Verify that \\((u+u') \\otimes v = u \\otimes v + u' \\otimes v\\) for \\(u=(1,0), u'=(0,1), v=(2,3)\\).\nRank-One Matrix: Show that every outer product \\(u \\otimes v\\) is a rank-one matrix. Give an example.\nThought Experiment: Why does the outer product produce the “simplest” tensors? How does this idea generalize from matrices to higher-order arrays?\n\n\n\n\n6.2 Contraction: Summing Paired Indices\nIf the tensor product (outer product) builds bigger tensors, then contraction is the operation that reduces them by “pairing up” an upper and a lower index and summing over it. This is the natural generalization of the dot product and the trace.\n\nDefinition\nGiven a tensor \\(T^{i}{}_{j}\\), contracting on the index \\(i\\) and \\(j\\) means:\n\\[\n\\mathrm{contr}(T) = T^{i}{}_{i}.\n\\]\n\nYou match one upper and one lower index.\nYou sum over that index.\nThe result is a new tensor of lower order.\n\n\n\nFamiliar Examples\n\nDot Product: For vectors \\(u^i\\) and \\(v_i\\):\n\\[\nu^i v_i\n\\]\nis a contraction, producing a scalar.\nMatrix-Vector Multiplication:\n\\[\ny^i = A^i{}_j x^j\n\\]\ncontracts on \\(j\\), leaving a vector \\(y^i\\).\nTrace of a Matrix:\n\\[\n\\mathrm{tr}(A) = A^i{}_i\n\\]\ncontracts on the row and column indices, producing a scalar.\n\n\n\nGeneral Contraction\nIf \\(T^{i_1 i_2 \\dots i_p}{}_{j_1 j_2 \\dots j_q}\\) is a tensor, contracting on \\(i_k\\) and \\(j_\\ell\\) yields a tensor of type \\((p-1, q-1)\\).\nThis is how tensor calculus generalizes dot products, matrix multiplication, and traces into one operation.\n\n\nWhy This Matters\n\nContraction is the dual operation to the tensor product.\nOuter product increases order, contraction decreases order.\nTogether, they generate most tensor operations in mathematics, physics, and machine learning.\n\n\n\nExercises\n\nDot Product as Contraction: Show explicitly that \\(u \\cdot v = u^i v_i\\) is a contraction.\nMatrix-Vector Multiplication: Let \\(A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\), \\(x=(5,6)\\). Compute \\(y^i = A^i{}_j x^j\\).\nTrace Calculation: For \\(A = \\begin{bmatrix}2 & 1 \\\\ 0 & -3\\end{bmatrix}\\), compute \\(\\mathrm{tr}(A)\\) via contraction.\nHigher-Order Contraction: Suppose \\(T^{ij}{}_{k\\ell}\\) is a 4th-order tensor. What is the order and type of the tensor obtained by contracting on \\(j\\) and \\(\\ell\\)?\nThought Experiment: Why can contraction only happen between one upper and one lower index? What would break if you tried to contract two uppers or two lowers?\n\n\n\n\n6.3 Permutations of Modes: Transpose, Unfold, Matricize\nTensors often need to be rearranged so that their structure fits the operation we want to perform. This section introduces three related operations: permutation of modes, transpose, and matricization (unfolding).\n\nPermuting Modes\nA tensor of order \\(k\\) has \\(k\\) indices:\n\\[\nT_{i_1 i_2 \\dots i_k}.\n\\]\nA permutation of modes reorders these indices.\n\nExample: if \\(T\\) is order 3, with entries \\(T_{ijk}\\), then a permutation might yield \\(T_{kij}\\).\nThis doesn’t change the data, just how we view the axes.\n\n\n\nTranspose (Matrix Case)\nFor a 2nd-order tensor (matrix), permutation of the two indices corresponds to the familiar transpose:\n\\[\nA_{ij} \\mapsto A_{ji}.\n\\]\nThis is just a special case of permuting modes.\n\n\nMatricization (Unfolding)\nFor higher-order tensors, it is often useful to flatten them into matrices.\n\nChoose one index (or a group of indices) to form the rows.\nThe remaining indices form the columns.\n\nExample: For a 3rd-order tensor \\(T_{ijk}\\):\n\nMode-1 unfolding: rows indexed by \\(i\\), columns by \\((j,k)\\).\nMode-2 unfolding: rows indexed by \\(j\\), columns by \\((i,k)\\).\nMode-3 unfolding: rows indexed by \\(k\\), columns by \\((i,j)\\).\n\nThis is widely used in numerical linear algebra and machine learning.\n\n\nWhy Permutations Matter\n\nThey allow us to reinterpret tensors for algorithms (e.g., apply matrix methods to unfolded tensors).\nThey help align indices correctly before contraction.\nThey reveal symmetry or structure hidden in the raw array.\n\n\n\nExercises\n\nPermutation Practice: If \\(T_{ijk}\\) has shape \\((2,3,4)\\), what is the shape of the permuted tensor \\(T_{kij}\\)?\nMatrix Transpose: Write the transpose of\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}.\n\\]\nMode-1 Unfolding: For a tensor \\(T_{ijk}\\) of shape \\((2,2,2)\\), what is the shape of its mode-1 unfolding?\nReversibility: Show that permuting modes twice with inverse permutations returns the original tensor.\nThought Experiment: Why might unfolding a tensor into a matrix help in data analysis (e.g., PCA, SVD)? What do we lose by unfolding?\n\n\n\n\n6.4 Kronecker Product vs. Tensor Product\nThe tensor product and the Kronecker product are closely related, but they live in slightly different worlds. Beginners often confuse them because they both “blow up” dimensions in similar ways. Let’s carefully separate them.\n\nTensor Product (Abstract)\n\nGiven two vector spaces \\(V, W\\), their tensor product \\(V \\otimes W\\) is a new vector space.\nIf \\(\\dim V = m\\) and \\(\\dim W = n\\), then \\(\\dim(V \\otimes W) = mn\\).\nBasis: \\(e_i \\otimes f_j\\).\nPurpose: encodes bilinear maps as linear maps.\n\nExample: For \\(u=(u_1,u_2)\\), \\(v=(v_1,v_2,v_3)\\):\n\\[\nu \\otimes v =\n\\begin{bmatrix}\nu_1v_1 & u_1v_2 & u_1v_3 \\\\\nu_2v_1 & u_2v_2 & u_2v_3\n\\end{bmatrix}.\n\\]\n\n\nKronecker Product (Matrix Operation)\n\nGiven two matrices \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), the Kronecker product is:\n\n\\[\nA \\otimes B =\n\\begin{bmatrix}\na_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\na_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots \\\\\na_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n\\end{bmatrix}.\n\\]\n\nThis produces a block matrix of size \\((mp) \\times (nq)\\).\nUsed heavily in linear algebra identities, signal processing, and ML.\n\n\n\nRelation Between the Two\n\nThe Kronecker product is the matrix representation of the tensor product once bases are chosen.\nIn other words:\n\nTensor product = basis-independent, abstract.\nKronecker product = concrete array form.\n\n\n\n\nExamples\n\nVector Tensor Product: \\((1,2) \\otimes (3,4) = \\begin{bmatrix}3 & 4 \\\\ 6 & 8\\end{bmatrix}.\\)\nMatrix Kronecker Product:\n\\[\n\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\otimes\n\\begin{bmatrix}0 & 5 \\\\ 6 & 7\\end{bmatrix}\n=\n\\begin{bmatrix}\n1\\cdot B & 2\\cdot B \\\\\n3\\cdot B & 4\\cdot B\n\\end{bmatrix}.\n\\]\n\n\n\nWhy This Matters\n\nTensor product explains the theory of multilinearity.\nKronecker product is the computational tool, letting us work with actual arrays.\nKeeping them distinct prevents confusion between concept- and representation*.\n\n\n\nExercises\n\nBasic Kronecker: Compute\n\\[\n\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} \\otimes\n\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}.\n\\]\nDimension Check: If \\(A\\) is \\(2 \\times 3\\) and \\(B\\) is \\(3 \\times 4\\), what is the size of \\(A \\otimes B\\)?\nTensor vs. Kronecker: Show explicitly that for vectors \\(u,v\\), the tensor product \\(u \\otimes v\\) corresponds to the Kronecker product of \\(u\\) and \\(v^\\top\\).\nOuter Product Relation: Express the outer product \\(u \\otimes v\\) as a Kronecker product when \\(u,v\\) are vectors.\nThought Experiment: Why is it useful to separate the abstract tensor product from the concrete Kronecker product? What problems might arise if we conflated them?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-7.-symmetry-and-antisymmetry",
    "href": "index.html#chapter-7.-symmetry-and-antisymmetry",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 7. Symmetry and (Anti)Symmetry",
    "text": "Chapter 7. Symmetry and (Anti)Symmetry\n\n7.1 Symmetrization Operators\nOne of the most important features of tensors is how they behave under permutations of indices. Some tensors stay the same when you swap indices (symmetric), others change sign (antisymmetric). To formalize this, we introduce symmetrization operators.\n\nSymmetric Tensors\nA tensor \\(T_{ij}\\) is symmetric if swapping indices does not change it:\n\\[\nT_{ij} = T_{ji}.\n\\]\nMore generally, a \\(k\\)-tensor \\(T_{i_1 i_2 \\dots i_k}\\) is symmetric if:\n\\[\nT_{i_{\\pi(1)} i_{\\pi(2)} \\dots i_{\\pi(k)}} = T_{i_1 i_2 \\dots i_k}\n\\]\nfor every permutation \\(\\pi\\).\nExample:\n\nThe Hessian matrix of a scalar function is symmetric.\nA covariance matrix is symmetric.\n\n\n\nSymmetrization Operator\nGiven any tensor \\(T\\), its symmetrized version is obtained by averaging over all permutations of indices:\n\\[\n\\mathrm{Sym}(T)_{i_1 i_2 \\dots i_k} = \\frac{1}{k!} \\sum_{\\pi \\in S_k} T_{i_{\\pi(1)} i_{\\pi(2)} \\dots i_{\\pi(k)}}.\n\\]\n\n\\(S_k\\) is the symmetric group on \\(k\\) indices.\nThis ensures the result is symmetric, even if the original tensor was not.\n\n\n\nExample: Symmetrizing a 2nd-Order Tensor\nGiven a matrix \\(A\\), its symmetrization is:\n\\[\n\\mathrm{Sym}(A) = \\frac{1}{2}(A + A^\\top).\n\\]\nThis extracts the symmetric part of a matrix.\n\n\nProperties\n\nSymmetrization is a projection operator: applying it twice gives the same result.\nThe space of symmetric tensors is a subspace of the full tensor space.\nSymmetric tensors are often much smaller in dimension than general tensors.\n\n\n\nWhy This Matters\n\nSymmetrization leads to symmetric tensor spaces, central in polynomial algebra and statistics.\nIt prepares us for the study of alternating tensors (exterior algebra) in the next part.\nMany applications (covariance, stress, Hessians) naturally produce symmetric tensors.\n\n\n\nExercises\n\nMatrix Symmetrization: Compute the symmetrized version of\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}.\n\\]\nCheck Symmetry: Is the tensor \\(T_{ij}\\) with entries \\(T_{12}=1, T_{21}=2, T_{11}=0, T_{22}=3\\) symmetric?\nSymmetrizing a 3rd-Order Tensor: Write the formula for the symmetrized version of a 3rd-order tensor \\(T_{ijk}\\).\nProjection Property: Show that if \\(T\\) is already symmetric, then \\(\\mathrm{Sym}(T) = T\\).\nThought Experiment: Why might symmetrization be important in statistics (e.g., covariance estimation) or in physics (e.g., stress tensors)?\n\n\n\n\n7.2 Alternation (Antisymmetrization)\nIf symmetrization enforces invariance under index swaps, alternation (or antisymmetrization) enforces the opposite: swapping two indices flips the sign. This construction is central to exterior algebra and geometric concepts like orientation and volume.\n\nAntisymmetric Tensors\nA tensor \\(T_{ij}\\) is antisymmetric if\n\\[\nT_{ij} = -T_{ji}.\n\\]\n\nIn particular, \\(T_{ii} = 0\\).\nFor higher orders:\n\\[\nT_{i_{\\pi(1)} i_{\\pi(2)} \\dots i_{\\pi(k)}} = \\mathrm{sgn}(\\pi) \\, T_{i_1 i_2 \\dots i_k},\n\\]\nwhere \\(\\mathrm{sgn}(\\pi)\\) is the sign of the permutation \\(\\pi\\).\n\nExamples:\n\nThe determinant is based on a fully antisymmetric tensor.\nThe cross product in \\(\\mathbb{R}^3\\) can be expressed using an antisymmetric tensor.\n\n\n\nAlternation Operator\nGiven any tensor \\(T\\), its alternated version is obtained by summing with signs:\n\\[\n\\mathrm{Alt}(T)_{i_1 i_2 \\dots i_k} = \\frac{1}{k!} \\sum_{\\pi \\in S_k} \\mathrm{sgn}(\\pi) \\, T_{i_{\\pi(1)} i_{\\pi(2)} \\dots i_{\\pi(k)}}.\n\\]\n\nThis forces antisymmetry.\nIf \\(T\\) was already antisymmetric, \\(\\mathrm{Alt}(T) = T\\).\n\n\n\nExample: Alternating a 2nd-Order Tensor\nFor a matrix \\(A\\), alternation gives:\n\\[\n\\mathrm{Alt}(A) = \\frac{1}{2}(A - A^\\top).\n\\]\nThis extracts the skew-symmetric part of a matrix.\n\n\nProperties\n\nAlternation is a projection operator: \\(\\mathrm{Alt}(\\mathrm{Alt}(T)) = \\mathrm{Alt}(T)\\).\nThe space of antisymmetric \\(k\\)-tensors is called \\(\\Lambda^k(V)\\), the exterior power.\nIts dimension is \\(\\binom{n}{k}\\), much smaller than the full tensor space.\n\n\n\nWhy This Matters\n\nAlternating tensors represent oriented areas, volumes, and higher-dimensional analogues.\nThey are the foundation of differential forms and exterior algebra.\nPhysics applications: angular momentum, electromagnetism, determinants.\n\n\n\nExercises\n\nSkew-Symmetric Matrix: Compute the alternated version of\n\\[\nA = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}.\n\\]\nCheck Antisymmetry: Is the tensor \\(T_{ij}\\) with \\(T_{12}=1, T_{21}=-1, T_{11}=0, T_{22}=0\\) antisymmetric?\nAlternating a 3rd-Order Tensor: Write the formula for \\(\\mathrm{Alt}(T)_{ijk}\\) explicitly in terms of the six permutations of \\((i,j,k)\\).\nZero Property: Show that if two indices are equal in an antisymmetric tensor, the component must vanish.\nThought Experiment: Why is antisymmetry essential for defining oriented volume (via determinants) and exterior algebra?\n\n\n\n\n7.3 Decompositions by Symmetry Type\nSo far we’ve seen how to symmetrize and antisymmetrize tensors. The next step is to notice that *any- tensor can be decomposed into symmetric and antisymmetric parts. This is very similar to how every matrix can be written as the sum of a symmetric and a skew-symmetric matrix.\n\nThe 2nd-Order Case (Matrices)\nGiven a matrix \\(A\\):\n\\[\nA = \\tfrac{1}{2}(A + A^\\top) + \\tfrac{1}{2}(A - A^\\top).\n\\]\n\nThe first term is symmetric.\nThe second term is antisymmetric.\nTogether, they reconstruct the original matrix.\n\nThis is the simplest case of decomposition by symmetry type.\n\n\nHigher-Order Generalization\nFor a tensor \\(T_{i_1 \\dots i_k}\\):\n\nApply the symmetrization operator to get the symmetric part.\nApply the alternation operator to get the antisymmetric part.\n\nBut for \\(k &gt; 2\\), there are many possible symmetry types (not just symmetric vs. antisymmetric).\n\n\nYoung Diagrams and Symmetry Types (Preview)\nIn general, the ways indices can be symmetrized/antisymmetrized correspond to representations of the symmetric group.\n\nFully symmetric: indices invariant under any swap.\nFully antisymmetric: indices change sign under swaps.\nMixed types: some indices symmetric among themselves, others antisymmetric.\n\nThese patterns can be visualized with Young diagrams (a deeper subject in representation theory).\n\n\nWhy Decomposition Matters\n\nSymmetry often reduces the effective dimension of tensor spaces.\nMany natural tensors fall into specific symmetry classes (e.g., metric tensors are symmetric, differential forms are antisymmetric).\nIn applications, splitting into symmetric/antisymmetric parts clarifies the geometric meaning:\n\nSymmetric part → stretching, scaling (like stress/strain).\nAntisymmetric part → rotation, orientation (like angular momentum).\n\n\n\n\nExercises\n\nMatrix Decomposition: Decompose\n\\[\nA = \\begin{bmatrix}1 & 3 \\\\ 2 & 4\\end{bmatrix}\n\\]\ninto symmetric and antisymmetric parts.\nSym + Anti Check: Verify that every \\(2 \\times 2\\) matrix can be written uniquely as \\(S + K\\), with \\(S\\) symmetric and \\(K\\) antisymmetric.\nSymmetric Projection: For \\(T_{ijk}\\), write the formula for its symmetric part \\(\\mathrm{Sym}(T)_{ijk}\\).\nAntisymmetric Projection: For \\(T_{ijk}\\), write the formula for its antisymmetric part \\(\\mathrm{Alt}(T)_{ijk}\\).\nThought Experiment: Why does decomposing tensors by symmetry type help in physics and engineering (e.g., stress tensors, electromagnetic fields)? ### 7.4 Applications: Determinants and Oriented Volumes\n\nNow we put the ideas of symmetry and antisymmetry into practice. Some of the most important geometric and physical quantities are built from alternating tensors - especially the determinant and oriented volumes.\n\n\nDeterminant as an Antisymmetric Multilinear Map\nThe determinant of an \\(n \\times n\\) matrix can be seen as an alternating \\(n\\)-linear form:\n\\[\n\\det(v_1, v_2, \\dots, v_n),\n\\]\nwhere \\(v_i\\) are the column vectors of the matrix.\n\nMultilinear: linear in each column.\nAlternating: if two columns are equal, the determinant vanishes.\nGeometric meaning: signed volume of the parallelepiped spanned by the columns.\n\n\n\nOriented Areas in \\(\\mathbb{R}^2\\)\nGiven two vectors \\(u, v \\in \\mathbb{R}^2\\), the area of the parallelogram they span is:\n\\[\n\\mathrm{Area}(u,v) = | \\det(u,v) |.\n\\]\nThe sign of \\(\\det(u,v)\\) indicates orientation (clockwise vs. counterclockwise).\n\n\nOriented Volumes in \\(\\mathbb{R}^3\\)\nGiven three vectors \\(u,v,w \\in \\mathbb{R}^3\\), the scalar triple product\n\\[\n[u,v,w] = u \\cdot (v \\times w)\n\\]\nis the determinant of the \\(3 \\times 3\\) matrix with columns \\(u,v,w\\).\n\nMagnitude: volume of the parallelepiped.\nSign: orientation (right-handed vs. left-handed system).\n\n\n\nExterior Powers and General Volumes\nThe antisymmetric space \\(\\Lambda^k(V)\\) encodes oriented \\(k\\)-dimensional volumes:\n\nFor \\(k=2\\): oriented areas (parallelograms).\nFor \\(k=3\\): oriented volumes (parallelepipeds).\nFor general \\(k\\): higher-dimensional “hyper-volumes.”\n\nThus, alternating tensors generalize determinants and orientation to any dimension.\n\n\nWhy This Matters\n\nDeterminants, areas, and volumes are not just numbers - they are built from antisymmetric tensors.\nThis viewpoint makes clear why determinants vanish when vectors are linearly dependent: antisymmetry forces collapse.\nOrientation is encoded algebraically, which is crucial in physics (torques, flux) and geometry (orientation of manifolds).\n\n\n\nExercises\n\n2D Determinant: Compute \\(\\det\\big((1,2),(3,4)\\big)\\). Interpret the result geometrically.\nArea from Determinant: Find the area of the parallelogram spanned by \\(u=(2,0)\\) and \\(v=(1,3)\\) using determinants.\nTriple Product: Compute \\([u,v,w]\\) for \\(u=(1,0,0)\\), \\(v=(0,2,0)\\), \\(w=(0,0,3)\\). What volume does this represent?\nLinear Dependence: Show that if \\(v_1, v_2, v_3\\) are linearly dependent in \\(\\mathbb{R}^3\\), then \\([v_1,v_2,v_3] = 0\\).\nThought Experiment: Why does antisymmetry naturally encode the geometric idea of orientation? How does this link back to the sign of a determinant?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-8.-wedge-products-and-k-vectors",
    "href": "index.html#chapter-8.-wedge-products-and-k-vectors",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 8. Wedge Products and k-Vectors",
    "text": "Chapter 8. Wedge Products and k-Vectors\n\n8.1 The Wedge Product and Geometric Meaning\nWe now begin Part IV: Exterior Algebra, which formalizes antisymmetric tensors into a powerful algebraic system. The key operation here is the wedge product (\\(\\wedge\\)).\n\nDefinition of the Wedge Product\nGiven two vectors \\(u, v \\in V\\), their wedge product is:\n\\[\nu \\wedge v = u \\otimes v - v \\otimes u.\n\\]\n\nIt is bilinear: linear in each input.\nIt is antisymmetric: \\(u \\wedge v = -v \\wedge u\\).\nIt vanishes if \\(u\\) and \\(v\\) are linearly dependent.\n\n\n\nGeometric Meaning\n\n\\(u \\wedge v\\) represents the oriented parallelogram spanned by \\(u\\) and \\(v\\).\nThe magnitude corresponds to the area:\n\\[\n\\|u \\wedge v\\| = \\|u\\| \\, \\|v\\| \\, \\sin\\theta,\n\\]\nwhere \\(\\theta\\) is the angle between \\(u\\) and \\(v\\).\nThe sign encodes orientation.\n\n\n\nHigher Wedge Products\nFor \\(k\\) vectors \\(u_1, u_2, \\dots, u_k\\):\n\\[\nu_1 \\wedge u_2 \\wedge \\cdots \\wedge u_k\n\\]\nrepresents the oriented \\(k\\)-dimensional volume element (parallelepiped).\n\nAntisymmetry ensures the wedge vanishes if the vectors are linearly dependent.\nThus, wedge products capture the idea of dimension of span:\n\nTwo vectors → area.\nThree vectors → volume.\nMore vectors → higher-dimensional volume.\n\n\n\n\nAlgebraic Properties\n\nAnticommutativity: \\(u \\wedge v = -v \\wedge u\\).\nAssociativity (up to sign): \\((u \\wedge v) \\wedge w = u \\wedge (v \\wedge w)\\).\nAlternating Property: \\(u \\wedge u = 0\\).\nDistributivity: \\((u+v) \\wedge w = u \\wedge w + v \\wedge w\\).\n\nTogether, these rules define the exterior algebra \\(\\Lambda(V)\\).\n\n\nWhy This Matters\n\nThe wedge product is the algebraic foundation of determinants, areas, and volumes.\nIt provides a coordinate-free way to work with oriented geometry.\nIn advanced topics, wedge products lead directly to differential forms and integration on manifolds.\n\n\n\nExercises\n\nBasic Wedge: Compute \\((1,0) \\wedge (0,1)\\) in \\(\\mathbb{R}^2\\). What does it represent?\nAntisymmetry Check: Show that \\((1,2,3) \\wedge (4,5,6) = - (4,5,6) \\wedge (1,2,3)\\).\nArea via Wedge: Use wedge products to find the area of the parallelogram spanned by \\(u=(2,0)\\) and \\(v=(1,3)\\).\nZero Wedge: Show that \\(u \\wedge v = 0\\) when \\(u=(1,2)\\), \\(v=(2,4)\\). Interpret geometrically.\nThought Experiment: Why does antisymmetry make wedge products vanish for linearly dependent vectors? How does this encode the idea of dimension reduction?\n\n\n\n\n8.2 Areas, Volumes, Orientation, Determinant\nNow that we’ve introduced the wedge product, we can connect it directly to geometry: areas, volumes, and orientation. The wedge product is the algebraic language for these concepts.\n\n2D: Areas from the Wedge Product\nFor vectors \\(u, v \\in \\mathbb{R}^2\\), the wedge product \\(u \\wedge v\\) represents the oriented area of the parallelogram they span.\n\nAlgebraically:\n\\[\nu \\wedge v = \\det \\begin{bmatrix} u_1 & v_1 \\\\ u_2 & v_2 \\end{bmatrix}.\n\\]\nGeometric meaning: \\(|u \\wedge v|\\) is the area, the sign encodes clockwise vs. counterclockwise orientation.\n\n\n\n3D: Volumes via Triple Wedge\nFor \\(u, v, w \\in \\mathbb{R}^3\\):\n\\[\nu \\wedge v \\wedge w\n\\]\nrepresents the oriented volume of the parallelepiped spanned by them.\n\nAlgebraically:\n\\[\nu \\wedge v \\wedge w = \\det \\begin{bmatrix} u_1 & v_1 & w_1 \\\\ u_2 & v_2 & w_2 \\\\ u_3 & v_3 & w_3 \\end{bmatrix}.\n\\]\nGeometric meaning: \\(|u \\wedge v \\wedge w|\\) is the volume, the sign encodes right- vs. left-handed orientation.\n\n\n\nGeneral \\(n\\)-Dimensional Case\nFor \\(n\\) vectors \\(v_1, \\dots, v_n \\in \\mathbb{R}^n\\):\n\\[\nv_1 \\wedge v_2 \\wedge \\cdots \\wedge v_n = \\det(v_1, v_2, \\dots, v_n),\n\\]\nthe determinant of the matrix with those vectors as columns.\n\nMagnitude: \\(n\\)-dimensional volume of the parallelepiped.\nSign: orientation (positive if vectors preserve orientation, negative if they reverse it).\n\n\n\nOrientation\n\nOrientation is the choice of “handedness” (right-hand vs. left-hand rule).\nThe wedge product encodes orientation automatically via antisymmetry.\nSwapping two vectors flips the sign, meaning the orientation is reversed.\n\n\n\nWhy This Matters\n\nWedge products generalize determinants to all dimensions.\nThey provide a uniform way to compute areas, volumes, and higher-dimensional volumes.\nOrientation, crucial in geometry and physics, is naturally handled through antisymmetry.\n\n\n\nExercises\n\nArea in 2D: Compute the oriented area of the parallelogram spanned by \\(u=(1,2)\\), \\(v=(3,4)\\) using \\(u \\wedge v\\).\nVolume in 3D: Compute \\(u \\wedge v \\wedge w\\) for \\(u=(1,0,0), v=(0,2,0), w=(0,0,3)\\). Interpret geometrically.\nOrientation Flip: Show that \\(u \\wedge v = -v \\wedge u\\) changes the orientation of the area.\nDeterminant Link: Verify that \\(u \\wedge v \\wedge w\\) in \\(\\mathbb{R}^3\\) equals \\(\\det[u,v,w]\\).\nThought Experiment: Why does the wedge product vanish if the set of vectors is linearly dependent? What does this mean geometrically for areas/volumes?\n\n\n\n\n8.3 Basis, Dimension, Grassmann Algebra\nWe’ve seen that wedge products describe oriented areas, volumes, and higher-dimensional analogues. To make this systematic, we construct exterior powers of a vector space, also known as Grassmann algebras.\n\nExterior Powers \\(\\Lambda^k(V)\\)\nFor a vector space \\(V\\):\n\n\\(\\Lambda^k(V)\\) is the space of all antisymmetric \\(k\\)-tensors (wedge products of \\(k\\) vectors).\n\\(\\Lambda^0(V) \\cong \\mathbb{R}\\) (scalars).\n\\(\\Lambda^1(V) \\cong V\\) (vectors).\n\\(\\Lambda^2(V)\\): oriented areas.\n\\(\\Lambda^3(V)\\): oriented volumes.\nIn general, \\(\\Lambda^k(V)\\) encodes oriented \\(k\\)-dimensional “volume elements.”\n\n\n\nBasis of \\(\\Lambda^k(V)\\)\nSuppose \\(V\\) has dimension \\(n\\) with basis \\(e_1, e_2, \\dots, e_n\\).\n\nA basis for \\(\\Lambda^k(V)\\) is given by wedge products:\n\\[\ne_{i_1} \\wedge e_{i_2} \\wedge \\cdots \\wedge e_{i_k}, \\quad i_1 &lt; i_2 &lt; \\cdots &lt; i_k.\n\\]\nThese are linearly independent and span all of \\(\\Lambda^k(V)\\).\n\n\n\nDimension Formula\nThe dimension of \\(\\Lambda^k(V)\\) is:\n\\[\n\\dim \\Lambda^k(V) = \\binom{n}{k}.\n\\]\n\nExample: If \\(n=4\\), then \\(\\dim \\Lambda^2(V) = \\binom{4}{2} = 6\\).\nThis matches the number of independent ways to choose \\(k\\) basis vectors from \\(n\\).\n\n\n\nGrassmann (Exterior) Algebra\nThe direct sum of all exterior powers:\n\\[\n\\Lambda(V) = \\Lambda^0(V) \\oplus \\Lambda^1(V) \\oplus \\cdots \\oplus \\Lambda^n(V),\n\\]\nis called the exterior algebra (or Grassmann algebra).\n\nMultiplication in this algebra is given by the wedge product.\nAnticommutativity ensures \\(u \\wedge u = 0\\).\nThis makes \\(\\Lambda(V)\\) into a graded algebra, organized by degree.\n\n\n\nWhy This Matters\n\nExterior algebra gives a systematic framework for working with antisymmetric tensors.\nThe dimension formula explains why areas, volumes, etc. fit neatly into subspaces.\nGrassmann algebra underlies modern geometry, topology, and physics (differential forms, integration on manifolds).\n\n\n\nExercises\n\nBasis in \\(\\Lambda^2\\): For \\(V = \\mathbb{R}^3\\) with basis \\((e_1,e_2,e_3)\\), list a basis of \\(\\Lambda^2(V)\\).\nDimension Count: If \\(\\dim V = 5\\), compute \\(\\dim \\Lambda^2(V)\\), \\(\\dim \\Lambda^3(V)\\), and \\(\\dim \\Lambda^4(V)\\).\nSimple Wedge Expansion: In \\(\\mathbb{R}^3\\), expand \\((e_1 + e_2) \\wedge e_3\\).\nGrassmann Algebra Structure: Identify which degrees (\\(k\\)) of \\(\\Lambda^k(V)\\) correspond to:\n\nscalars,\nvectors,\noriented areas,\noriented volumes.\n\nThought Experiment: Why does the formula \\(\\dim \\Lambda^k(V) = \\binom{n}{k}\\) match the combinatorial idea of “choosing \\(k\\) directions from \\(n\\)”?\n\n\n\n\n8.4 Differential Forms (Gentle Preview)\nWe close Part IV with a glimpse of how wedge products extend beyond linear algebra into calculus on manifolds. This leads to differential forms, the language of modern geometry and physics.\n\nDifferential 1-Forms\n\nA 1-form is a covector field: at each point, it eats a vector and gives a number.\nExample in \\(\\mathbb{R}^2\\):\n\\[\n\\omega = x \\, dy - y \\, dx,\n\\]\nwhere \\(dx, dy\\) are basis 1-forms.\n\n\n\nHigher-Degree Forms\n\nA k-form is an antisymmetric multilinear map that takes \\(k\\) vectors (directions) and outputs a scalar.\nBuilt using wedge products of 1-forms:\n\n\\(dx \\wedge dy\\) is a 2-form (measures oriented area in the \\(xy\\)-plane).\n\\(dx \\wedge dy \\wedge dz\\) is a 3-form (measures oriented volume in \\(\\mathbb{R}^3\\)).\n\n\n\n\nExterior Derivative \\(d\\)\nDifferential forms come with a derivative operator \\(d\\):\n\nTakes a \\(k\\)-form to a \\((k+1)\\)-form.\nGeneralizes gradient, curl, and divergence in one unified framework.\nExample:\n\nFor \\(f(x,y)\\) a scalar function (0-form):\n\\[\ndf = \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy,\n\\]\nwhich is the gradient written as a 1-form.\n\n\n\n\nIntegration of Forms\n\nDifferential forms can be integrated over curves, surfaces, and higher-dimensional manifolds.\nExample: integrating a 1-form along a curve gives the work done by a force field.\nIntegrating a 2-form over a surface gives the flux through the surface.\n\n\n\nWhy This Matters\n\nDifferential forms unify multivariable calculus (gradient, curl, divergence) into one elegant theory.\nThey are the foundation of Stokes’ theorem, which generalizes all the fundamental theorems of calculus.\nPhysics: electromagnetism, fluid dynamics, and general relativity all use differential forms.\n\n\n\nExercises\n\n1-Form Action: Let \\(\\omega = 3dx + 2dy\\). Evaluate \\(\\omega(v)\\) for \\(v = (1,4)\\).\nWedge of 1-Forms: Compute \\(dx \\wedge dy(v,w)\\) for \\(v=(1,0), w=(0,2)\\).\nExterior Derivative: For \\(f(x,y) = x^2 y\\), compute \\(df\\).\nIntegration Example: Interpret \\(\\int_\\gamma (y \\, dx)\\) along a path \\(\\gamma\\) in \\(\\mathbb{R}^2\\). What does it represent physically?\nThought Experiment: How does the wedge product in linear algebra naturally lead into differential forms in calculus?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-9.-hodge-dual-and-metrics-optional-but-useful",
    "href": "index.html#chapter-9.-hodge-dual-and-metrics-optional-but-useful",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 9. Hodge Dual and Metrics (Optional but Useful)",
    "text": "Chapter 9. Hodge Dual and Metrics (Optional but Useful)\n\n9.1 Inner Products on Exterior Powers\nTo use wedge products effectively, we need a way to measure their size and compare them. This is where inner products on exterior powers come in. They extend the familiar dot product on vectors to areas, volumes, and higher-dimensional elements.\n\nInduced Inner Product\nSuppose \\(V\\) has an inner product \\(\\langle \\cdot, \\cdot \\rangle\\). We can extend it to \\(\\Lambda^k(V)\\) by declaring wedge products of basis vectors to be orthonormal:\n\\[\n\\langle e_{i_1} \\wedge \\cdots \\wedge e_{i_k}, \\, e_{j_1} \\wedge \\cdots \\wedge e_{j_k} \\rangle =\n\\begin{cases}\n\\det(\\delta_{i_a j_b}) & \\text{if sets are equal}, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]\nIn simpler words:\n\nTake all ordered \\(k\\)-tuples of basis vectors.\nThe wedge products of these are orthonormal if the underlying sets match.\n\n\n\nExample in \\(\\mathbb{R}^2\\)\nLet \\(e_1, e_2\\) be an orthonormal basis. Then\n\\[\n\\langle e_1 \\wedge e_2, e_1 \\wedge e_2 \\rangle = 1.\n\\]\nSo \\(e_1 \\wedge e_2\\) has unit length, representing a unit square area.\n\n\nExample in \\(\\mathbb{R}^3\\)\nLet \\(e_1, e_2, e_3\\) be orthonormal. Then\n\n\\(e_1 \\wedge e_2, e_1 \\wedge e_3, e_2 \\wedge e_3\\) form an orthonormal basis of \\(\\Lambda^2(\\mathbb{R}^3)\\).\n\\(e_1 \\wedge e_2 \\wedge e_3\\) is the unit element of \\(\\Lambda^3(\\mathbb{R}^3)\\), representing a unit cube volume.\n\n\n\nNorms of Wedge Products\nFor \\(u,v \\in V\\):\n\\[\n\\|u \\wedge v\\|^2 = \\|u\\|^2 \\|v\\|^2 - \\langle u,v \\rangle^2.\n\\]\nThis is the formula for the area of a parallelogram spanned by \\(u,v\\).\n\nIf \\(u,v\\) are orthogonal: area = product of lengths.\nIf \\(u,v\\) are parallel: area = 0.\n\nFor higher \\(k\\), the squared norm of \\(u_1 \\wedge \\cdots \\wedge u_k\\) equals the determinant of the Gram matrix \\([\\langle u_i,u_j \\rangle]\\).\n\n\nWhy This Matters\n\nInner products on exterior powers connect wedge products to geometry quantitatively (areas, volumes).\nThey unify length, area, and volume measurement into one consistent framework.\nThis lays the groundwork for the Hodge star operator, which depends on having an inner product.\n\n\n\nExercises\n\nArea Formula: Compute \\(\\|u \\wedge v\\|\\) for \\(u=(1,0), v=(1,1)\\) in \\(\\mathbb{R}^2\\).\nGram Determinant: Show that \\(\\|u \\wedge v\\|^2 = \\det \\begin{bmatrix} \\langle u,u \\rangle & \\langle u,v \\rangle \\\\ \\langle v,u \\rangle & \\langle v,v \\rangle \\end{bmatrix}\\).\n3D Basis Check: Verify that \\(e_1 \\wedge e_2, e_1 \\wedge e_3, e_2 \\wedge e_3\\) are orthonormal in \\(\\Lambda^2(\\mathbb{R}^3)\\).\nVolume Norm: Compute the norm of \\(e_1 \\wedge e_2 \\wedge (e_1+e_3)\\). Interpret geometrically.\nThought Experiment: Why does the Gram determinant naturally appear in wedge product norms? How does this connect to the idea of measuring volume via linear independence?\n\n\n\n\n9.2 Hodge Star, Pseudo-Vectors, Cross Products\nWith an inner product defined on exterior powers, we can now introduce the Hodge star operator (\\(\\star\\)), which creates a bridge between \\(k\\)-forms and \\((n-k)\\)-forms. This operator encodes geometric duality, and in 3D it gives rise to the familiar cross product.\n\nThe Hodge Star Operator\nLet \\(V\\) be an \\(n\\)-dimensional inner product space with an orientation. The Hodge star is a linear map:\n\\[\n\\star : \\Lambda^k(V) \\to \\Lambda^{n-k}(V).\n\\]\nIt is defined so that for all \\(\\alpha, \\beta \\in \\Lambda^k(V)\\):\n\\[\n\\alpha \\wedge \\star \\beta = \\langle \\alpha, \\beta \\rangle \\, \\mathrm{vol},\n\\]\nwhere \\(\\mathrm{vol}\\) is the unit volume element in \\(\\Lambda^n(V)\\).\n\n\nExamples in \\(\\mathbb{R}^3\\)\nWith orthonormal basis \\(e_1, e_2, e_3\\):\n\n\\(\\star e_1 = e_2 \\wedge e_3\\).\n\\(\\star (e_1 \\wedge e_2) = e_3\\).\n\\(\\star (e_1 \\wedge e_2 \\wedge e_3) = 1\\).\n\nSo the star turns 1-vectors into 2-forms, 2-forms into 1-vectors, and the 3-form into a scalar.\n\n\nCross Product as Hodge Dual\nIn \\(\\mathbb{R}^3\\), the cross product \\(u \\times v\\) can be expressed using the Hodge star:\n\\[\nu \\times v = \\star (u \\wedge v).\n\\]\n\n\\(u \\wedge v\\) represents the oriented area spanned by \\(u,v\\).\nThe Hodge star converts that 2-form into the unique vector perpendicular to the plane, with magnitude equal to the area.\n\nThus, the cross product is not a primitive concept - it is the Hodge star of a wedge product.\n\n\nPseudo-Vectors\n\nObjects like angular momentum or magnetic field transform like vectors, but with an orientation twist.\nThese are pseudo-vectors, naturally arising from antisymmetric 2-forms in \\(\\mathbb{R}^3\\).\nThe Hodge star provides the link between antisymmetric 2-forms and pseudo-vectors.\n\n\n\nWhy This Matters\n\nThe Hodge star encodes geometric duality between subspaces and their complements.\nIt explains the origin of the cross product in 3D and why it does not generalize to higher dimensions.\nIt sets the stage for physics applications: electromagnetism, fluid dynamics, and general relativity all use the Hodge dual.\n\n\n\nExercises\n\nBasic Star: In \\(\\mathbb{R}^3\\), compute \\(\\star (e_1)\\), \\(\\star (e_2)\\), and \\(\\star (e_3)\\).\nStar on 2-Forms: Verify that \\(\\star (e_1 \\wedge e_2) = e_3\\) in \\(\\mathbb{R}^3\\).\nCross Product via Star: Compute \\(u \\times v\\) using \\(u \\times v = \\star (u \\wedge v)\\) for \\(u=(1,0,0), v=(0,1,0)\\).\nStar Squared: Show that in \\(\\mathbb{R}^3\\), applying the star twice gives \\(\\star \\star \\alpha = \\alpha\\) for 1-forms.\nThought Experiment: Why does the cross product only exist in \\(\\mathbb{R}^3\\) (and in a special sense in \\(\\mathbb{R}^7\\))? How does the Hodge star viewpoint clarify this?\n\n\n\n\n9.3 Volume Forms and Integration Glimpses\nWe now tie together the Hodge star, wedge products, and inner products to introduce the concept of volume forms - the mathematical tools that let us integrate over spaces of any dimension.\n\nVolume Form\nIn an \\(n\\)-dimensional oriented inner product space \\(V\\), the volume form is the unit \\(n\\)-form:\n\\[\n\\mathrm{vol} = e_1 \\wedge e_2 \\wedge \\cdots \\wedge e_n,\n\\]\nwhere \\(\\{e_i\\}\\) is an orthonormal positively oriented basis.\n\nIt represents the “unit \\(n\\)-dimensional volume element.”\nAny \\(n\\) vectors \\(v_1,\\dots,v_n\\) give\n\\[\n\\mathrm{vol}(v_1,\\dots,v_n) = \\det[v_1 \\ \\cdots \\ v_n],\n\\]\ni.e. their oriented volume.\n\n\n\nVolume via Hodge Star\nFor a \\(k\\)-form \\(\\alpha\\), its dual \\(\\star \\alpha\\) is an \\((n-k)\\)-form.\n\nThe wedge \\(\\alpha \\wedge \\star \\alpha\\) is proportional to \\(\\mathrm{vol}\\).\nThis expresses how much “volume” \\(\\alpha\\) contributes in \\(n\\)-dimensions.\n\nExample in \\(\\mathbb{R}^3\\):\n\nIf \\(\\alpha = u \\wedge v\\), then\n\\[\n\\alpha \\wedge \\star \\alpha = \\|u \\wedge v\\|^2 \\, \\mathrm{vol},\n\\]\nencoding the squared area of the parallelogram spanned by \\(u,v\\).\n\n\n\nIntegration of Differential Forms (Glimpse)\nThe volume form allows us to integrate over manifolds:\n\nIn \\(\\mathbb{R}^n\\), integrating \\(\\mathrm{vol}\\) over a region gives its volume.\nA \\(k\\)-form can be integrated over a \\(k\\)-dimensional surface (curve, area, etc.).\nExample:\n\nIn \\(\\mathbb{R}^2\\), \\(\\mathrm{vol} = dx \\wedge dy\\).\nThe integral \\(\\int_\\Omega dx \\wedge dy\\) is the area of region \\(\\Omega\\).\n\n\nThis is the seed of Stokes’ theorem, which unifies the fundamental theorem of calculus, Green’s theorem, and the divergence theorem.\n\n\nWhy This Matters\n\nVolume forms make precise the link between algebra (determinants) and geometry (volume).\nThey are the foundation for integration on curved spaces (manifolds).\nIn physics, volume forms appear in conservation laws, flux integrals, and field theories.\n\n\n\nExercises\n\n2D Volume Form: Show that for \\(u=(1,0), v=(0,2)\\) in \\(\\mathbb{R}^2\\): \\(\\mathrm{vol}(u,v) = \\det \\begin{bmatrix}1 & 0 \\\\ 0 & 2\\end{bmatrix}\\).\n3D Example: Compute \\(\\mathrm{vol}(u,v,w)\\) for \\(u=(1,0,0), v=(0,1,0), w=(0,0,2)\\).\nStar Relation: In \\(\\mathbb{R}^3\\), verify that \\((u \\wedge v) \\wedge \\star (u \\wedge v) = \\|u \\wedge v\\|^2 \\, \\mathrm{vol}\\).\nIntegration Preview: Evaluate \\(\\int_{[0,1]\\times [0,1]} dx \\wedge dy\\). Interpret the result.\nThought Experiment: Why is orientation essential when defining a volume form? What would go wrong in integration if we ignored orientation?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-10.-symmetric-powers-and-homogeneous-polynomials",
    "href": "index.html#chapter-10.-symmetric-powers-and-homogeneous-polynomials",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 10. Symmetric Powers and Homogeneous Polynomials",
    "text": "Chapter 10. Symmetric Powers and Homogeneous Polynomials\n\n10.1 Symmetric Tensors as Polynomial Coefficients\nSo far, we have studied antisymmetric tensors and exterior algebra. Now we turn to the opposite case: symmetric tensors. These are equally important, and they connect directly to polynomials.\n\nSymmetric Tensors\nA symmetric tensor of order \\(k\\) satisfies:\n\\[\nT_{i_1 i_2 \\dots i_k} = T_{i_{\\pi(1)} i_{\\pi(2)} \\dots i_{\\pi(k)}}\n\\]\nfor any permutation \\(\\pi\\).\nExample:\n\nA quadratic form \\(Q(x) = x^\\top A x\\) comes from a symmetric 2-tensor.\nHigher-order symmetric tensors generalize this to cubic, quartic, etc.\n\n\n\nFrom Symmetric Tensors to Polynomials\nEvery symmetric \\(k\\)-tensor corresponds to a homogeneous polynomial of degree \\(k\\).\n\nGiven a symmetric tensor \\(T_{i_1 \\dots i_k}\\):\n\\[\np(x) = T_{i_1 \\dots i_k} x^{i_1} \\cdots x^{i_k}.\n\\]\nExample: In \\(\\mathbb{R}^2\\), let \\(T_{11}=1, T_{12}=T_{21}=2, T_{22}=3\\). Then\n\\[\np(x,y) = x^2 + 4xy + 3y^2.\n\\]\n\nThis is a homogeneous polynomial of degree 2.\n\n\nFrom Polynomials to Symmetric Tensors\nConversely, every homogeneous polynomial defines a symmetric tensor:\n\nExample: \\(p(x,y,z) = 2x^2z + 3yz^2\\).\nIts coefficients directly correspond to entries of a symmetric 3-tensor.\n\nThus, symmetric tensors and homogeneous polynomials are two sides of the same coin.\n\n\nWhy This Matters\n\nThis correspondence makes symmetric tensors central in algebraic geometry and statistics.\nMoments of probability distributions are symmetric tensors.\nPolynomial optimization problems can be rephrased in terms of symmetric tensors.\n\n\n\nExercises\n\nQuadratic Form: Show that the polynomial \\(p(x,y) = 3x^2 + 2xy + y^2\\) corresponds to a symmetric 2-tensor. Write its matrix.\nCubic Example: Write the symmetric 3-tensor for \\(p(x,y) = x^3 + 3x^2y + 2y^3\\).\nBackwards: Given the symmetric tensor with entries \\(T_{111}=1, T_{112}=2, T_{122}=3, T_{222}=4\\), write the polynomial in two variables.\nHomogeneity Check: Explain why the polynomial \\(x^2 + xy + y^2\\) is homogeneous of degree 2, but \\(x^2 + y + 1\\) is not.\nThought Experiment: Why might it be useful to switch between tensor and polynomial viewpoints in machine learning or physics? ### 10.2 Polarization Identities\n\nWe saw that symmetric tensors and homogeneous polynomials are two sides of the same coin. But how exactly do we go from one to the other in a precise way? The tool for this is the polarization identity, which allows us to reconstruct a symmetric multilinear map (or tensor) from its associated polynomial.\n\n\nFrom Symmetric Tensor to Polynomial\nIf \\(T\\) is a symmetric \\(k\\)-linear form on \\(V\\), we define the polynomial\n\\[\np(x) = T(x,x,\\dots,x).\n\\]\nThis is a homogeneous polynomial of degree \\(k\\).\nExample:\n\nIf \\(T(x,y) = \\langle x,y \\rangle\\), then \\(p(x) = \\langle x,x \\rangle = \\|x\\|^2\\).\n\n\n\nFrom Polynomial to Symmetric Tensor\nGiven a homogeneous polynomial \\(p(x)\\) of degree \\(k\\), we want to recover the symmetric tensor \\(T\\).\nThe polarization identity says:\n\\[\nT(x_1, \\dots, x_k) = \\frac{1}{k! \\, 2^k} \\sum_{\\epsilon_1,\\dots,\\epsilon_k = \\pm 1} \\epsilon_1 \\cdots \\epsilon_k \\, p(\\epsilon_1 x_1 + \\cdots + \\epsilon_k x_k).\n\\]\nThis formula extracts the multilinear coefficients hidden inside the polynomial.\n\n\nExample: Quadratic Forms\nFor \\(p(x) = \\|x\\|^2\\):\n\nUsing polarization, we recover the bilinear form:\n\\[\nT(x,y) = \\tfrac{1}{2} \\big( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\big) = \\langle x,y \\rangle.\n\\]\n\nThis is the classical polarization identity for inner products.\n\n\nWhy This Matters\n\nThe polarization identity ensures that the correspondence between symmetric tensors and homogeneous polynomials is exact and invertible.\nIt provides a way to compute multilinear structure from polynomial data.\nIn applications:\n\nIn physics, energy polynomials give rise to symmetric stress tensors.\nIn statistics, cumulants and moments correspond to symmetric tensors recovered from generating functions.\n\n\n\n\nExercises\n\nInner Product Recovery: Use the quadratic polarization identity to show that\n\\[\n\\langle x,y \\rangle = \\tfrac{1}{4} \\big( \\|x+y\\|^2 - \\|x-y\\|^2 \\big).\n\\]\nCubic Case: Let \\(p(x) = x_1^3\\) in \\(\\mathbb{R}^2\\). Use the polarization formula to compute \\(T(e_1,e_1,e_1)\\) and \\(T(e_1,e_1,e_2)\\).\nVerification: For \\(p(x,y) = x^2 + y^2\\), verify that the polarization identity recovers the symmetric bilinear form \\(T(x,y) = x_1y_1 + x_2y_2\\).\nHomogeneity Check: Show why polarization fails if \\(p(x)\\) is not homogeneous.\nThought Experiment: Why is it important that the tensor be symmetric in order for the polynomial ↔︎ tensor correspondence to work smoothly?\n\n\n\n\n10.2 Polarization Identities\nWe saw that symmetric tensors and homogeneous polynomials are two sides of the same coin. But how exactly do we go from one to the other in a precise way? The tool for this is the polarization identity, which allows us to reconstruct a symmetric multilinear map (or tensor) from its associated polynomial.\n\nFrom Symmetric Tensor to Polynomial\nIf \\(T\\) is a symmetric \\(k\\)-linear form on \\(V\\), we define the polynomial\n\\[\np(x) = T(x,x,\\dots,x).\n\\]\nThis is a homogeneous polynomial of degree \\(k\\).\nExample:\n\nIf \\(T(x,y) = \\langle x,y \\rangle\\), then \\(p(x) = \\langle x,x \\rangle = \\|x\\|^2\\).\n\n\n\nFrom Polynomial to Symmetric Tensor\nGiven a homogeneous polynomial \\(p(x)\\) of degree \\(k\\), we want to recover the symmetric tensor \\(T\\).\nThe polarization identity says:\n\\[\nT(x_1, \\dots, x_k) = \\frac{1}{k! \\, 2^k} \\sum_{\\epsilon_1,\\dots,\\epsilon_k = \\pm 1} \\epsilon_1 \\cdots \\epsilon_k \\, p(\\epsilon_1 x_1 + \\cdots + \\epsilon_k x_k).\n\\]\nThis formula extracts the multilinear coefficients hidden inside the polynomial.\n\n\nExample: Quadratic Forms\nFor \\(p(x) = \\|x\\|^2\\):\n\nUsing polarization, we recover the bilinear form:\n\\[\nT(x,y) = \\tfrac{1}{2} \\big( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\big) = \\langle x,y \\rangle.\n\\]\n\nThis is the classical polarization identity for inner products.\n\n\nWhy This Matters\n\nThe polarization identity ensures that the correspondence between symmetric tensors and homogeneous polynomials is exact and invertible.\nIt provides a way to compute multilinear structure from polynomial data.\nIn applications:\n\nIn physics, energy polynomials give rise to symmetric stress tensors.\nIn statistics, cumulants and moments correspond to symmetric tensors recovered from generating functions.\n\n\n\n\nExercises\n\nInner Product Recovery: Use the quadratic polarization identity to show that\n\\[\n\\langle x,y \\rangle = \\tfrac{1}{4} \\big( \\|x+y\\|^2 - \\|x-y\\|^2 \\big).\n\\]\nCubic Case: Let \\(p(x) = x_1^3\\) in \\(\\mathbb{R}^2\\). Use the polarization formula to compute \\(T(e_1,e_1,e_1)\\) and \\(T(e_1,e_1,e_2)\\).\nVerification: For \\(p(x,y) = x^2 + y^2\\), verify that the polarization identity recovers the symmetric bilinear form \\(T(x,y) = x_1y_1 + x_2y_2\\).\nHomogeneity Check: Show why polarization fails if \\(p(x)\\) is not homogeneous.\nThought Experiment: Why is it important that the tensor be symmetric in order for the polynomial ↔︎ tensor correspondence to work smoothly?\n\n\n\n\n10.3 Moments and Cumulants\nSymmetric tensors are not just abstract objects - they naturally appear in probability and statistics. Two key concepts, moments and cumulants, can be represented as symmetric tensors, providing a multilinear view of random variables and their dependencies.\n\nMoments as Symmetric Tensors\nFor a random vector \\(X \\in \\mathbb{R}^n\\), the \\(k\\)-th moment tensor is:\n\\[\nM^{(k)} = \\mathbb{E}[X^{\\otimes k}],\n\\]\nwhere\n\\[\nX^{\\otimes k} = X \\otimes X \\otimes \\cdots \\otimes X \\quad (k \\text{ times}).\n\\]\n\nEntries:\n\\[\n(M^{(k)})_{i_1 i_2 \\dots i_k} = \\mathbb{E}[X_{i_1} X_{i_2} \\cdots X_{i_k}].\n\\]\nSince multiplication is commutative, \\(M^{(k)}\\) is a symmetric tensor.\n\nExamples:\n\n\\(M^{(1)}\\) = mean vector.\n\\(M^{(2)}\\) = second moment matrix (related to covariance).\nHigher-order moments capture skewness, kurtosis, etc.\n\n\n\nCumulants as Symmetric Tensors\nCumulants refine moments by removing redundancy:\n\nDefined via the log of the moment-generating function.\nThe \\(k\\)-th cumulant tensor \\(C^{(k)}\\) is also symmetric.\nExample:\n\n\\(C^{(1)}\\) = mean.\n\\(C^{(2)}\\) = covariance matrix.\n\\(C^{(3)}\\) = skewness tensor.\n\\(C^{(4)}\\) = kurtosis tensor.\n\n\n\n\nWhy Tensors?\n\nMoments and cumulants encode multi-way dependencies between variables.\nAs tensors, they can be studied with linear algebra and decompositions.\nApplications:\n\nSignal processing: blind source separation via cumulant tensors.\nStatistics: identifying distributions via moment tensors.\nMachine learning: feature extraction from higher-order statistics.\n\n\n\n\nConnection to Symmetric Polynomials\n\nMoments correspond to coefficients of homogeneous polynomials (moment generating functions).\nPolarization identities let us move between the polynomial representation and the tensor form.\n\n\n\nExercises\n\nSecond Moment: Let \\(X = (X_1,X_2)\\) with \\(\\mathbb{E}[X_1^2]=2, \\mathbb{E}[X_1X_2]=1, \\mathbb{E}[X_2^2]=3\\). Write the 2nd moment tensor \\(M^{(2)}\\).\nSymmetry Check: Show that \\((M^{(3)})_{ijk} = \\mathbb{E}[X_i X_j X_k]\\) is symmetric in all indices.\nCovariance as Cumulant: Explain why the covariance matrix is the 2nd cumulant tensor \\(C^{(2)}\\).\nHigher Cumulants: What does \\(C^{(3)}\\) measure that is not captured by \\(C^{(2)}\\)?\nThought Experiment: Why might cumulants be more useful than moments in separating independent signals or detecting non-Gaussianity?\n\n\n\n\n10.4 Low-Rank Symmetric Decompositions\nSymmetric tensors, like general tensors, can be decomposed into simpler parts. In the symmetric case, this often means writing a symmetric tensor as a sum of outer products of vectors with themselves. These low-rank symmetric decompositions play a central role in data science, signal processing, and machine learning.\n\nSymmetric Rank\nFor a symmetric tensor \\(T \\in \\mathrm{Sym}^k(V)\\), the symmetric rank is the smallest \\(r\\) such that\n\\[\nT = \\sum_{i=1}^r \\lambda_i \\, v_i^{\\otimes k},\n\\]\nwhere \\(v_i \\in V\\) and \\(\\lambda_i \\in \\mathbb{R}\\).\n\nEach term \\(v_i^{\\otimes k} = v_i \\otimes v_i \\otimes \\cdots \\otimes v_i\\) (k times) is a simple symmetric tensor.\nThis is the symmetric analogue of the rank-one decomposition for matrices.\n\n\n\nExample: Quadratic Forms\nA quadratic form (symmetric 2-tensor) can be decomposed as:\n\\[\nQ(x) = \\sum_{i=1}^r \\lambda_i (v_i^\\top x)^2.\n\\]\nThis is the spectral decomposition of a symmetric matrix.\n\n\nHigher-Order Case\nFor cubic or quartic symmetric tensors:\n\nExample in 3rd order:\n\\[\nT(x,y,z) = \\sum_{i=1}^r \\lambda_i (v_i^\\top x)(v_i^\\top y)(v_i^\\top z).\n\\]\nThis is the CP decomposition restricted to the symmetric case.\n\n\n\nApplications\n\nStatistics: cumulant tensors often admit low-rank symmetric decompositions, useful in blind source separation.\nMachine learning: compressing polynomial kernels, tensor regression, deep learning model compression.\nAlgebraic geometry: the study of Waring decompositions of polynomials.\n\n\n\nWhy This Matters\n\nLow-rank symmetric decompositions reduce complexity, making huge tensors manageable.\nThey connect tensor algebra with classical spectral theory and polynomial factorization.\nThey explain why symmetric tensors are a natural language for representing data with latent low-dimensional structure.\n\n\n\nExercises\n\nMatrix Case: Decompose\n\\[\nA = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\n\\]\nas a sum of symmetric rank-one matrices.\nSimple Symmetric Tensor: Write explicitly \\(v^{\\otimes 3}\\) for \\(v=(1,2)\\).\nSymmetric Decomposition: Express the quadratic form \\(Q(x,y) = 4x^2 + 2xy + y^2\\) as a sum of rank-one terms \\(\\lambda_i (a_i x + b_i y)^2\\).\nInterpretation: For a 3rd-order symmetric tensor in \\(\\mathbb{R}^2\\), how many coefficients does it have? Compare this with how many are needed to specify a rank-1 symmetric decomposition.\nThought Experiment: Why might low-rank symmetric decompositions be especially valuable in machine learning (hint: think compression and interpretability)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-11.-reshaping-vectorization-and-commutation",
    "href": "index.html#chapter-11.-reshaping-vectorization-and-commutation",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 11. Reshaping, Vectorization, and Commutation",
    "text": "Chapter 11. Reshaping, Vectorization, and Commutation\n\n11.1 Mode-n Unfolding and Matricization\nWhen working with higher-order tensors, it is often useful to “flatten” them into matrices so that standard linear algebra tools can be applied. This process is called unfolding or matricization.\n\nDefinition: Mode-n Unfolding\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), the mode-n unfolding arranges all entries into a matrix where:\n\nThe row index corresponds to the \\(n\\)-th mode.\nThe column index corresponds to all other modes combined.\n\nFormally:\n\\[\nT_{i_1 i_2 \\dots i_N} \\quad \\mapsto \\quad T^{(n)}_{i_n, j},\n\\]\nwhere \\(j\\) encodes the multi-index \\((i_1,\\dots,i_{n-1},i_{n+1},\\dots,i_N)\\).\n\n\nExample (3rd-Order Tensor)\nIf \\(T\\) has shape \\((I_1,I_2,I_3)\\):\n\nMode-1 unfolding: size \\(I_1 \\times (I_2 I_3)\\).\nMode-2 unfolding: size \\(I_2 \\times (I_1 I_3)\\).\nMode-3 unfolding: size \\(I_3 \\times (I_1 I_2)\\).\n\nThis reshaping preserves all entries, only changing their layout.\n\n\nWhy Unfold?\n\nBrings tensors into a familiar matrix framework.\nEnables the use of matrix decompositions (SVD, QR, eigenvalue methods).\nUsed in algorithms for tensor decompositions (Tucker, CP, HOSVD).\nUseful in machine learning for feature extraction, low-rank approximations, and compression.\n\n\n\nConnection to Linear Maps\nMode-\\(n\\) unfolding allows us to represent how a linear operator acts on the \\(n\\)-th mode of a tensor.\n\nFor example, applying a matrix \\(A \\in \\mathbb{R}^{J \\times I_n}\\) to mode-\\(n\\) corresponds to left-multiplying the unfolded tensor:\n\\[\n(T \\times_n A)^{(n)} = A \\, T^{(n)}.\n\\]\n\nThis makes tensor-matrix products consistent with matrix multiplication.\n\n\nWhy This Matters\n\nMode-n unfolding bridges the gap between multilinear algebra and classical linear algebra.\nIt provides the foundation for defining multilinear rank and tensor decompositions.\nWithout unfolding, many computational algorithms for tensors would be infeasible.\n\n\n\nExercises\n\nShape Check: If \\(T\\) has dimensions \\((2,3,4)\\), what are the shapes of its mode-1, mode-2, and mode-3 unfoldings?\nMatrix Form: Write out explicitly the mode-1 unfolding of a tensor \\(T \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\).\nOperator Action: Let \\(T \\in \\mathbb{R}^{2 \\times 3 \\times 4}\\). If \\(A \\in \\mathbb{R}^{5 \\times 2}\\), what is the shape of \\((T \\times_1 A)\\)?\nReversibility: Explain why unfolding is reversible (i.e., why no information is lost).\nThought Experiment: Why is it advantageous to analyze tensors via unfoldings rather than directly in their multi-index form?\n\n\n\n\n11.2 Vec Operator and Kronecker Identities\nOnce a tensor has been unfolded into a matrix, a common next step is to “flatten” that matrix into a vector. This operation is called vectorization, or the vec operator. It provides a bridge between multilinear algebra and standard linear algebra identities involving Kronecker products.\n\nThe Vec Operator\nFor a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\):\n\\[\n\\mathrm{vec}(A) \\in \\mathbb{R}^{mn}\n\\]\nis obtained by stacking the columns of \\(A\\) into a single vector.\nExample:\n\\[\nA = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}, \\quad\n\\mathrm{vec}(A) = \\begin{bmatrix} a \\\\ c \\\\ b \\\\ d \\end{bmatrix}.\n\\]\nFor higher-order tensors, we apply vec after unfolding into a matrix.\n\n\nKey Identity (Matrix Multiplication)\nFor matrices \\(A, X, B\\) of compatible sizes:\n\\[\n\\mathrm{vec}(AXB^\\top) = (B \\otimes A)\\, \\mathrm{vec}(X).\n\\]\n\nHere, \\(\\otimes\\) is the Kronecker product.\nThis identity rewrites a two-sided matrix multiplication as a single linear transformation.\n\n\n\nTensor Extension\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), linear maps applied along each mode can be expressed in vec form using Kronecker products.\nIf \\(A_n \\in \\mathbb{R}^{J_n \\times I_n}\\), then\n\\[\n(T \\times_1 A_1 \\times_2 A_2 \\cdots \\times_N A_N)^{\\mathrm{vec}} = (A_N \\otimes \\cdots \\otimes A_2 \\otimes A_1) \\, \\mathrm{vec}(T).\n\\]\nThis is the multilinear analogue of the matrix vec identity.\n\n\nApplications\n\nEfficient coding of multilinear transformations.\nProofs of tensor decomposition algorithms.\nNumerical linear algebra (solving tensor equations).\nMachine learning: efficient representation of Kronecker-structured layers.\n\n\n\nWhy This Matters\n\nThe vec operator turns multilinear problems into linear ones.\nKronecker products capture structure in large transformations compactly.\nThese tools make computation with tensors more systematic and efficient.\n\n\n\nExercises\n\nVec of a Matrix: Compute \\(\\mathrm{vec}\\left(\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\\right)\\).\nMatrix Identity Check: Let \\(A = \\begin{bmatrix}1 & 0 \\\\ 0 & 2\\end{bmatrix}, X = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}, B = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}\\). Verify that \\(\\mathrm{vec}(AXB^\\top) = (B \\otimes A) \\, \\mathrm{vec}(X)\\).\nTensor Shape: If \\(T \\in \\mathbb{R}^{2 \\times 3 \\times 4}\\), what is the dimension of \\(\\mathrm{vec}(T)\\)?\nKronecker Ordering: Explain why the order of factors in \\((A_N \\otimes \\cdots \\otimes A_1)\\) matters.\nThought Experiment: Why does vectorization make multilinear algebra easier to connect with existing linear algebra tools?\n\n\n\n\n11.3 Linear Operators Acting on Tensors\nSo far, we have unfolded tensors and vectorized them to connect with matrix operations. Now we look at tensors as natural domains and codomains for linear operators. This viewpoint is powerful because many tensor operations are simply linear maps on tensor product spaces.\n\nOperators on Tensor Product Spaces\nIf \\(A: V \\to V'\\) and \\(B: W \\to W'\\) are linear maps, then their tensor product operator\n\\[\nA \\otimes B : V \\otimes W \\to V' \\otimes W'\n\\]\nis defined by\n\\[\n(A \\otimes B)(v \\otimes w) = (Av) \\otimes (Bw).\n\\]\nThis definition extends linearly to all of \\(V \\otimes W\\).\n\n\nExample with Matrices\nIf \\(A \\in \\mathbb{R}^{m \\times n}\\), \\(B \\in \\mathbb{R}^{p \\times q}\\), and \\(X \\in \\mathbb{R}^{n \\times q}\\):\n\\[\n(A \\otimes B)\\,\\mathrm{vec}(X) = \\mathrm{vec}(BXA^\\top).\n\\]\nThis links tensor product operators to Kronecker product identities.\n\n\nMode-n Multiplication as Operator Action\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}\\), multiplying along the \\(n\\)-th mode by a matrix \\(A \\in \\mathbb{R}^{J \\times I_n}\\):\n\\[\nT' = T \\times_n A,\n\\]\nis equivalent to applying the operator \\(I \\otimes \\cdots \\otimes A \\otimes \\cdots \\otimes I\\) to \\(\\mathrm{vec}(T)\\).\nThus, tensor–matrix products are just specialized cases of linear operators on tensor spaces.\n\n\nHigher-Order Operators\nMore generally, a linear operator can act on multiple modes simultaneously:\n\\[\nT' = T \\times_1 A_1 \\times_2 A_2 \\cdots \\times_N A_N.\n\\]\nThis corresponds to the operator\n\\[\nA_N \\otimes \\cdots \\otimes A_2 \\otimes A_1\n\\]\non the vectorized tensor.\n\n\nWhy This Matters\n\nMany “complicated” tensor operations are just linear maps in disguise.\nOperator language clarifies the connection between abstract multilinear algebra and concrete matrix computations.\nThis sets the stage for defining tensor rank, decompositions, and algorithms.\n\n\n\nExercises\n\nOperator on Product: Let \\(A = \\begin{bmatrix}1 & 0 \\\\ 0 & 2\\end{bmatrix}\\), \\(B = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}\\). Compute \\((A \\otimes B)(e_1 \\otimes e_2)\\).\nMode-1 Action: For \\(T \\in \\mathbb{R}^{2 \\times 3}\\), let \\(A = \\begin{bmatrix}1 & 1 \\\\ 0 & 1\\end{bmatrix}\\). Write explicitly how \\(T \\times_1 A\\) transforms rows.\nVec Form: Verify that \\((A \\otimes B)\\,\\mathrm{vec}(X) = \\mathrm{vec}(BXA^\\top)\\) for \\(A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}, B = I\\).\nComposed Operators: Show that \\((A_1 \\otimes B_1)(A_2 \\otimes B_2) = (A_1A_2) \\otimes (B_1B_2)\\).\nThought Experiment: Why is it useful to think of tensor–matrix multiplication as applying a linear operator on a tensor space, instead of just as array reshaping?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-12.-metrics-forms-and-raisinglowering-indices",
    "href": "index.html#chapter-12.-metrics-forms-and-raisinglowering-indices",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 12. Metrics, Forms, and Raising/Lowering Indices",
    "text": "Chapter 12. Metrics, Forms, and Raising/Lowering Indices\n\n12.1 Using Inner Products to Move Indices\nIn tensor calculus, indices can appear as upper (contravariant) or lower (covariant). Inner products give us a way to convert between them. This process is called raising and lowering indices.\n\nCovariant vs. Contravariant Indices\n\nA vector \\(v\\) has components \\(v^i\\) (upper index).\nA covector (linear functional) \\(\\omega\\) has components \\(\\omega_i\\) (lower index).\nA tensor may mix both kinds, e.g. \\(T^{i}{}_{j}\\).\n\nWithout an inner product, upper and lower indices live in different spaces (\\(V\\) and \\(V^*\\)).\n\n\nUsing the Metric Tensor\nAn inner product (or metric) \\(g\\) provides a natural way to link vectors and covectors.\n\nMetric tensor: \\(g_{ij} = \\langle e_i, e_j \\rangle\\).\nInverse metric: \\(g^{ij}\\) satisfies \\(g^{ij} g_{jk} = \\delta^i_k\\).\n\n\n\nLowering an Index\nGiven a vector \\(v^i\\), define its covector by:\n\\[\nv_i = g_{ij} v^j.\n\\]\nThis maps \\(V \\to V^*\\).\n\n\nRaising an Index\nGiven a covector \\(\\omega_i\\), define its vector by:\n\\[\n\\omega^i = g^{ij} \\omega_j.\n\\]\nThis maps \\(V^- \\to V\\).\n\n\nExample in \\(\\mathbb{R}^2\\) with Euclidean Metric\nLet \\(g = I\\) (the identity matrix). Then raising and lowering indices does nothing:\n\n\\(v_i = v^i\\).\n\\(\\omega^i = \\omega_i\\).\n\nBut in a non-Euclidean metric (e.g., relativity), raising/lowering changes components significantly.\n\n\nWhy This Matters\n\nIndex manipulation allows us to move seamlessly between vector and covector viewpoints.\nIn geometry and physics, metrics define lengths, angles, and duality.\nIn relativity, raising and lowering indices with the Minkowski metric distinguishes between time and space components.\n\n\n\nExercises\n\nLowering Indices: In \\(\\mathbb{R}^2\\) with metric\n\\[\ng = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix},\n\\]\ncompute the covector \\(v_i\\) for \\(v^j = (1,4)\\).\nRaising Indices: Using the same metric, compute \\(\\omega^i\\) from \\(\\omega_j = (2,6)\\).\nCheck Consistency: Verify that raising and then lowering an index returns the original vector.\nPhysics Example: In special relativity, the Minkowski metric is\n\\[\ng = \\mathrm{diag}(-1,1,1,1).\n\\]\nShow how lowering the time component of a 4-vector changes its sign.\nThought Experiment: Why is an inner product (metric) essential for connecting vectors and covectors? What would break if we tried to raise/lower indices without one? ### 12.2 Dualizations and Adjoint Operations\n\nRaising and lowering indices with a metric doesn’t just apply to vectors and covectors - it also extends naturally to linear maps and tensors. This leads to the notions of duals and adjoints, which are central in multilinear algebra, physics, and functional analysis.\n\n\nDual of a Linear Map\nGiven a linear map \\(A: V \\to W\\), its dual map is\n\\[\nA^*: W^- \\to V^*\n\\]\ndefined by\n\\[\n(A^- \\omega)(v) = \\omega(Av),\n\\]\nfor all \\(v \\in V, \\, \\omega \\in W^*\\).\n\nIn matrix form: If \\(A\\) has matrix \\(M\\), then \\(A^*\\) has matrix \\(M^\\top\\).\n\n\n\nAdjoint of a Linear Map\nIf \\(V\\) and \\(W\\) have inner products, we can define the adjoint \\(A^\\dagger: W \\to V\\) by the property:\n\\[\n\\langle Av, w \\rangle_W = \\langle v, A^\\dagger w \\rangle_V.\n\\]\n\nIn Euclidean space with the standard dot product, \\(A^\\dagger = A^\\top\\).\nWith a general metric tensor \\(g\\), the adjoint depends on raising/lowering indices:\n\\[\n(A^\\dagger)^i{}_j = g^{ik} A^l{}_k g_{lj}.\n\\]\n\n\n\nExtension to Tensors\n\nFor tensors with mixed indices, dualization flips upper ↔︎ lower indices.\nExample: For \\(T^i{}_j\\), the dual acts as\n\\[\n(T^*)_i{}^j = g_{ik} g^{jl} T^k{}_l.\n\\]\n\nThis systematically moves indices while preserving linear relationships.\n\n\nWhy This Matters\n\nDualization is purely algebraic (map between spaces and their duals).\nAdjoint operations involve the metric and carry geometric meaning (orthogonality, length).\nIn physics, adjoints appear in energy conservation laws, quantum mechanics (Hermitian operators), and relativity.\n\n\n\nExercises\n\nDual Map: Let \\(A = \\begin{bmatrix}1 & 2 \\\\ 0 & 3\\end{bmatrix}\\). Compute its dual \\(A^*\\).\nAdjoint in Euclidean Space: Show that in \\(\\mathbb{R}^2\\) with the standard dot product, the adjoint of \\(A\\) is just \\(A^\\top\\).\nAdjoint with Metric: In \\(\\mathbb{R}^2\\) with metric\n\\[\ng = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix},\n\\]\ncompute the adjoint of \\(A = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}\\).\nTensor Dualization: For a tensor \\(T^i{}_j = \\delta^i_j\\) (the identity), compute its dual under the Euclidean metric.\nThought Experiment: Why does the adjoint depend on the choice of inner product, while the dual does not? What does this tell us about algebra vs. geometry? ### 12.3 Coordinate Rules Made Simple\n\nSo far, we’ve seen raising/lowering of indices, dual maps, and adjoints. These can look abstract, but in practice they reduce to straightforward coordinate rules once a basis and metric are fixed. This section summarizes the “index gymnastics” in a beginner-friendly way.\n\n\nRaising and Lowering\n\nLowering:\n\\[\nv_i = g_{ij} v^j\n\\]\nRaising:\n\\[\n\\omega^i = g^{ij} \\omega_j\n\\]\n\nHere \\(g_{ij}\\) is the metric, \\(g^{ij}\\) its inverse.\nTip: Think of lowering as “applying \\(g\\)” and raising as “applying \\(g^{-1}\\).”\n\n\nDual Maps\nFor a matrix \\(A\\) representing a linear map:\n\\[\n(A^*)_{ij} = A_{ji}.\n\\]\nSo the dual corresponds to a transpose in coordinates.\n\n\nAdjoint Maps\nWith a general metric \\(g\\), the adjoint of a matrix \\(A\\) is:\n\\[\nA^\\dagger = g^{-1} A^\\top g.\n\\]\n\nIf \\(g = I\\) (Euclidean metric), then \\(A^\\dagger = A^\\top\\).\nThis formula generalizes to any inner product space.\n\n\n\nTensors with Mixed Indices\nFor a tensor \\(T^{i}{}_j\\), indices can be moved by contracting with \\(g_{ij}\\) or \\(g^{ij}\\):\n\nTo lower an upper index:\n\\[\nT_{ij} = g_{ik} T^k{}_j.\n\\]\nTo raise a lower index:\n\\[\nT^{ij} = g^{jk} T^i{}_k.\n\\]\n\nThis bookkeeping ensures correct transformations under basis changes.\n\n\nWhy This Matters\n\nThese coordinate rules are the practical toolkit for working with metrics and adjoints.\nIn relativity, this is exactly how time and space components are manipulated.\nIn machine learning and physics, these rules underlie how gradients, covariances, and bilinear forms are expressed.\n\n\n\nExercises\n\nLower an Index: In \\(\\mathbb{R}^2\\) with metric \\(g = \\begin{bmatrix}2 & 0 \\\\ 0 & 3\\end{bmatrix}\\), compute \\(v_i\\) for \\(v^j = (1,2)\\).\nRaise an Index: With the same metric, compute \\(\\omega^i\\) for \\(\\omega_j = (4,6)\\).\nAdjoint Check: For \\(A = \\begin{bmatrix}1 & 2 \\\\ 0 & 3\\end{bmatrix}\\), compute \\(A^\\dagger\\) under \\(g = I\\).\nMixed Tensor: Given \\(T^i{}_j = \\begin{bmatrix}1 & 0 \\\\ 2 & 3\\end{bmatrix}\\), compute \\(T_{ij}\\) using \\(g = I\\).\nThought Experiment: Why do physicists insist on keeping track of upper vs. lower indices, while engineers often ignore the distinction in Euclidean spaces?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-13.-ranks-for-tensors",
    "href": "index.html#chapter-13.-ranks-for-tensors",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 13. Ranks for Tensors",
    "text": "Chapter 13. Ranks for Tensors\n\n13.1 Matrix Rank vs. Tensor Rank\nBefore diving into advanced tensor decompositions, we need to understand what rank means for tensors. Unlike matrices, where rank is simple and unique, tensors have several different notions of rank. This section starts with the familiar matrix rank and then introduces tensor rank.\n\nMatrix Rank (Review)\nFor a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\):\n\nThe rank is the dimension of its column space (or row space).\nEquivalently, the smallest \\(r\\) such that\n\\[\nA = \\sum_{i=1}^r u_i v_i^\\top,\n\\]\nwhere \\(u_i \\in \\mathbb{R}^m, v_i \\in \\mathbb{R}^n\\).\nEach term is a rank-one matrix.\n\nExample:\n\\[\n\\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}\n= \\begin{bmatrix}1 \\\\ 2\\end{bmatrix} \\begin{bmatrix}1 & 2\\end{bmatrix},\n\\]\nso its rank is 1.\n\n\nTensor Rank\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), the tensor rank (sometimes called CP rank) is the smallest \\(r\\) such that\n\\[\nT = \\sum_{i=1}^r u^{(1)}_i \\otimes u^{(2)}_i \\otimes \\cdots \\otimes u^{(N)}_i,\n\\]\nwhere each \\(u^{(k)}_i \\in \\mathbb{R}^{I_k}\\).\n\nEach term is a rank-one tensor (outer product of vectors).\nRank measures how many simple pieces are needed to build the tensor.\n\n\n\nKey Differences: Matrix vs. Tensor\n\nUniqueness: Matrix rank is uniquely defined; tensor rank can vary depending on the notion (CP rank, Tucker rank, etc.).\nComputation: Matrix rank is easy to compute (via SVD); tensor rank is NP-hard to compute in general.\nUpper Bound: For a matrix \\(m \\times n\\), rank ≤ min(m,n). For a tensor, the maximum possible rank is often not obvious.\n\n\n\nExample: 3-Way Tensor\nLet\n\\[\nT_{ijk} = 1 \\quad \\text{if } i=j=k, \\quad 0 \\text{ otherwise}.\n\\]\nThis “diagonal” tensor has tensor rank = dimension \\(n\\).\n\n\nWhy This Matters\n\nTensor rank generalizes the concept of complexity from matrices to higher-order data.\nLow-rank structure is crucial in applications: compression, latent factor models, signal separation.\nUnderstanding tensor rank lays the foundation for CP, Tucker, and TT decompositions.\n\n\n\nExercises\n\nMatrix Rank: Compute the rank of\n\\[\nA = \\begin{bmatrix}1 & 1 \\\\ 2 & 2\\end{bmatrix}.\n\\]\nTensor Rank-1: Show that the tensor \\(T_{ijk} = a_i b_j c_k\\) has rank 1.\nDecomposition Practice: Express\n\\[\nT_{ij} = \\begin{bmatrix}1 & 2 \\\\ 3 & 6\\end{bmatrix}\n\\]\nas a sum of rank-one matrices.\nDiagonal Tensor: For \\(T_{ijk}\\) with \\(T_{111}=1, T_{222}=1\\), all others 0, determine its rank.\nThought Experiment: Why might tensor rank be harder to compute and less well-behaved than matrix rank?\n\n\n\n\n13.2 Multilinear (Tucker) Rank and Mode Ranks\nBesides CP rank, tensors have another important notion of rank: the multilinear rank, also called the Tucker rank. This definition is based on the ranks of mode-\\(n\\) unfoldings and gives a more stable and computable measure of complexity.\n\nMode-n Rank\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\):\n\nThe mode-n rank is the matrix rank of its mode-\\(n\\) unfolding \\(T^{(n)}\\).\nDenote it by \\(\\text{rank}_n(T)\\).\n\n\n\nMultilinear (Tucker) Rank\nThe multilinear rank of \\(T\\) is the tuple\n\\[\n\\big( \\text{rank}_1(T), \\text{rank}_2(T), \\dots, \\text{rank}_N(T) \\big).\n\\]\nThis captures how much independent variation the tensor has along each mode.\n\n\nExample (3rd-Order Tensor)\nSuppose \\(T \\in \\mathbb{R}^{3 \\times 4 \\times 5}\\).\n\nMode-1 unfolding rank = 2.\nMode-2 unfolding rank = 3.\nMode-3 unfolding rank = 4.\n\nThen the multilinear rank of \\(T\\) is \\((2,3,4)\\).\n\n\nComparison with CP Rank\n\nCP rank: minimal number of rank-1 outer products.\nTucker rank: ranks of unfoldings (tuple of integers).\nCP rank is a single number, often hard to compute.\nTucker rank is a multi-dimensional profile, easier to compute via SVD of unfoldings.\n\n\n\nTucker Decomposition\nThe multilinear rank is closely tied to the Tucker decomposition:\n\\[\nT = G \\times_1 U^{(1)} \\times_2 U^{(2)} \\cdots \\times_N U^{(N)},\n\\]\nwhere\n\n\\(G\\) = core tensor,\n\\(U^{(n)}\\) = basis matrices capturing each mode’s subspace.\n\nThe dimensions of \\(G\\) are exactly the multilinear rank.\n\n\nWhy This Matters\n\nMultilinear rank provides a practical, computable measure of tensor complexity.\nIt is robust under noise and approximations, unlike CP rank.\nWidely used in tensor compression (Tucker, HOSVD) and machine learning.\n\n\n\nExercises\n\nMatrix Case: Show that for a matrix (2nd-order tensor), the Tucker rank reduces to the usual matrix rank.\nRank Profile: Suppose \\(T \\in \\mathbb{R}^{2 \\times 3 \\times 4}\\) has mode ranks (2,2,1). What does this tell you about its structure?\nUnfolding Rank: For a tensor \\(T_{ijk} = u_i v_j\\) (independent of \\(k\\)), compute its Tucker rank.\nComparison: Give an example of a tensor with low Tucker rank but high CP rank.\nThought Experiment: Why might Tucker rank be preferred over CP rank in applications like compression or noise-robust signal processing? ### 13.3 Identifiability and Uniqueness\n\nWhen we decompose a tensor into simpler components, a natural question arises: is the decomposition unique? For matrices, the SVD gives a stable, essentially unique factorization. For tensors, things are more subtle. This section explores identifiability - the conditions under which tensor decompositions are unique.\n\n\nIdentifiability in CP Rank\nFor a CP (CANDECOMP/PARAFAC) decomposition\n\\[\nT = \\sum_{i=1}^r u^{(1)}_i \\otimes u^{(2)}_i \\otimes \\cdots \\otimes u^{(N)}_i,\n\\]\nthe decomposition is said to be identifiable if no other decomposition with the same rank \\(r\\) exists (up to trivial scaling and permutation).\n\nTrivial indeterminacies:\n\nScaling: multiplying one factor by \\(\\alpha\\) and another by \\(1/\\alpha\\).\nPermutation: reordering the components.\n\n\n\n\nKruskal’s Condition (for Uniqueness)\nA famous result: If each factor matrix \\(U^{(n)}\\) has Kruskal rank \\(k_n\\) (maximum number of columns that are linearly independent), and\n\\[\nk_1 + k_2 + \\cdots + k_N \\geq 2r + (N-1),\n\\]\nthen the CP decomposition is unique (up to scaling and permutation).\n\n\nIdentifiability in Tucker Rank\nFor Tucker decomposition\n\\[\nT = G \\times_1 U^{(1)} \\times_2 U^{(2)} \\cdots \\times_N U^{(N)},\n\\]\nuniqueness is generally weaker:\n\nThe core tensor \\(G\\) is not unique.\nHowever, the subspaces spanned by the factor matrices \\(U^{(n)}\\) are unique (this is the essence of HOSVD).\n\n\n\nWhy Uniqueness Matters\n\nIn applications like chemometrics, signal separation, and latent factor models, unique decomposition means interpretable components.\nWithout identifiability, decompositions may still compress data but lose meaning.\nCP rank uniqueness is one reason tensors can capture latent structure more faithfully than matrices.\n\n\n\nChallenges\n\nChecking uniqueness conditions is not always easy.\nIn practice, algorithms may converge to non-unique solutions.\nRegularization and domain knowledge are often used to guide towards interpretable decompositions.\n\n\n\nExercises\n\nMatrix vs. Tensor: Why is the SVD of a matrix unique (up to signs), but CP decomposition of a tensor not always unique?\nScaling Ambiguity: Show explicitly how scaling one factor vector and compensating in another keeps the CP decomposition unchanged.\nPermutation Ambiguity: Given two equivalent CP decompositions, illustrate how permuting rank-1 components yields the same tensor.\nKruskal’s Condition: Suppose a 3rd-order tensor has factor matrices with Kruskal ranks (3,3,3). For rank \\(r=3\\), does Kruskal’s condition guarantee uniqueness?\nThought Experiment: Why might uniqueness of tensor decompositions be more valuable in data analysis than uniqueness of matrix decompositions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-14.-cannonical-decompositions",
    "href": "index.html#chapter-14.-cannonical-decompositions",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 14. Cannonical Decompositions",
    "text": "Chapter 14. Cannonical Decompositions\n\n14.1 CP (CANDECOMP/PARAFAC)\nThe Canonical Polyadic (CP) decomposition - also known as CANDECOMP/PARAFAC - is one of the most fundamental ways to factorize a tensor. It generalizes the idea of expressing a matrix as a sum of rank-one outer products to higher-order tensors.\n\nDefinition\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), a CP decomposition of rank \\(r\\) is:\n\\[\nT = \\sum_{i=1}^r u^{(1)}_i \\otimes u^{(2)}_i \\otimes \\cdots \\otimes u^{(N)}_i,\n\\]\nwhere each \\(u^{(n)}_i \\in \\mathbb{R}^{I_n}\\).\n\nEach term is a rank-one tensor.\nThe smallest such \\(r\\) is the CP rank of \\(T\\).\n\n\n\nExample (Matrix Case)\nFor a matrix \\(A\\), the CP decomposition reduces to the familiar rank factorization:\n\\[\nA = \\sum_{i=1}^r u_i v_i^\\top.\n\\]\n\n\nExample (3rd-Order Tensor)\nFor \\(T \\in \\mathbb{R}^{I \\times J \\times K}\\), the CP decomposition is:\n\\[\nT_{ijk} = \\sum_{r=1}^R a_{ir} b_{jr} c_{kr},\n\\]\nwhere \\(A = [a_{ir}], B = [b_{jr}], C = [c_{kr}]\\) are called factor matrices.\n\n\nProperties\n\nCP decomposition is unique under mild conditions (contrast with matrix factorization).\nProvides a compact representation: instead of storing \\(IJK\\) entries, we store only factor matrices of size \\((I+J+K)R\\).\nOften interpretable in applications (each component corresponds to a latent factor).\n\n\n\nApplications\n\nPsychometrics: original use of PARAFAC for analyzing survey data.\nChemometrics: spectral analysis of chemical mixtures.\nSignal processing: blind source separation.\nMachine learning: latent factor models, recommender systems, neural net compression.\n\n\n\nWhy This Matters\n\nCP is the most direct generalization of matrix rank factorization.\nUnlike SVD, CP works without orthogonality constraints, making it more flexible.\nIts uniqueness is a key reason for its wide use in data analysis.\n\n\n\nExercises\n\nMatrix Analogy: Show that the CP decomposition for a \\(2 \\times 2\\) matrix is the same as its rank decomposition.\n3rd-Order Example: Suppose \\(T_{ijk} = u_i v_j w_k\\). Show that \\(T\\) has CP rank 1.\nFactor Matrices: Write explicitly the factor matrices \\(A,B,C\\) for\n\\[\nT_{ijk} = \\delta_{ij}\\delta_{jk}.\n\\]\nCompression: Estimate storage cost for a tensor of size \\(50 \\times 40 \\times 30\\) if represented directly vs. CP decomposition with rank \\(r=10\\).\nThought Experiment: Why might CP decomposition be more interpretable than Tucker decomposition in some applications?\n\n\n\n\n14.2 Tucker and HOSVD\nThe Tucker decomposition is another foundational way to factorize tensors. It generalizes the matrix SVD to higher dimensions and is closely tied to the concept of multilinear rank. When computed with orthogonal factors, it is often called the Higher-Order SVD (HOSVD).\n\nTucker Decomposition\nFor a tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), the Tucker decomposition is:\n\\[\nT = G \\times_1 U^{(1)} \\times_2 U^{(2)} \\cdots \\times_N U^{(N)},\n\\]\nwhere:\n\n\\(G \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\cdots \\times R_N}\\) is the core tensor,\n\\(U^{(n)} \\in \\mathbb{R}^{I_n \\times R_n}\\) are the factor matrices,\n\\((R_1, R_2, \\dots, R_N)\\) is the multilinear rank of \\(T\\).\n\n\n\nComparison with CP\n\nCP: sum of rank-one components, no core tensor.\nTucker: includes a core tensor that mixes components across modes.\nCP rank: single number, often hard to compute.\nTucker rank: tuple \\((R_1,\\dots,R_N)\\), easier to compute (via SVD of unfoldings).\n\n\n\nHigher-Order SVD (HOSVD)\nThe HOSVD is a special Tucker decomposition where:\n\nEach factor matrix \\(U^{(n)}\\) has orthonormal columns (from the SVD of the mode-\\(n\\) unfolding).\nThe core tensor \\(G\\) has certain orthogonality properties.\n\nHOSVD is not unique but provides a stable, interpretable decomposition.\n\n\nExample (3rd-Order Tensor)\nFor \\(T \\in \\mathbb{R}^{I \\times J \\times K}\\):\n\\[\nT = \\sum_{p=1}^{R_1}\\sum_{q=1}^{R_2}\\sum_{r=1}^{R_3} g_{pqr}\\, u^{(1)}_p \\otimes u^{(2)}_q \\otimes u^{(3)}_r.\n\\]\nHere, the core entries \\(g_{pqr}\\) show how basis vectors across different modes interact.\n\n\nApplications\n\nCompression: reduce each dimension by truncating factor matrices.\nSignal processing: spatiotemporal data analysis.\nMachine learning: feature extraction and dimensionality reduction.\nNeuroscience: multi-subject brain activity analysis.\n\n\n\nWhy This Matters\n\nTucker/HOSVD generalize the SVD to tensors, making them intuitive for those familiar with matrices.\nTucker rank is easier to compute than CP rank, and truncation gives practical low-rank approximations.\nProvides a balance between interpretability (factor matrices) and flexibility (core tensor).\n\n\n\nExercises\n\nMatrix Analogy: Show that Tucker decomposition reduces to the usual SVD when \\(N=2\\).\nMode Ranks: For \\(T \\in \\mathbb{R}^{3 \\times 4 \\times 5}\\), suppose its multilinear rank is \\((2,3,2)\\). What is the size of the core tensor?\nFactor Matrix Construction: Explain how to construct the mode-1 factor matrix \\(U^{(1)}\\) from the SVD of the mode-1 unfolding.\nCompression: Estimate storage cost for a tensor of size \\(30 \\times 40 \\times 50\\) with Tucker rank \\((5,5,5)\\). Compare to full storage.\nThought Experiment: Why might Tucker/HOSVD be better than CP for approximation, even if CP is more interpretable?\n\n\n\n\n14.3 Tensor Trains (TT) and Hierarchical Formats\nWhen tensors become very large (high dimensions or many modes), storing and computing with them directly becomes impossible. Tensor Train (TT) decomposition and related hierarchical formats provide scalable ways to represent such tensors with drastically reduced storage.\n\nTensor Train (TT) Decomposition\nA tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\) is represented as a product of smaller 3-way tensors (called cores):\n\\[\nT_{i_1 i_2 \\cdots i_N} = G^{(1)}_{i_1} G^{(2)}_{i_2} \\cdots G^{(N)}_{i_N},\n\\]\nwhere:\n\nEach \\(G^{(k)}_{i_k}\\) is an \\(r_{k-1} \\times r_k\\) matrix,\n\\((r_0, r_1, \\dots, r_N)\\) are the TT ranks, with \\(r_0 = r_N = 1\\).\n\nThus, the tensor entry is computed by multiplying a chain of matrices.\n\n\nExample (Order-3 Tensor)\nFor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}\\):\n\\[\nT_{i_1 i_2 i_3} = \\sum_{a=1}^{r_1}\\sum_{b=1}^{r_2} G^{(1)}_{i_1, a} \\, G^{(2)}_{a, i_2, b} \\, G^{(3)}_{b, i_3}.\n\\]\n\n\nStorage Cost\n\nFull tensor: \\(I_1 I_2 \\cdots I_N\\) entries.\nTT decomposition: about \\(\\sum_{k=1}^N I_k r_{k-1} r_k\\).\nFor moderate TT ranks, this is exponentially smaller.\n\n\n\nHierarchical Formats (HT, H-Tucker)\n\nHierarchical Tucker (HT) generalizes TT with a tree structure.\nBoth TT and HT exploit low-rank structure in different unfolding schemes.\nWidely used in scientific computing, quantum physics, and deep learning.\n\n\n\nApplications\n\nScientific computing: solving PDEs with high-dimensional discretizations.\nQuantum physics: matrix product states (MPS) in quantum many-body systems.\nMachine learning: compressing large models, representing structured kernels.\n\n\n\nWhy This Matters\n\nTT and HT make computations feasible in dimensions where naive methods fail (“curse of dimensionality”).\nThey connect tensor methods with physics (MPS) and numerical mathematics (low-rank solvers).\nProvide scalable building blocks for high-dimensional learning and simulation.\n\n\n\nExercises\n\nStorage Comparison: Compute storage size for a tensor of size \\(10 \\times 10 \\times 10 \\times 10\\) vs. TT decomposition with TT ranks all equal to 5.\nChain Structure: For \\(T \\in \\mathbb{R}^{4 \\times 4 \\times 4}\\), sketch the TT structure (cores and ranks).\nRank-1 Case: Show that if all TT ranks are 1, the TT decomposition reduces to a rank-one tensor.\nQuantum Link: Explain how TT decomposition corresponds to Matrix Product States (MPS) in physics.\nThought Experiment: Why does the chain-like structure of TT decomposition scale better than CP or Tucker decompositions in very high dimensions?\n\n\n\n\n14.4 Connections to SVD and PCA\nTensor decompositions are natural generalizations of matrix factorizations such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA). Understanding these connections helps link classical linear algebra intuition with modern multilinear methods.\n\nSVD Recap (Matrix Case)\nFor a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\):\n\\[\nA = U \\Sigma V^\\top,\n\\]\nwhere\n\n\\(U\\) and \\(V\\) are orthogonal,\n\\(\\Sigma\\) is diagonal with singular values.\n\nThis factorization is unique (up to signs) and provides:\n\nRank,\nBest low-rank approximation,\nGeometric interpretation (rotations and scalings).\n\n\n\nPCA Recap\nPCA applies SVD to a data matrix:\n\nRows = observations, columns = features.\nExtracts orthogonal directions (principal components) that maximize variance.\n\n\n\nCP and SVD\n\nCP decomposition generalizes matrix rank factorization to higher orders.\nA matrix is just a 2-way tensor: CP = rank decomposition = SVD without orthogonality.\nFor \\(N \\geq 3\\), CP decomposition does not correspond to orthogonal factors, but it still reveals latent components.\n\n\n\nTucker/HOSVD and SVD\n\nTucker decomposition generalizes SVD to higher-order tensors.\nHOSVD is literally the higher-order SVD:\n\nCompute SVD of each mode unfolding.\nFactor matrices = left singular vectors.\nCore tensor = interaction of components across modes.\n\nTruncating singular vectors gives best low-rank approximations in a multilinear sense.\n\n\n\nPCA vs. Multilinear PCA\n\nPCA = best low-rank approximation of a data matrix.\nTucker decomposition = multilinear PCA, reducing dimensionality along each mode simultaneously.\nApplications: face recognition, video compression, multi-way data analysis.\n\n\n\nWhy This Matters\n\nCP ↔︎ matrix rank factorization.\nTucker/HOSVD ↔︎ SVD/PCA.\nThese links provide intuition: tensor decompositions are not arbitrary; they extend familiar tools to multi-way data.\n\n\n\nExercises\n\nSVD Analogy: Show how SVD of a \\(3 \\times 3\\) matrix can be viewed as a Tucker decomposition with a diagonal core.\nCP vs. SVD: Explain why CP decomposition of a matrix reduces to its rank factorization, but not to SVD.\nHOSVD Steps: Outline the steps of HOSVD for a 3rd-order tensor \\(T \\in \\mathbb{R}^{4 \\times 5 \\times 6}\\).\nPCA Analogy: Suppose we have a video dataset stored as a tensor \\((\\text{frames} \\times \\text{height} \\times \\text{width})\\). Explain how Tucker decomposition acts as a multilinear PCA.\nThought Experiment: Why is orthogonality central in SVD/PCA, but not in CP decomposition? What are the trade-offs?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-15.-working-with-tensors-in-code",
    "href": "index.html#chapter-15.-working-with-tensors-in-code",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 15. Working with Tensors in Code",
    "text": "Chapter 15. Working with Tensors in Code\n\n15.1 Efficient Indexing and Memory Layout\nWorking with tensors in practice requires careful attention to how they are stored and accessed in memory. Poor indexing can make even simple operations slow. This section explains how indexing works under the hood and how to optimize memory layout for performance.\n\nLinearization of Multi-Indices\nA tensor \\(T \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\) is stored in linear memory.\n\nEach entry \\(T_{i_1,i_2,\\dots,i_N}\\) is mapped to a single offset in memory.\nFormula (row-major / C-style layout):\n\\[\n\\text{offset}(i_1,\\dots,i_N) = i_1 (I_2 I_3 \\cdots I_N) + i_2 (I_3 \\cdots I_N) + \\cdots + i_N.\n\\]\nFormula (column-major / Fortran, MATLAB):\n\\[\n\\text{offset}(i_1,\\dots,i_N) = i_1 + I_1(i_2 + I_2(i_3 + \\cdots + I_{N-1} i_N)).\n\\]\n\n\n\nStrides\n\nA stride tells how far (in memory) you move when an index increases by 1.\nExample: in row-major order, stride along the last index = 1.\nEfficient access requires stepping through memory with small, contiguous strides.\n\n\n\nCache Efficiency\n\nCPUs fetch memory in blocks (cache lines).\nAccessing elements in a contiguous block is fast.\nJumping across large strides leads to cache misses and slow performance.\n\n\n\nPractical Implications\n\nLoop ordering matters: iterate over the innermost (stride-1) dimension in the inner loop.\nSlicing tensors may produce views with non-contiguous strides (NumPy, PyTorch). Copying may be needed for efficiency.\nTensor libraries often allow explicit control of memory layout (row-major vs. column-major).\n\n\n\nWhy This Matters\n\nMany tensor operations are memory-bound, not compute-bound.\nEfficient indexing and layout can make the difference between minutes and milliseconds.\nA good understanding of strides helps when debugging tensor code in NumPy, PyTorch, JAX, etc.\n\n\n\n\nExercises\n\nOffset Calculation (Row-Major): For \\(T \\in \\mathbb{R}^{2 \\times 3 \\times 4}\\), what is the memory offset of \\(T_{1,2,3}\\) (0-based indexing)?\nOffset Calculation (Column-Major): For the same tensor, compute the offset of \\(T_{1,2,3}\\) in column-major order.\nStride Check: In row-major order for \\(T \\in \\mathbb{R}^{3 \\times 4 \\times 5}\\), what are the strides for each dimension?\nCache-Efficient Loop: Write pseudocode for iterating over all entries of a 3D tensor in row-major order.\nThought Experiment: Why might a deep learning library internally reorder tensor layouts depending on the hardware (CPU vs. GPU)?\n\n\n\n15.2 BLAS, Einsum, Performance Patterns\nAfter understanding memory layout, the next step in efficient tensor computation is using optimized libraries and abstractions. This section covers BLAS, the einsum notation, and common performance patterns that make tensor operations practical at scale.\n\nBLAS: Basic Linear Algebra Subprograms\n\nBLAS is the standard library for high-performance vector and matrix operations.\nMany tensor operations reduce to BLAS calls:\n\nMatrix multiplication (GEMM) is the backbone of most tensor contractions.\nLevel-1 BLAS: vector ops (\\(y \\leftarrow ax + y\\)).\nLevel-2 BLAS: matrix-vector ops.\nLevel-3 BLAS: matrix-matrix ops (highest efficiency).\n\n\nWhy this matters: Efficient tensor libraries (NumPy, PyTorch, TensorFlow, JAX) rely heavily on BLAS under the hood.\n\n\nEinsum Notation\nThe Einstein summation convention (“einsum”) expresses tensor contractions concisely.\nExample:\n\nMatrix multiplication:\n\\[\nC_{ij} = \\sum_k A_{ik} B_{kj}\n\\]\nIn einsum: einsum('ik,kj-&gt;ij', A, B).\nInner product:\n\\[\n\\langle u,v \\rangle = \\sum_i u_i v_i\n\\]\nIn einsum: einsum('i,i-&gt;', u, v).\nOuter product:\n\\[\nT_{ij} = u_i v_j\n\\]\nIn einsum: einsum('i,j-&gt;ij', u, v).\n\nAdvantages:\n\nExpresses contractions clearly without reshaping.\nOften compiles to highly efficient BLAS/GPU kernels.\n\n\n\nPerformance Patterns\n\nBatching: Group operations across multiple tensors (e.g., batched GEMM in GPUs).\nBlocking / Tiling: Break large tensors into cache-sized blocks to improve locality.\nFusing Operations: Combine multiple small operations into one kernel (important in GPU computing).\nAvoiding Copies: Use views/strides instead of reshaping whenever possible.\nAutomatic Differentiation: Frameworks like PyTorch and JAX integrate einsum with backpropagation efficiently.\n\n\n\nWhy This Matters\n\nBLAS-level performance is critical for large-scale tensor applications.\nEinsum notation unifies different tensor operations under one compact framework.\nRecognizing performance patterns makes code scale across CPUs, GPUs, and accelerators.\n\n\n\nExercises\n\nEinsum Practice: Write the einsum expression for computing\n\\[\nC_{ij} = \\sum_{k,l} A_{ik} B_{kl} D_{lj}.\n\\]\nOuter Product: Using einsum, compute the 3rd-order tensor \\(T_{ijk} = u_i v_j w_k\\).\nBLAS Levels: Classify the following into BLAS level-1, 2, or 3:\n\nDot product,\nMatrix-vector product,\nMatrix-matrix product.\n\nBatching Example: If you have 100 matrices of size \\(50 \\times 50\\), why is a batched GEMM faster than 100 separate GEMM calls?\nThought Experiment: Why might einsum be more maintainable than manually reshaping and transposing arrays for contractions?\n\n\n\n\n15.3 Stability, Conditioning, Scaling Tricks\nEfficient computation is not enough - tensor operations must also be numerically stable. Large-scale problems often involve ill-conditioned matrices or tensors, and small floating-point errors can accumulate dramatically. This section introduces stability concerns and practical tricks for keeping computations reliable.\n\nConditioning and Stability\n\nCondition number: For a matrix \\(A\\),\n\\[\n\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\n\\]\nmeasures sensitivity of solutions to perturbations.\nIn tensors, contractions can amplify errors, especially when factors are nearly linearly dependent.\nHigh tensor ranks often worsen conditioning.\n\nRule of thumb: Poorly conditioned problems cannot be solved accurately, no matter the algorithm.\n\n\nCommon Stability Issues in Tensors\n\nOverflows/underflows: multiplying many large/small entries.\nLoss of orthogonality: iterative algorithms drift from true subspaces.\nCancellation errors: subtracting nearly equal numbers.\nExploding/vanishing gradients: in automatic differentiation with deep tensor networks.\n\n\n\nScaling Tricks\n\nNormalization: Rescale vectors and factor matrices to keep entries in a safe range.\nOrthogonalization: Regularly re-orthogonalize factor matrices in decompositions (QR or SVD steps).\nLog-domain computations: Replace products with sums of logarithms to prevent overflow (e.g., in probabilistic models).\nBalanced scaling: In CP decompositions, distribute scale evenly across modes to avoid extreme values.\n\n\n\nRegularization\n\nAdd small perturbations (like \\(\\lambda I\\)) to stabilize inversions (“Tikhonov regularization”).\nIn optimization, add penalties to discourage ill-conditioned solutions.\nHelps avoid overfitting in statistical tensor models.\n\n\n\nWhy This Matters\n\nNumerical stability is essential for trustworthy tensor computations.\nScaling and orthogonalization tricks are used in nearly every practical algorithm.\nWithout them, decompositions may diverge, optimizations may fail, and results may become meaningless.\n\n\n\nExercises\n\nCondition Number: Compute the condition number of\n\\[\nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 10^{-6}\\end{bmatrix}.\n\\]\nWhat does it tell you about stability?\nOverflow Example: Suppose we compute the product of 100 numbers all equal to 1.01. Estimate the result. Why might floating-point overflow occur?\nScaling in CP: Explain why rescaling one factor by \\(10^6\\) and another by \\(10^{-6}\\) in a CP decomposition gives the same tensor but may cause instability.\nOrthogonalization: Describe how QR factorization can help maintain numerical stability in HOSVD computations.\nThought Experiment: Why might log-domain computation be essential in probabilistic models with tensors (e.g., hidden Markov models, Bayesian networks)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-16.-automatic-differentiation-and-gradients",
    "href": "index.html#chapter-16.-automatic-differentiation-and-gradients",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 16. Automatic Differentiation and Gradients",
    "text": "Chapter 16. Automatic Differentiation and Gradients\n\n16.1 Jacobians/Hessians as Tensors\nIn calculus, derivatives of multivariable functions are naturally represented as tensors. Recognizing this viewpoint helps connect analysis with multilinear algebra, and explains why tensors appear in optimization, machine learning, and physics.\n\nJacobian as a Matrix (2nd-Order Tensor)\nFor a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian matrix is\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}.\n\\]\n\nIt describes how small changes in input \\(x\\) produce changes in output \\(f(x)\\).\nIn tensor terms: \\(J \\in \\mathbb{R}^{m \\times n}\\).\n\nExample:\n\\[\nf(x,y) = (x^2, xy), \\quad J = \\begin{bmatrix} 2x & 0 \\\\ y & x \\end{bmatrix}.\n\\]\n\n\nHessian as a 2nd-Order Derivative Tensor\nFor a scalar function \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\), the Hessian is\n\\[\nH_{ij} = \\frac{\\partial^2 g}{\\partial x_i \\partial x_j}.\n\\]\n\nIt is a symmetric matrix (\\(H_{ij} = H_{ji}\\)).\nEncodes curvature: quadratic approximation of \\(g(x)\\).\nIn optimization, eigenvalues of \\(H\\) indicate convexity.\n\n\n\nHigher-Order Derivatives as Tensors\n\nThird derivatives form a 3rd-order tensor:\n\\[\nT_{ijk} = \\frac{\\partial^3 g}{\\partial x_i \\partial x_j \\partial x_k}.\n\\]\nIn general, the \\(k\\)-th derivative of \\(g\\) is a symmetric \\(k\\)-tensor.\nThese appear in Taylor expansions, perturbation analysis, and physics (nonlinear elasticity, quantum chemistry).\n\n\n\nAutomatic Differentiation (AD) Perspective\n\nAD frameworks (PyTorch, JAX, TensorFlow) compute Jacobians, Hessians, and higher derivatives automatically.\nUnder the hood, they construct tensors of partial derivatives and contract them efficiently.\nTensor viewpoint clarifies why gradients, Jacobians, and Hessians are different “levels” of the same structure.\n\n\n\nWhy This Matters\n\nViewing derivatives as tensors unifies multivariable calculus and multilinear algebra.\nExplains the role of Jacobians in transformations, Hessians in optimization, and higher derivatives in scientific modeling.\nEssential foundation for backpropagation and deep learning.\n\n\n\n\nExercises\n\nJacobian Practice: Compute the Jacobian of\n\\[\nf(x,y,z) = (xy, yz, xz).\n\\]\nHessian Example: For \\(g(x,y) = x^2y + y^3\\), compute the Hessian matrix.\nSymmetry Check: Show explicitly that mixed partials of \\(g(x,y)\\) are equal: \\(\\frac{\\partial^2 g}{\\partial x \\partial y} = \\frac{\\partial^2 g}{\\partial y \\partial x}\\).\nThird Derivative Tensor: Write down all nonzero entries of the 3rd derivative tensor of \\(g(x) = x^4\\) (1D case).\nThought Experiment: Why is it natural that higher derivatives are symmetric tensors? What would break if they weren’t?\n\n\n\n16.2 Backprop as Structured Contractions\nBackpropagation, the core algorithm behind training neural networks, is fundamentally a sequence of tensor contractions guided by the chain rule. Multilinear algebra provides a clean way to see why backprop works and why it is efficient.\n\nChain Rule in Tensor Form\nFor functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and \\(g: \\mathbb{R}^m \\to \\mathbb{R}^p\\):\n\\[\nJ_{g \\circ f}(x) = J_g(f(x)) \\, J_f(x),\n\\]\nwhere \\(J\\) denotes the Jacobian.\n\nComposition of functions = multiplication (contraction) of Jacobians.\nBackpropagation efficiently evaluates this chain without materializing huge Jacobians.\n\n\n\nForward vs. Reverse Mode\n\nForward mode AD: propagate derivatives forward (good when inputs are few).\nReverse mode AD (backprop): propagate sensitivities backward (good when outputs are few, e.g. scalar loss).\n\nIn reverse mode:\n\\[\n\\frac{\\partial L}{\\partial x} = \\left( \\frac{\\partial y}{\\partial x} \\right)^\\top \\frac{\\partial L}{\\partial y}.\n\\]\nThis is a tensor contraction: contract gradient vector with a Jacobian.\n\n\nLayer-by-Layer in Neural Networks\nEach layer is a function:\n\\[\nh^{(l+1)} = \\sigma(W^{(l)} h^{(l)} + b^{(l)}).\n\\]\nBackprop proceeds by:\n\nCompute forward activations.\nFor loss \\(L\\), compute gradient wrt output: \\(\\frac{\\partial L}{\\partial h^{(L)}}\\).\nContract backwards through each layer:\n\nJacobian of linear part: \\(W^{(l)}\\).\nJacobian of nonlinearity: diagonal tensor of \\(\\sigma'(z)\\).\n\n\n\n\nExample (Two Layers)\nFor \\(L(f(x))\\) with \\(f(x) = \\sigma(Wx)\\):\n\\[\n\\frac{\\partial L}{\\partial x} = W^\\top \\big( \\sigma'(Wx) \\odot \\frac{\\partial L}{\\partial f} \\big).\n\\]\nHere:\n\n\\(\\odot\\) = elementwise product,\nContraction with \\(W^\\top\\) propagates gradient backward.\n\n\n\nTensor Viewpoint\n\nJacobians are tensors.\nBackprop avoids forming full Jacobians (which would be huge) by contracting only along needed directions.\nEach step is a structured contraction of the gradient with the local Jacobian.\n\n\n\nWhy This Matters\n\nExplains efficiency: backprop runs in time proportional to the forward pass.\nShows the unity of AD, calculus, and multilinear algebra.\nClarifies why backprop generalizes beyond neural nets (any differentiable computational graph).\n\n\n\nExercises\n\nChain Rule Contraction: Let \\(f(x,y) = (x+y, xy)\\), \\(g(u,v) = u^2+v\\). Write the backprop step explicitly using contractions.\nLinear Layer: For \\(h = Wx\\), show that \\(\\frac{\\partial L}{\\partial x} = W^\\top \\frac{\\partial L}{\\partial h}\\).\nNonlinear Layer: For \\(h = \\tanh(z)\\), derive the contraction rule for backpropagation.\nEfficiency Check: Estimate the cost of explicitly forming the Jacobian of a fully connected layer with \\(m\\) outputs and \\(n\\) inputs, versus the cost of backprop.\nThought Experiment: Why is reverse-mode AD (backprop) much more efficient than forward-mode AD for training neural networks?\n\n\n\n\n16.3 Practical Tips for PyTorch/JAX/NumPy\nAutomatic differentiation (AD) frameworks like PyTorch, JAX, and NumPy (with autograd extensions) make tensor calculus practical. But efficiency and clarity depend on how you structure code. This section gives concrete tips to avoid pitfalls and exploit the strengths of these libraries.\n\nTip 1. Use Vectorization, Not Loops\n\nReplace Python loops with tensorized operations.\nExample (inefficient):\ny = torch.zeros(n)\nfor i in range(n):\n    y[i] = a[i] - b[i]\nExample (efficient):\ny = a - b\n\n\n\nTip 2. Exploit Broadcasting\n\nBroadcasting avoids unnecessary reshaping and repetition.\nExample:\n# Add bias vector to each row\nY = X + b   # automatic broadcasting\nBroadcasting keeps memory use low and code clean.\n\n\n\nTip 3. Prefer einsum for Complex Contractions\n\neinsum is expressive and optimized.\nExample: matrix multiplication:\nC = torch.einsum('ik,kj-&gt;ij', A, B)\nWorks the same in NumPy and JAX.\n\n\n\nTip 4. Control Gradient Flow\n\nIn PyTorch: x.detach() to stop gradients.\nIn JAX: use jax.lax.stop_gradient(x).\nImportant for stabilizing training and avoiding accidental memory blowups.\n\n\n\nTip 5. Check Shapes with Assertions\n\nMany AD errors come from shape mismatches.\nInsert sanity checks:\nassert X.shape == (batch, features)\nShape discipline avoids subtle bugs in backprop.\n\n\n\nTip 6. Monitor Numerical Stability\n\nUse functions like torch.nn.functional.log_softmax instead of naive softmax to avoid overflow.\nAdd small epsilons in denominators: x / (y + 1e-8).\nUse mixed precision cautiously (FP16 vs. FP32).\n\n\n\nTip 7. Benchmark with Profilers\n\nPyTorch: torch.profiler.\nJAX: jax.profiler.trace.\nNumPy: %timeit (Jupyter).\nHelps identify bottlenecks in contractions and data movement.\n\n\n\nWhy This Matters\n\nWriting tensor code is easy; writing fast, stable, and scalable tensor code requires discipline.\nFollowing these practices prevents performance cliffs and silent gradient bugs.\nBridges theory (multilinear algebra) with implementation (real training pipelines).\n\n\n\nExercises\n\nVectorization: Rewrite a loop-based dot product in PyTorch using vectorized syntax.\nBroadcasting: Given \\(X \\in \\mathbb{R}^{100 \\times 50}\\) and \\(b \\in \\mathbb{R}^{50}\\), add \\(b\\) to each row using broadcasting.\nEinsum Practice: Write the einsum expression for batched matrix multiplication \\(Y_b = A_b B_b\\), with batch dimension \\(b\\).\nGradient Stop: In PyTorch, why might we use x.detach() inside a training loop? Give an example.\nThought Experiment: Why is log_softmax numerically safer than exp(x)/sum(exp(x))?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-17.-data-science-and-signal-processing",
    "href": "index.html#chapter-17.-data-science-and-signal-processing",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 17. Data Science and Signal Processing",
    "text": "Chapter 17. Data Science and Signal Processing\n\n17.1 Multilinear Regression\nRegression is one of the most basic tools in data science: fitting a model that predicts an output from input data. When the data is naturally multi-way (tensor-structured) instead of flat vectors or matrices, multilinear regression becomes a natural extension.\n\nOrdinary Regression (Review)\nFor data pairs \\((x_i, y_i)\\):\n\\[\ny \\approx Wx + b.\n\\]\nHere, \\(x \\in \\mathbb{R}^n\\), \\(y \\in \\mathbb{R}^m\\), and \\(W \\in \\mathbb{R}^{m \\times n}\\).\nThis assumes vector inputs.\n\n\nMultilinear Regression Model\nSuppose input data is a tensor \\(X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\). Instead of flattening \\(X\\) into a vector, we preserve its structure by using a multilinear map:\n\\[\n\\hat{y} = X \\times_1 W^{(1)} \\times_2 W^{(2)} \\cdots \\times_N W^{(N)} + b,\n\\]\nwhere each \\(W^{(n)}\\) acts along mode-\\(n\\).\n\nThis reduces parameter count dramatically compared to a full vectorized regression.\nPreserves interpretability along each mode (e.g., time, space, frequency).\n\n\n\nExample\n\nInput: video clip \\(X \\in \\mathbb{R}^{\\text{frames} \\times \\text{height} \\times \\text{width}}\\).\nFlattened regression would need millions of parameters.\nMultilinear regression uses three factor matrices \\(W^{(\\text{frames})}, W^{(\\text{height})}, W^{(\\text{width})}\\), drastically reducing parameters.\n\n\n\nTraining\n\nSolve by least squares or regularized optimization.\nOften implemented via alternating minimization (update one \\(W^{(n)}\\) at a time).\nRegularization (e.g., low-rank constraints) prevents overfitting.\n\n\n\nApplications\n\nNeuroscience: predict brain activity from multi-way stimulus data.\nChemometrics: regression on spectral cubes.\nTime-series analysis: structured prediction across modes (time, channels, features).\n\n\n\nWhy This Matters\n\nExploits multi-way structure instead of destroying it by flattening.\nReduces model complexity while keeping interpretability.\nLays the groundwork for tensor methods in supervised learning.\n\n\n\nExercises\n\nFlattened vs. Multilinear: For input \\(X \\in \\mathbb{R}^{10 \\times 20}\\) and output scalar, how many parameters does flattened regression need? How many parameters if we use multilinear regression with \\(W^{(1)} \\in \\mathbb{R}^{10 \\times 3}, W^{(2)} \\in \\mathbb{R}^{20 \\times 3}\\)?\nMode Multiplication: Write explicitly how \\(X \\times_1 W^{(1)} \\times_2 W^{(2)}\\) works for a 2D input (matrix).\nInterpretability: Explain why multilinear regression can separate effects of time and space in spatiotemporal data.\nOptimization: Why is alternating minimization a natural algorithm for training multilinear regression models?\nThought Experiment: In what situations would flattening be acceptable, and when is multilinear regression clearly superior?\n\n\n\n\n17.2 Spatiotemporal Data and Video Tensors\nMany real-world datasets are not flat vectors or simple matrices but inherently multi-way arrays. A prime example is spatiotemporal data - measurements varying across both space and time. Video is a natural case: each frame is a 2D image, and the sequence of frames adds a temporal dimension, giving a 3rd-order tensor.\n\nVideo as a Tensor\nA grayscale video with \\(F\\) frames, height \\(H\\), and width \\(W\\) is naturally represented as:\n\\[\nX \\in \\mathbb{R}^{F \\times H \\times W}.\n\\]\nFor color video, an additional channel dimension is added:\n\\[\nX \\in \\mathbb{R}^{F \\times H \\times W \\times 3}.\n\\]\nFlattening this into a matrix or vector loses structure and explodes parameter count.\n\n\nTensor Decomposition for Spatiotemporal Data\n\nTucker decomposition:\n\nSeparates time, spatial rows, and spatial columns into low-rank factors.\nCompresses video efficiently while preserving essential dynamics.\n\nCP decomposition:\n\nRepresents data as a sum of rank-one spatiotemporal components.\nEach component factors into (time profile) × (spatial pattern).\n\nTensor Train (TT):\n\nHandles very long video sequences by chaining local factors.\n\n\n\n\nApplications\n\nCompression: reduce storage while keeping perceptual quality.\nBackground modeling: separate foreground objects from static background (via low-rank + sparse decomposition).\nForecasting: use multilinear regression on decomposed factors to predict future frames.\nPattern discovery: extract temporal modes (e.g., daily cycles) and spatial modes (e.g., recurring structures).\n\n\n\nBeyond Video: General Spatiotemporal Data\n\nClimate data: temperature, humidity, pressure (time × latitude × longitude × altitude).\nNeuroscience: brain activity measured over time across sensors.\nTraffic flows: time × location × type of vehicle.\n\nAll benefit from multilinear analysis.\n\n\nWhy This Matters\n\nTreating spatiotemporal data as tensors respects its inherent multi-way structure.\nLeads to compact models, better interpretability, and efficient computation.\nBridges data science, machine learning, and physics-based modeling.\n\n\n\nExercises\n\nVideo Dimensions: A color video with 100 frames of size \\(64 \\times 64\\). What is its tensor shape?\nCompression: Estimate the storage size (in entries) of this video vs. a Tucker decomposition with ranks \\((10, 10, 10, 3)\\).\nForeground/Background: Explain how a low-rank + sparse model might separate background (low-rank) from moving objects (sparse).\nTemporal Modes: If CP decomposition yields components of the form (time × space), how would you interpret a component with strong daily periodicity in the time factor?\nThought Experiment: Why might tensor decompositions uncover hidden structure in spatiotemporal data that PCA on flattened vectors would miss?\n\n\n\n\n17.3 Blind Source Separation\nBlind Source Separation (BSS) is the problem of extracting hidden signals (sources) from observed mixtures, without detailed knowledge of how they were mixed. Tensors provide powerful tools for solving BSS, often outperforming classical matrix-based methods.\n\nThe Mixing Problem\nSuppose we observe signals \\(x(t) \\in \\mathbb{R}^m\\) that are mixtures of \\(n\\) hidden sources \\(s(t) \\in \\mathbb{R}^n\\):\n\\[\nx(t) = A s(t),\n\\]\nwhere \\(A\\) is an unknown mixing matrix.\nGoal: Recover \\(s(t)\\) and \\(A\\) from only the observations \\(x(t)\\).\n\n\nClassical Approach: ICA (Independent Component Analysis)\n\nAssumes sources are statistically independent.\nUses second- and higher-order statistics to separate signals.\nWorks well for simple mixtures, but struggles with multi-way structure.\n\n\n\nTensor Approach to BSS\nMoments and cumulants of observed signals are naturally represented as symmetric tensors:\n\n2nd-order cumulant (covariance): matrix.\n4th-order cumulant: 4th-order tensor.\n\nBy analyzing these higher-order tensors:\n\nSources can be separated even when covariance is insufficient.\nCP decomposition of the cumulant tensor reveals source directions.\n\n\n\nExample: Cocktail Party Problem\n\nMicrophones record overlapping voices in a room.\nCovariance matrix cannot separate voices if they overlap in energy.\n4th-order cumulant tensor factorization recovers independent voices.\n\n\n\nApplications\n\nAudio processing: separating voices, music, or environmental sounds.\nMedical imaging: separating independent brain activity sources from EEG/fMRI data.\nTelecommunications: extracting signals from mixed channels.\nFinance: identifying independent factors driving market time series.\n\n\n\nWhy This Matters\n\nTensor methods exploit multi-way statistical structure, not just pairwise correlations.\nCP decomposition guarantees identifiability in cases where matrix factorizations fail.\nThis makes BSS one of the most successful real-world applications of multilinear algebra.\n\n\n\nExercises\n\nCovariance Limitation: Why might two voices with similar pitch have indistinguishable covariance, but separable 4th-order statistics?\nTensor Rank: Explain why the CP rank of the 4th-order cumulant tensor corresponds to the number of independent sources.\nPractical Example: Given three observed mixtures of two signals, sketch how CP decomposition could be used to separate them.\nICA vs. Tensor: Compare ICA (matrix-based) and tensor-based approaches for BSS. Which one uses more information about the data?\nThought Experiment: Why might tensors be especially effective for BSS when the number of sensors is close to the number of sources?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-18.-machine-learning-and-deep-models",
    "href": "index.html#chapter-18.-machine-learning-and-deep-models",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 18. Machine Learning and Deep Models",
    "text": "Chapter 18. Machine Learning and Deep Models\n\n18.1 Convolutions as Multilinear Maps\nConvolutions, a cornerstone of modern deep learning, are fundamentally multilinear operations. While often introduced algorithmically (sliding filters over data), they can be expressed neatly within the tensor algebra framework.\n\nConvolution as a Tensor Contraction\nConsider a 1D convolution of an input signal \\(x \\in \\mathbb{R}^n\\) with a kernel \\(h \\in \\mathbb{R}^k\\):\n\\[\ny_i = \\sum_{j=1}^k h_j \\, x_{i-j}.\n\\]\nThis is a bilinear map: linear in both the input \\(x\\) and kernel \\(h\\).\n\nIn higher dimensions (2D, 3D), the same structure holds: convolution = tensor contraction between input data and kernel.\n\n\n\nConvolution as a Multilinear Operator\nFor 2D convolution (images):\n\nInput: \\(X \\in \\mathbb{R}^{H \\times W \\times C}\\) (height × width × channels).\nKernel: \\(K \\in \\mathbb{R}^{r \\times s \\times C \\times M}\\) (filter height × filter width × channels × output channels).\nOutput:\n\\[\nY_{i,j,m} = \\sum_{p,q,c} K_{p,q,c,m} \\, X_{i+p, j+q, c}.\n\\]\n\nThis is a 4-way contraction across indices \\(p,q,c\\).\n\n\nTensor Perspective Benefits\n\nUnification: Convolution is just a structured multilinear map.\nEfficiency: Frameworks optimize convolution via tensor contractions and reshaping into matrix multiplications (im2col trick).\nGeneralization: Other operations (cross-correlation, attention) are tensor contractions of similar form.\n\n\n\nConnection to Low-Rank Tensors\n\nConvolution kernels can be approximated by low-rank tensor decompositions (e.g., CP, Tucker).\nThis reduces parameters and speeds up training in deep neural networks.\nExample: a 3D convolution kernel decomposed into separable 1D kernels.\n\n\n\nApplications Beyond Deep Nets\n\nSignal processing: filtering, denoising, feature extraction.\nPhysics: differential operators (Laplacian, wave equation) as convolutions.\nGraphics: image blurring, sharpening, edge detection.\n\n\n\nWhy This Matters\n\nShows that convolutions are not “magic,” but structured tensor contractions.\nProvides a natural bridge between deep learning and multilinear algebra.\nExplains why tensor decompositions are effective for convolutional networks.\n\n\n\nExercises\n\n1D Convolution: Write the convolution of \\(x = (1,2,3,4)\\) with \\(h = (1,-1)\\) explicitly.\nTensor Formulation: For a grayscale image \\(X \\in \\mathbb{R}^{H \\times W}\\) and filter \\(K \\in \\mathbb{R}^{r \\times s}\\), express convolution as a tensor contraction.\nKernel Decomposition: Show how a separable 2D kernel (rank-1 matrix) can be written as outer product of two 1D kernels.\nLow-Rank Compression: Estimate parameter savings if a \\(7 \\times 7 \\times 64 \\times 128\\) kernel is approximated by separable \\(7 \\times 1\\) and \\(1 \\times 7\\) filters.\nThought Experiment: Why might expressing convolutions as multilinear maps help in designing more efficient deep learning architectures?\n\n\n\n\n18.2 Low-Rank Tensor Compression of Nets\nModern neural networks, especially convolutional and transformer-based models, contain millions (or even billions) of parameters. Many of these parameters are highly redundant. Low-rank tensor decompositions provide a principled way to compress networks without losing much accuracy.\n\nRedundancy in Neural Nets\n\nConvolution kernels: \\(K \\in \\mathbb{R}^{r \\times s \\times C_{\\text{in}} \\times C_{\\text{out}}}\\).\nFully connected layers: weight matrices \\(W \\in \\mathbb{R}^{m \\times n}\\).\nThese often have effective rank much smaller than full dimensions.\n\n\n\nCP Decomposition for Compression\nA convolutional kernel \\(K\\) can be approximated as:\n\\[\nK \\approx \\sum_{i=1}^R a^{(1)}_i \\otimes a^{(2)}_i \\otimes a^{(3)}_i \\otimes a^{(4)}_i,\n\\]\nwhere \\(R\\) is small.\n\nReduces parameters from \\(r \\cdot s \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}\\) to about \\(R(r+s+C_{\\text{in}}+C_{\\text{out}})\\).\n\n\n\nTucker Decomposition for Compression\nFactorize kernel as:\n\\[\nK \\approx G \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)} \\times_4 U^{(4)},\n\\]\nwhere \\(G\\) is a small core tensor.\n\nAllows flexible rank choices along each mode.\nOften used for compressing fully connected layers.\n\n\n\nTensor Train (TT) for Compression\nLarge fully connected layers \\(W \\in \\mathbb{R}^{m \\times n}\\) can be reshaped into a high-order tensor and approximated in TT format.\n\nParameters scale as \\(\\mathcal{O}(d r^2 n)\\) instead of \\(\\mathcal{O}(mn)\\).\nEnables deployment of large models on resource-limited devices.\n\n\n\nApplications in Deep Learning\n\nCNNs: low-rank approximations of convolution filters.\nTransformers: compress attention matrices with tensor decomposition.\nMobile AI: deploy compressed models on smartphones or edge devices.\n\n\n\nTrade-offs\n\nPros: fewer parameters, lower memory, faster inference.\nCons: extra decomposition step, potential accuracy loss if ranks are too low, need for retraining/fine-tuning.\n\n\n\nWhy This Matters\n\nLow-rank tensor methods make deep learning more efficient and accessible.\nThey link classical multilinear algebra directly with modern AI engineering.\nProvide theoretical tools to understand redundancy and overparameterization.\n\n\n\nExercises\n\nParameter Counting: Compare parameter count of a \\(7 \\times 7 \\times 64 \\times 128\\) convolution kernel vs. CP decomposition with rank \\(R=20\\).\nTucker Compression: Suppose a kernel has shape \\(10 \\times 10 \\times 32 \\times 64\\). If Tucker ranks are \\((5,5,10,10)\\), how many parameters are needed (core + factors)?\nTT Format: Explain how reshaping a \\(1024 \\times 1024\\) weight matrix into a 4th-order tensor enables TT compression.\nAccuracy vs. Efficiency: Why might too aggressive a low-rank approximation harm model accuracy?\nThought Experiment: Could a neural net be trained directly in compressed tensor form, instead of compressing after training? What might be the advantages? ### 18.3 Attention as Tensor Contractions\n\nThe attention mechanism, central to transformer models, can be seen as a sequence of structured tensor contractions. Expressing attention in multilinear algebra terms clarifies both its efficiency and its flexibility.\n\n\nStandard Attention Formula\nGiven queries \\(Q \\in \\mathbb{R}^{n \\times d}\\), keys \\(K \\in \\mathbb{R}^{m \\times d}\\), and values \\(V \\in \\mathbb{R}^{m \\times d_v}\\):\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right) V.\n\\]\n\n\\(QK^\\top\\): similarity scores between queries and keys.\nSoftmax: normalizes across keys.\nMultiplication with \\(V\\): aggregates values.\n\n\n\nTensor Contraction View\n\nSimilarity computation:\n\\[\nS_{ij} = \\sum_{k} Q_{ik} K_{jk},\n\\]\na contraction over feature index \\(k\\).\nWeighted aggregation:\n\\[\nO_{i\\ell} = \\sum_{j} \\text{softmax}(S_{ij}) \\, V_{j\\ell}.\n\\]\n\nThus, attention = two contractions:\n\nContract \\(Q\\) with \\(K\\) (dot product).\nContract softmax weights with \\(V\\).\n\n\n\nMulti-Head Attention as Blocked Contractions\n\nSplit \\(Q, K, V\\) into \\(h\\) heads (smaller feature dimensions).\nPerform attention contraction in parallel for each head.\nConcatenate results.\n\nThis is equivalent to block-structured tensor contractions.\n\n\nLow-Rank and Tensorized Variants\n\nLow-rank attention: approximate \\(QK^\\top\\) with a low-rank factorization.\nTensorized attention: represent weights in CP/Tucker/TT form for efficiency.\nLinear attention: replace full contraction with kernelized approximations.\n\n\n\nApplications and Insights\n\nShows attention is not “black magic” but structured multilinear algebra.\nExplains why tensor decompositions reduce attention cost.\nConnects attention with classical bilinear forms and projections.\n\n\n\nWhy This Matters\n\nBrings transformers into the same framework as convolutions and regression.\nProvides a language for designing efficient attention mechanisms.\nHelps bridge deep learning architectures with tensor theory.\n\n\n\nExercises\n\nDot-Product Attention: Express \\(S = QK^\\top\\) as an einsum contraction.\nAggregation Step: Show how multiplying softmax-normalized scores with \\(V\\) is another einsum contraction.\nMulti-Head Splitting: If \\(d=64\\) and \\(h=8\\), what is the per-head dimension?\nLow-Rank Trick: If \\(QK^\\top\\) is approximated by \\(Q(UV^\\top)K^\\top\\) with rank \\(r\\), how does this reduce complexity?\nThought Experiment: Why might thinking of attention as a tensor contraction help design new transformer variants?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-19.-physics-graphics-and-beyond",
    "href": "index.html#chapter-19.-physics-graphics-and-beyond",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 19. Physics, Graphics, and Beyond",
    "text": "Chapter 19. Physics, Graphics, and Beyond\n\n19.1 Stress/Strain Tensors\nIn physics and engineering, stress and strain are key concepts for understanding how materials deform under forces. Both are naturally expressed as second-order tensors, making them a classic application of multilinear algebra.\n\nStrain Tensor (Deformation)\nWhen a material is deformed, each point moves by a displacement vector \\(u(x)\\).\n\nThe strain tensor measures local stretching, compression, and shear.\n\n\\[\n\\varepsilon_{ij} = \\tfrac{1}{2} \\left( \\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i} \\right).\n\\]\n\nSymmetric: \\(\\varepsilon_{ij} = \\varepsilon_{ji}\\).\nDiagonal entries: stretching along axes.\nOff-diagonal entries: shear distortions.\n\n\n\nStress Tensor (Internal Forces)\nThe stress tensor \\(\\sigma_{ij}\\) describes internal forces per unit area inside a material.\n\nDefined so that force on a surface with normal \\(n_j\\) is\n\n\\[\nf_i = \\sigma_{ij} n_j.\n\\]\n\nDiagonal entries: normal stresses (compression/tension).\nOff-diagonal entries: shear stresses.\n\n\n\nHooke’s Law (Linear Elasticity)\nStress and strain are related by a 4th-order elasticity tensor \\(C\\):\n\\[\n\\sigma_{ij} = \\sum_{k,l} C_{ijkl} \\, \\varepsilon_{kl}.\n\\]\n\nIn isotropic materials, \\(C\\) depends only on two constants (Young’s modulus and Poisson’s ratio).\nThis is a bilinear relation between strain and stress tensors.\n\n\n\nEigenvalues and Principal Axes\n\nStress tensor \\(\\sigma\\) can be diagonalized:\n\nEigenvalues = principal stresses.\nEigenvectors = principal directions.\n\nInterpretation: directions along which stress is purely compressive or tensile.\n\n\n\nApplications\n\nCivil engineering: bridge and building safety.\nMechanical engineering: design of engines, aircraft, machines.\nGeophysics: stress in Earth’s crust, earthquakes.\nMedical imaging: elastography for tissue stiffness.\n\n\n\nWhy This Matters\n\nStress and strain are everyday tensor applications in engineering.\nThey demonstrate how multilinear algebra naturally describes geometry and physics.\nProvide a tangible connection between abstract tensors and real-world forces.\n\n\n\nExercises\n\nStrain Calculation: For displacement field \\(u(x,y) = (x+y, y)\\), compute the strain tensor \\(\\varepsilon\\).\nStress on a Plane: If\n\\[\n\\sigma = \\begin{bmatrix} 10 & 2 \\\\ 2 & 5 \\end{bmatrix}, \\quad n = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n\\]\ncompute the force vector \\(f\\).\nSymmetry: Show that both stress and strain tensors are symmetric.\nPrincipal Stresses: Find the eigenvalues of \\(\\sigma = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\). Interpret them.\nThought Experiment: Why is it natural that stress and strain are tensors instead of just vectors?\n\n\n\n\n19.2 Inertia Tensors and Principal Axes\nIn mechanics, the moment of inertia describes how mass distribution resists rotational motion. While for simple objects it is a scalar, in general it is a second-order tensor - the inertia tensor.\n\nDefinition of Inertia Tensor\nFor a rigid body with mass density \\(\\rho(\\mathbf{r})\\), the inertia tensor is:\n\\[\nI_{ij} = \\int \\left( \\|\\mathbf{r}\\|^2 \\delta_{ij} - r_i r_j \\right) \\rho(\\mathbf{r}) \\, dV,\n\\]\nwhere \\(\\mathbf{r} = (x,y,z)\\) is the position vector relative to the chosen origin.\n\n\\(I_{ij}\\) encodes how difficult it is to rotate the body around axis \\(i\\).\nSymmetric: \\(I_{ij} = I_{ji}\\).\n\n\n\nAngular Momentum and Kinetic Energy\nFor angular velocity \\(\\omega\\):\n\nAngular momentum:\n\\[\n\\mathbf{L} = I \\, \\boldsymbol{\\omega}.\n\\]\nRotational kinetic energy:\n\\[\nT = \\tfrac{1}{2} \\boldsymbol{\\omega}^\\top I \\boldsymbol{\\omega}.\n\\]\n\nThus, \\(I\\) acts as the matrix linking angular velocity to angular momentum.\n\n\nPrincipal Axes\n\nInertia tensor can be diagonalized:\n\\[\nI = P \\Lambda P^\\top,\n\\]\nwhere \\(\\Lambda\\) contains principal moments of inertia, and \\(P\\) gives principal axes.\nRotations about principal axes are “decoupled” and simpler to analyze.\n\n\n\nExamples\n\nSolid sphere (mass \\(M\\), radius \\(R\\)):\n\\[\nI = \\tfrac{2}{5} M R^2 I_3.\n\\]\n(isotropic: same inertia around all axes).\nThin rod (length \\(L\\), axis through center):\n\\[\nI = \\tfrac{1}{12} M L^2.\n\\]\nRectangular box: inertia tensor has different diagonal entries depending on edge lengths.\n\n\n\nApplications\n\nMechanical engineering: robotics, aerospace, vehicle dynamics.\nAstronomy: rotation of planets, stability of satellites.\nComputer graphics: simulating rigid-body dynamics in physics engines.\n\n\n\nWhy This Matters\n\nInertia tensors are a clear example of a physical system governed by symmetric tensors.\nPrincipal axes give both mathematical elegance and practical insight (e.g., why objects tumble).\nConnects linear algebra (eigenvalues) directly with physical motion.\n\n\n\nExercises\n\nRod Example: Compute the inertia tensor of a thin rod of length \\(L\\) and mass \\(M\\) lying along the \\(x\\)-axis.\nSphere Symmetry: Show that a solid sphere’s inertia tensor is isotropic (same in all directions).\nPrincipal Axes: Diagonalize\n\\[\nI = \\begin{bmatrix} 5 & 1 & 0 \\\\ 1 & 4 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}.\n\\]\nInterpret the eigenvalues.\nAngular Momentum: For \\(\\omega = (1,0,0)\\) and \\(I = \\mathrm{diag}(2,3,4)\\), compute \\(\\mathbf{L}\\).\nThought Experiment: Why are principal axes of inertia so useful in spacecraft design?\n\n\n\n\n19.3 3D Graphics: Transforms and Shading\nComputer graphics relies heavily on linear and multilinear algebra. Behind every rendered image are tensor operations that handle transformations, lighting, and shading.\n\nHomogeneous Coordinates and Transforms\n\nA 3D point \\((x,y,z)\\) is represented as a 4D vector \\((x,y,z,1)\\).\nTransformations are represented as \\(4 \\times 4\\) matrices:\n\nTranslation, rotation, scaling, perspective projection.\n\nComposition of transformations = matrix multiplication (tensor contraction).\n\nExample:\n\\[\n\\begin{bmatrix}\nx' \\\\ y' \\\\ z' \\\\ 1\n\\end{bmatrix}\n= T R S\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{bmatrix}.\n\\]\n\n\nLighting as a Tensor Operation\nThe Phong reflection model:\n\\[\nI = k_a I_a + k_d (\\mathbf{L} \\cdot \\mathbf{N}) I_d + k_s (\\mathbf{R} \\cdot \\mathbf{V})^n I_s,\n\\]\nwhere\n\n\\(\\mathbf{L}\\) = light direction,\n\\(\\mathbf{N}\\) = surface normal,\n\\(\\mathbf{R}\\) = reflection direction,\n\\(\\mathbf{V}\\) = viewer direction.\n\nEach dot product is a tensor contraction between vectors.\n\n\nNormals and Transformations\n\nSurface normals transform differently than points (using inverse transpose of transformation matrix).\nPreserves correct shading under scaling/shearing.\nAnother case where raising/lowering indices (via metrics) appears in practice.\n\n\n\nShading as Multilinear Maps\n\nShading combines:\n\nLight properties (color, intensity).\nSurface properties (material, texture).\nGeometry (normals, tangents).\n\nThe mapping from these multi-way inputs to final pixel intensity is a multilinear function.\n\n\n\nApplications in Graphics Pipelines\n\nVertex shaders: apply transformations (matrix multiplications).\nFragment shaders: compute color via multilinear lighting models.\nPhysics-based rendering: more advanced tensor models (BRDFs, radiance fields).\n\n\n\nWhy This Matters\n\nBrings tensor ideas into a domain familiar to many learners: 3D graphics.\nShows that rendering engines are, at heart, optimized tensor pipelines.\nBuilds intuition that tensor algebra is not abstract - it powers everyday technology (games, movies, VR).\n\n\n\nExercises\n\nHomogeneous Transform: Write the homogeneous transformation matrix for rotating 90° about the \\(z\\)-axis and then translating by (2,3,0).\nDot Product Lighting: Given \\(\\mathbf{L} = (0,0,1)\\), \\(\\mathbf{N} = (0,0,1)\\), compute the diffuse term in the Phong model.\nNormal Transformation: Explain why a non-uniform scaling requires using the inverse transpose matrix to transform normals.\nMatrix Composition: If \\(M_1\\) is a scaling matrix and \\(M_2\\) is a rotation, what is the composite transformation?\nThought Experiment: How might tensor decompositions (CP, Tucker) be used to compress lighting models or neural radiance fields (NeRFs)?\n\n\n\n\n19.4 Quantum States and Operators\nQuantum mechanics is one of the most natural playgrounds for multilinear algebra: states, observables, and dynamics are all encoded as tensors.\n\nQuantum States as Vectors\n\nA pure quantum state is a vector in a complex Hilbert space:\n\\[\n|\\psi\\rangle \\in \\mathbb{C}^n.\n\\]\nExample: a single qubit is a vector in \\(\\mathbb{C}^2\\):\n\\[\n|\\psi\\rangle = \\alpha |0\\rangle + \\beta |1\\rangle, \\quad |\\alpha|^2 + |\\beta|^2 = 1.\n\\]\n\n\n\nOperators as Matrices (2nd-Order Tensors)\n\nObservables and dynamics are represented as linear operators (Hermitian or unitary matrices).\nMeasurement probabilities come from contractions:\n\\[\np = \\langle \\psi | A | \\psi \\rangle.\n\\]\n\n\n\nComposite Systems and Tensor Products\n\nMulti-particle systems live in the tensor product of state spaces.\nTwo qubits:\n\\[\n\\mathbb{C}^2 \\otimes \\mathbb{C}^2 = \\mathbb{C}^4.\n\\]\nExample entangled state (Bell state):\n\\[\n|\\Phi^+\\rangle = \\tfrac{1}{\\sqrt{2}} (|00\\rangle + |11\\rangle).\n\\]\n\nEntanglement is simply non-separability in tensor terms.\n\n\nDensity Matrices (Mixed States)\n\nGeneral states described by density operator \\(\\rho\\), a positive semidefinite Hermitian matrix with trace 1.\nExpectation values:\n\\[\n\\langle A \\rangle = \\mathrm{Tr}(\\rho A).\n\\]\n\n\n\nQuantum Gates as Tensor Maps\n\nSingle-qubit gates: \\(2 \\times 2\\) unitary matrices (e.g., Pauli matrices).\nMulti-qubit gates: act via Kronecker (tensor) products.\nExample: CNOT = a \\(4 \\times 4\\) matrix acting on two-qubit states.\n\n\n\nApplications\n\nQuantum computing: algorithms rely on tensor contractions for simulating circuits.\nQuantum many-body physics: tensor networks (MPS, PEPS) compress exponential state spaces.\nChemistry: molecular states represented as high-order tensors.\n\n\n\nWhy This Matters\n\nShows how tensors form the mathematical backbone of quantum mechanics.\nExplains entanglement as a tensor phenomenon.\nConnects multilinear algebra directly with one of the most exciting modern sciences.\n\n\n\nExercises\n\nQubit State: Write the state vector for a qubit in superposition \\(|\\psi\\rangle = \\tfrac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)\\).\nOperator Expectation: For Pauli-\\(Z\\) operator \\(\\sigma_z = \\begin{bmatrix}1 & 0 \\\\ 0 & -1\\end{bmatrix}\\), compute \\(\\langle \\psi | \\sigma_z | \\psi \\rangle\\) for \\(|\\psi\\rangle\\) above.\nTensor Product: Compute \\(|0\\rangle \\otimes |1\\rangle\\) explicitly as a vector in \\(\\mathbb{C}^4\\).\nCNOT Gate: Write the \\(4 \\times 4\\) matrix representation of the CNOT gate.\nThought Experiment: Why are tensor decompositions (MPS, PEPS, TT) essential for simulating quantum many-body systems efficiently?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-20.-manifolds-and-tensor-fields-preview",
    "href": "index.html#chapter-20.-manifolds-and-tensor-fields-preview",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 20. Manifolds and Tensor Fields (Preview)",
    "text": "Chapter 20. Manifolds and Tensor Fields (Preview)\n\n20.1 Tangent and Cotangent Bundles\nSo far, we treated tensors on vector spaces with fixed bases. In geometry and physics, tensors often live on manifolds, where each point has its own tangent space. The tangent and cotangent bundles provide the foundation for tensor fields.\n\nTangent Space at a Point\nFor a smooth manifold \\(M\\) and point \\(p \\in M\\):\n\nThe tangent space \\(T_p M\\) is the vector space of all possible velocity vectors of curves through \\(p\\).\nDimension of \\(T_p M\\) = dimension of \\(M\\).\nExample: On a sphere \\(S^2\\), \\(T_p S^2\\) is the plane tangent to the sphere at \\(p\\).\n\n\n\nTangent Bundle\nThe tangent bundle is the union of all tangent spaces:\n\\[\nTM = \\bigsqcup_{p \\in M} T_p M.\n\\]\n\nA smooth manifold itself (of dimension \\(2n\\) if \\(\\dim M = n\\)).\nA point in \\(TM\\) is a pair \\((p, v)\\) with \\(v \\in T_p M\\).\n\n\n\nCotangent Space and Bundle\n\nThe cotangent space \\(T^*_p M\\) is the dual space of \\(T_p M\\): linear functionals on tangent vectors.\nElements are called covectors (or differential 1-forms).\nThe cotangent bundle \\(T^*M = \\bigsqcup_{p \\in M} T^*_p M\\).\n\n\n\nLocal Coordinates\nIf \\(M\\) has coordinates \\((x^1, \\dots, x^n)\\):\n\nBasis of tangent space: \\(\\{\\partial/\\partial x^i\\}\\).\nBasis of cotangent space: \\(\\{dx^i\\}\\).\nAny tangent vector \\(v \\in T_p M\\):\n\\[\nv = \\sum_i v^i \\frac{\\partial}{\\partial x^i}.\n\\]\nAny covector \\(\\omega \\in T^*_p M\\):\n\\[\n\\omega = \\sum_i \\omega_i dx^i.\n\\]\n\n\n\nWhy This Matters\n\nTangent and cotangent bundles generalize the vector/covector distinction from linear algebra to curved spaces.\nThey form the stage on which tensor fields (next sections) live.\nCentral in physics: tangent = velocities, cotangent = momenta.\n\n\n\nExercises\n\nTangent Space: On the circle \\(S^1\\), describe the tangent space at the point \\((1,0)\\).\nCotangent Basis: In \\(\\mathbb{R}^2\\) with coordinates \\((x,y)\\), what are the basis vectors of the tangent and cotangent spaces?\nPairing: For \\(v = v^1 \\partial/\\partial x + v^2 \\partial/\\partial y\\) and \\(\\omega = \\omega_1 dx + \\omega_2 dy\\), compute \\(\\omega(v)\\).\nBundle Structure: Explain why the tangent bundle of a 2D manifold has dimension 4.\nThought Experiment: Why is momentum naturally a covector (in \\(T^*_p M\\)) instead of a vector?\n\n\n\n\n20.2 Tensor Fields and Coordinate Changes\nSo far we have defined tangent and cotangent spaces at a single point. To do geometry and physics, we need tensors that vary smoothly across a manifold. These are called tensor fields.\n\nTensor Fields\n\nA tensor field of type (r,s) assigns to each point \\(p \\in M\\) a tensor\n\\[\nT(p) \\in (T_pM)^{\\otimes r} \\otimes (T^*_pM)^{\\otimes s}.\n\\]\nExamples:\n\nVector field = (1,0) tensor field (assigns a tangent vector at each point).\nCovector field (1-form): (0,1) tensor field.\nMetric: (0,2) symmetric tensor field.\n\n\n\n\nCoordinate Expressions\nIf \\((x^1,\\dots,x^n)\\) are local coordinates, then:\n\nA vector field:\n\\[\nX = \\sum_i X^i(x) \\frac{\\partial}{\\partial x^i}.\n\\]\nA covector field:\n\\[\n\\omega = \\sum_i \\omega_i(x) dx^i.\n\\]\nA general (r,s) tensor field:\n\\[\nT = \\sum T^{i_1 \\cdots i_r}{}_{j_1 \\cdots j_s}(x)\n\\frac{\\partial}{\\partial x^{i_1}} \\otimes \\cdots \\otimes \\frac{\\partial}{\\partial x^{i_r}}\n\\otimes dx^{j_1} \\otimes \\cdots \\otimes dx^{j_s}.\n\\]\n\n\n\nCoordinate Transformations\nIf we change coordinates from \\(x^i\\) to \\(\\tilde{x}^j\\):\n\nBasis vectors transform as\n\\[\n\\frac{\\partial}{\\partial \\tilde{x}^j} = \\sum_i \\frac{\\partial x^i}{\\partial \\tilde{x}^j} \\frac{\\partial}{\\partial x^i}.\n\\]\nDual basis transforms oppositely:\n\\[\nd\\tilde{x}^j = \\sum_i \\frac{\\partial \\tilde{x}^j}{\\partial x^i} dx^i.\n\\]\nTensor components transform with a mix of both rules (contravariant and covariant).\n\nExample: For a (1,1) tensor field \\(A^i{}_j\\):\n\\[\n\\tilde{A}^i{}_j =\n\\frac{\\partial \\tilde{x}^i}{\\partial x^p}\n\\frac{\\partial x^q}{\\partial \\tilde{x}^j}\nA^p{}_q.\n\\]\n\n\nWhy This Matters\n\nTensor fields generalize the coordinate-free viewpoint: the object is intrinsic, components adapt to coordinates.\nPhysics laws are tensorial: their form is preserved under coordinate transformations.\nExplains why tensors are the “language of nature” in relativity and continuum mechanics.\n\n\n\nExercises\n\nVector Field: Write the vector field \\(X = x \\frac{\\partial}{\\partial x} + y \\frac{\\partial}{\\partial y}\\) on \\(\\mathbb{R}^2\\).\n1-Form Field: Write the 1-form \\(\\omega = x \\, dx + y \\, dy\\). Evaluate \\(\\omega(X)\\) for the vector field above.\nTransformation Rule: Show how a vector field \\(X^i\\) transforms under coordinate change \\(x^i \\mapsto \\tilde{x}^j\\).\nMixed Tensor: Verify the transformation law for a (1,1) tensor \\(A^i{}_j\\).\nThought Experiment: Why is it essential in physics that tensorial equations look the same in any coordinate system?\n\n\n\n\n20.3 Covariant Derivatives and Curvature\nOn flat spaces like \\(\\mathbb{R}^n\\), derivatives of vector fields are straightforward. On curved manifolds, however, we cannot subtract vectors at different points directly because they belong to different tangent spaces. The covariant derivative solves this problem and leads naturally to curvature.\n\nCovariant Derivative\n\nFor a vector field \\(X\\) and another vector field \\(Y\\), the covariant derivative \\(\\nabla_X Y\\) measures how \\(Y\\) changes along \\(X\\).\nUnlike the usual derivative, \\(\\nabla_X Y \\in T_p M\\), so it lives in the tangent space at the same point.\n\nIn coordinates \\((x^i)\\):\n\\[\n\\nabla_i Y^j = \\frac{\\partial Y^j}{\\partial x^i} + \\Gamma^j_{ik} Y^k,\n\\]\nwhere \\(\\Gamma^j_{ik}\\) are the Christoffel symbols of the connection.\n\n\nParallel Transport\n\nParallel transport moves a vector along a curve while keeping it “as constant as possible.”\nDepends on the connection.\nIn Euclidean space, this agrees with ordinary translation; on curved manifolds (like spheres), the result depends on the path.\n\n\n\nCurvature Tensor\nThe failure of parallel transport to be path-independent is measured by the Riemann curvature tensor:\n\\[\nR^i{}_{jkl} = \\partial_k \\Gamma^i_{jl} - \\partial_l \\Gamma^i_{jk}\n+ \\Gamma^i_{km} \\Gamma^m_{jl} - \\Gamma^i_{lm} \\Gamma^m_{jk}.\n\\]\n\nIf \\(R=0\\), the manifold is flat (locally Euclidean).\nNonzero \\(R\\) encodes intrinsic curvature.\n\n\n\nRicci Tensor and Scalar Curvature\n\nContracting indices of the Riemann tensor gives the Ricci tensor \\(R_{ij}\\).\nFurther contraction gives the scalar curvature \\(R\\).\nCentral in Einstein’s field equations of general relativity:\n\\[\nG_{ij} = R_{ij} - \\tfrac{1}{2} g_{ij} R.\n\\]\n\n\n\nWhy This Matters\n\nCovariant derivative generalizes differentiation to curved spaces.\nCurvature is intrinsic: no embedding is needed to detect it.\nThese ideas connect multilinear algebra to geometry, relativity, and modern physics.\n\n\n\nExercises\n\nFlat Space: Show that in Euclidean coordinates, Christoffel symbols vanish and the covariant derivative reduces to the usual derivative.\nSphere Example: Explain why parallel transport around a closed loop on a sphere rotates a vector.\nRiemann Tensor Symmetries: Verify that \\(R^i{}_{jkl} = -R^i{}_{jlk}\\).\nRicci Contraction: Show how to obtain \\(R_{ij}\\) from \\(R^k{}_{ikj}\\).\nThought Experiment: Why does general relativity require curvature, while Newtonian gravity does not?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#chapter-21.-representation-theory-and-invariants-preview",
    "href": "index.html#chapter-21.-representation-theory-and-invariants-preview",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Chapter 21. Representation Theory and Invariants (Preview)",
    "text": "Chapter 21. Representation Theory and Invariants (Preview)\n\n21.1 Group Actions on Tensor Spaces\nSymmetry plays a central role in mathematics and physics. Groups capture symmetry, and their actions on tensor spaces reveal invariants and structure. This section introduces how groups act on tensors and why this matters.\n\nGroup Actions\n\nA group action of \\(G\\) on a vector space \\(V\\) is a map\n\\[\nG \\times V \\to V, \\quad (g,v) \\mapsto g \\cdot v,\n\\]\nsuch that \\(e \\cdot v = v\\) (identity acts trivially) and \\((gh)\\cdot v = g\\cdot(h\\cdot v)\\).\nExample: The rotation group \\(SO(3)\\) acts on \\(\\mathbb{R}^3\\) by matrix multiplication.\n\n\n\nGroup Actions on Tensor Products\nIf a group \\(G\\) acts on \\(V\\), then it acts naturally on tensor powers:\n\\[\ng \\cdot (v_1 \\otimes v_2 \\otimes \\cdots \\otimes v_k)\n= (g \\cdot v_1) \\otimes (g \\cdot v_2) \\otimes \\cdots \\otimes (g \\cdot v_k).\n\\]\nThus, tensors inherit group actions from their underlying vector spaces.\n\n\nExamples\n\nRotations on Vectors: In physics, tensors transform under coordinate rotations. Stress and strain tensors are invariant laws expressed under such actions.\nPermutation Group: Acts on tensor indices by permuting them. Leads to symmetric and antisymmetric tensors.\nGeneral Linear Group \\(GL(n)\\): Natural action on \\(\\mathbb{R}^n\\). Extends to higher-order tensors, explaining transformation rules under basis changes.\n\n\n\nWhy Group Actions Matter\n\nProvide the language for defining invariants: tensorial equations remain true under group actions.\nIn physics: laws of nature are symmetric under rotations, Lorentz transformations, gauge groups.\nIn data science: invariance to permutations, rotations, or scalings improves model generalization.\n\n\n\nTensor Invariants\n\nA tensor is invariant under a group if \\(g \\cdot T = T\\) for all \\(g \\in G\\).\nExample: the Euclidean inner product \\(\\langle x,y\\rangle\\) is invariant under \\(SO(n)\\).\nDeterminants, traces, and volume forms are other classical invariants.\n\n\n\nExercises\n\nVector Rotation: Show that the Euclidean norm \\(\\|x\\|^2 = x_1^2 + x_2^2 + x_3^2\\) is invariant under \\(SO(3)\\).\nPermutation Action: Describe how the permutation \\((12)\\) acts on a tensor \\(T_{ijk}\\).\nGL(n) Action: For a (1,1) tensor \\(A^i{}_j\\), write its transformation rule under \\(GL(n)\\).\nInvariant Tensor: Prove that the Kronecker delta \\(\\delta_{ij}\\) is invariant under orthogonal transformations.\nThought Experiment: Why is invariance under certain group actions a guiding principle in formulating physical laws?\n\n\n\n\n21.2 Invariant Tensors and Symmetry\nInvariant tensors capture quantities that remain unchanged under group actions. They are the backbone of conservation laws in physics, canonical forms in mathematics, and inductive biases in machine learning.\n\nDefinition\nA tensor \\(T\\) is invariant under a group \\(G\\) if\n\\[\ng \\cdot T = T \\quad \\text{for all } g \\in G.\n\\]\n\nExample: Under the rotation group \\(SO(n)\\), the Kronecker delta \\(\\delta_{ij}\\) is invariant.\nExample: The Levi-Civita symbol \\(\\varepsilon_{ijk}\\) is invariant under \\(SO(3)\\) but changes sign under reflections.\n\n\n\nClassical Invariant Tensors\n\nInner Product: \\(\\langle x, y \\rangle = \\delta_{ij} x^i y^j\\) is invariant under orthogonal transformations.\nDeterminant: Expressed with \\(\\varepsilon_{i_1 i_2 \\dots i_n}\\), invariant under \\(SL(n)\\) (special linear group).\nVolume Form: Orientation-preserving transformations preserve volume.\nMetric Tensor \\(g_{ij}\\): Fundamental invariant under coordinate changes, defining distances.\n\n\n\nInvariant Theory\n\nStudies polynomial functions or tensors that remain unchanged under group actions.\nExample: Symmetric polynomials invariant under the permutation group \\(S_n\\).\nExample: Trace and determinant are invariants under conjugation by \\(GL(n)\\).\n\n\n\nPhysics Examples\n\nConservation of energy and momentum ↔︎ invariance under time and space translations (Noether’s theorem).\nAngular momentum ↔︎ invariance under rotations.\nGauge invariants define observable quantities in quantum field theory.\n\n\n\nApplications in Data Science & ML\n\nDesigning models invariant to certain transformations (e.g., convolutional nets = translation invariance).\nGraph neural networks = invariance under node permutations.\nTensor methods enforce symmetries directly in architecture.\n\n\n\nWhy This Matters\n\nInvariants identify essential quantities independent of coordinates or representation.\nSymmetry reduces complexity: from many possible features to a few invariant ones.\nThis unifies physics, pure math, and modern AI design.\n\n\n\nExercises\n\nRotation Invariance: Show explicitly that \\(\\delta_{ij}\\) is unchanged under rotation matrices \\(R \\in SO(3)\\).\nLevi-Civita: Verify that \\(\\varepsilon_{ijk}\\) changes sign under reflection (determinant = –1).\nDeterminant: Why is the determinant invariant under \\(SL(n)\\) but not under all of \\(GL(n)\\)?\nGraph Example: Explain why node permutation invariance is crucial in graph neural networks.\nThought Experiment: Why do invariants often correspond to conserved physical quantities?\n\n\n\n\n21.3 Why Invariants Matter in Algorithms\nInvariants are not just elegant mathematical objects - they are practical tools that make algorithms more robust, efficient, and interpretable. When an algorithm respects the right invariants, it exploits symmetry instead of fighting it.\n\nInvariants Reduce Redundancy\n\nWithout invariants, algorithms may waste time learning the same thing in multiple coordinate systems.\nExample: PCA uses covariance, which is rotation-invariant, so it doesn’t matter how data is oriented.\nExample: Graph algorithms often rely on permutation-invariant structures (degree, adjacency spectrum).\n\n\n\nInvariants Improve Robustness\n\nIf an algorithm outputs the same result under symmetry transformations, results are stable and reproducible.\nExample: CNNs are translation-invariant, making them robust to shifts in input images.\nExample: Physics simulations rely on energy invariants for numerical stability.\n\n\n\nInvariants Simplify Computation\n\nMany invariants collapse high-dimensional data into simpler summaries.\nExample: Determinant summarizes a matrix’s volume-scaling property.\nExample: Tensor contractions with invariant tensors (like \\(\\delta_{ij}\\) or \\(\\varepsilon_{ijk}\\)) simplify calculations.\n\n\n\nAlgorithm Design via Invariants\n\nSignal processing: invariant features (moments, cumulants) used for blind source separation.\nMachine learning: group-equivariant networks encode invariances (rotations, permutations).\nOptimization: invariance-aware preconditioning improves convergence.\n\n\n\nWhy This Matters\n\nInvariants let us design algorithms that generalize better, by focusing only on structure that truly matters.\nThey connect deep theory (group actions, tensor algebra) with practical implementations (CNNs, GNNs, physics engines).\nUnderstanding invariants is key to the next generation of geometry- and symmetry-aware AI systems.\n\n\n\nExercises\n\nRotation-Invariant Feature: For a set of 2D points, explain why pairwise distances are invariant under rotations and translations.\nPermutation-Invariance: Show that the sum of node features in a graph is invariant under permutations of node labels.\nAlgorithm Stability: Why does enforcing conservation of energy in a simulation improve long-term stability?\nInvariant Contraction: Use \\(\\delta_{ij}\\) to show that contracting \\(A_{ij}\\delta_{ij}\\) yields the trace of \\(A\\), an invariant.\nThought Experiment: If a machine learning model ignores known invariances in the data, what are the risks?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#a.-symbols-and-notation-cheatsheet",
    "href": "index.html#a.-symbols-and-notation-cheatsheet",
    "title": "The Little Book of Multilinear Algebra",
    "section": "A. Symbols and Notation Cheatsheet",
    "text": "A. Symbols and Notation Cheatsheet\nThis appendix gathers the most common symbols used throughout the book. Use it as a quick reference when working through chapters and exercises.\n\nVector Spaces and Duals\n\n\\(V, W, U\\) - vector spaces\n\\(\\dim V\\) - dimension of \\(V\\)\n\\(V^*\\) - dual space (space of linear functionals on \\(V\\))\n\\(v \\in V\\) - vector\n\\(\\alpha \\in V^*\\) - covector (linear form)\n\n\n\nTensors\n\n\\(T^r_s(V)\\) - space of tensors of type (r,s) over \\(V\\)\n\\(u \\otimes v\\) - tensor (outer) product of \\(u\\) and \\(v\\)\n\\(T_{i_1 \\dots i_s}^{j_1 \\dots j_r}\\) - components of a (r,s) tensor\nContraction - summing over one upper and one lower index\nSymmetric / antisymmetric tensors - invariant or alternating under index permutations\n\n\n\nIndices and Summation\n\n\\(i,j,k,l,m,n\\) - generic indices\nEinstein summation convention - repeated upper/lower indices are summed over\n\\(\\delta_{ij}\\) - Kronecker delta, identity for index contraction\n\\(\\varepsilon_{ijk}\\) - Levi-Civita symbol (totally antisymmetric, used for cross products, determinants)\n\n\n\nLinear Maps and Operators\n\n\\(A: V \\to W\\) - linear map from \\(V\\) to \\(W\\)\n\\(A^i{}_j\\) - components of a (1,1) tensor, i.e., a linear operator\n\\(\\mathrm{tr}(A)\\) - trace of operator \\(A\\)\n\\(\\det(A)\\) - determinant of \\(A\\)\n\n\n\nTensor Operations\n\n\\(\\otimes\\) - tensor product\n\\(\\wedge\\) - wedge product (alternating tensor product)\n\\(\\times_n\\) - mode-\\(n\\) product of a tensor with a matrix\n\\(\\mathrm{vec}(\\cdot)\\) - vectorization operator (flatten tensor into a vector)\n\\(\\mathrm{Tr}(\\cdot)\\) - matrix trace\n\\(\\nabla\\) - gradient / covariant derivative\n\n\n\nSpecial Objects\n\n\\(g_{ij}\\) - metric tensor\n\\(R^i{}_{jkl}\\) - Riemann curvature tensor\n\\(R_{ij}\\) - Ricci tensor\n\\(R\\) - scalar curvature\n\\(|v|\\) or \\(\\|v\\|\\) - norm of a vector\n\\(\\langle u, v \\rangle\\) - inner product of \\(u\\) and \\(v\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-b.-proof-sketches-of-core-theorems",
    "href": "index.html#appendix-b.-proof-sketches-of-core-theorems",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix B. Proof Sketches of Core Theorems",
    "text": "Appendix B. Proof Sketches of Core Theorems\n\nAppendix B.1 Universal Property of the Tensor Product (Proof Sketch)\nThe tensor product \\(V \\otimes W\\) is not just a convenient construction: it is characterized uniquely by its universal property. This section gives an intuitive sketch of the proof.\n\nStatement of the Universal Property\nGiven vector spaces \\(V, W\\) over a field \\(\\mathbb{F}\\):\n\nThere exists a vector space \\(V \\otimes W\\) together with a bilinear map\n\\[\n\\otimes : V \\times W \\to V \\otimes W\n\\]\nsuch that:\n\nFor any bilinear map \\(f : V \\times W \\to U\\) into another vector space \\(U\\), there exists a unique linear map\n\\[\n\\tilde{f}: V \\otimes W \\to U\n\\]\nsatisfying\n\\[\nf(v,w) = \\tilde{f}(v \\otimes w).\n\\]\n\n\nIdea of the Proof\n\nBuild a candidate space:\n\nStart with the free vector space generated by pairs \\((v,w)\\).\nImpose relations to enforce bilinearity:\n\n\\((v_1+v_2, w) \\sim (v_1, w) + (v_2, w)\\)\n\\((v, w_1+w_2) \\sim (v, w_1) + (v, w_2)\\)\n\\((\\alpha v, w) \\sim \\alpha (v,w)\\), \\((v, \\alpha w) \\sim \\alpha (v,w)\\).\n\n\nThe quotient space is defined as \\(V \\otimes W\\).\nDefine the canonical bilinear map:\n\\[\n(v,w) \\mapsto v \\otimes w.\n\\]\nFactorization property:\n\nFor any bilinear map \\(f: V \\times W \\to U\\), define \\(\\tilde{f}\\) by\n\\[\n\\tilde{f}(v \\otimes w) = f(v,w).\n\\]\nThis is well-defined because the relations in step 1 match the bilinear properties of \\(f\\).\n\nUniqueness:\n\nAny linear map \\(\\tilde{f}\\) satisfying the condition must agree on generators \\(v \\otimes w\\).\nSince these generate the whole space, \\(\\tilde{f}\\) is unique.\n\n\n\n\nWhy This Matters\n\nThe tensor product is defined *not- by coordinates but by this universal property.\nIt guarantees that tensors are the natural home for bilinear (and multilinear) maps.\nIn applications, this explains why changing bases, reshaping, or contracting tensors always behaves consistently.\n\n\n\nExercises\n\nMatrix Multiplication as a Bilinear Map: Show that the bilinear map \\(f: \\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}\\), \\(f(x,y) = x^\\top A y\\), factors through a linear map on \\(\\mathbb{R}^n \\otimes \\mathbb{R}^m\\).\nDimension Formula: Using a basis \\(\\{e_i\\}\\) for \\(V\\) and \\(\\{f_j\\}\\) for \\(W\\), prove that \\(\\{e_i \\otimes f_j\\}\\) is a basis of \\(V \\otimes W\\).\nUniqueness Check: Why can’t two different linear maps \\(\\tilde{f}_1, \\tilde{f}_2 : V \\otimes W \\to U\\) both satisfy \\(f(v,w) = \\tilde{f}(v \\otimes w)\\)?\nFree Vector Space Analogy: Compare the construction of \\(V \\otimes W\\) to the way free groups or free vector spaces are defined.\nThought Experiment: If the tensor product is defined via a universal property, why does this make it “canonical” (independent of choices)?\n\n\n\n\nAppendix B.2 Dimension Formula for Tensor Products (Proof Sketch)\nThe tensor product has a clean relationship between dimensions:\n\\[\n\\dim(V \\otimes W) = \\dim(V) \\cdot \\dim(W).\n\\]\nHere’s a sketch of why this is true.\n\nStep 1. Choose Bases\n\nLet \\(\\{e_i\\}_{i=1}^m\\) be a basis for \\(V\\).\nLet \\(\\{f_j\\}_{j=1}^n\\) be a basis for \\(W\\).\n\n\n\nStep 2. Construct Tensor Basis\n\nConsider all simple tensors \\(e_i \\otimes f_j\\), with \\(1 \\leq i \\leq m\\), \\(1 \\leq j \\leq n\\).\nThere are exactly \\(mn\\) such tensors.\n\n\n\nStep 3. Spanning Argument\n\nAny element of \\(V \\otimes W\\) is a linear combination of simple tensors \\(v \\otimes w\\).\nExpanding \\(v = \\sum a_i e_i\\), \\(w = \\sum b_j f_j\\), we get\n\\[\nv \\otimes w = \\sum_{i,j} a_i b_j \\,(e_i \\otimes f_j).\n\\]\nTherefore, all tensors are linear combinations of \\(\\{e_i \\otimes f_j\\}\\).\n\n\n\nStep 4. Linear Independence\n\nSuppose \\(\\sum_{i,j} c_{ij} \\, (e_i \\otimes f_j) = 0\\).\nApply the universal property with bilinear maps defined by coordinate projections.\nOne shows all coefficients \\(c_{ij}\\) must vanish.\nHence, \\(\\{e_i \\otimes f_j\\}\\) is linearly independent.\n\n\n\nConclusion\n\n\\(\\{e_i \\otimes f_j\\}\\) is a basis.\nThus, \\(\\dim(V \\otimes W) = mn = \\dim(V) \\cdot \\dim(W)\\).\n\n\n\nWhy This Matters\n\nDimension formula ensures that tensor products don’t “create new degrees of freedom” beyond combinations of existing bases.\nMakes tensor products predictable: if \\(V = \\mathbb{R}^m\\) and \\(W = \\mathbb{R}^n\\), then \\(V \\otimes W \\cong \\mathbb{R}^{mn}\\).\nExplains why tensor reshaping between matrices, higher arrays, and Kronecker products is consistent.\n\n\n\nExercises\n\nBasis Construction: For \\(V = \\mathbb{R}^2\\) with basis \\(\\{e_1,e_2\\}\\) and \\(W = \\mathbb{R}^3\\) with basis \\(\\{f_1,f_2,f_3\\}\\), explicitly list the basis of \\(V \\otimes W\\).\nCounting Dimensions: If \\(\\dim(V)=4\\) and \\(\\dim(W)=5\\), what is \\(\\dim(V \\otimes W)\\)?\nMatrix Analogy: Show that \\(V \\otimes W\\) with chosen bases is isomorphic to the space of \\(m \\times n\\) matrices.\nIndependence Check: Prove linear independence of \\(\\{e_i \\otimes f_j\\}\\) by applying a bilinear map \\(f(e_i,f_j) = \\delta_{ii_0}\\delta_{jj_0}\\).\nThought Experiment: How does this dimension formula generalize to \\(V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_k\\)?\n\n\n\n\nAppendix B.3 Decomposition of Tensors into Symmetric and Antisymmetric Parts (Proof Sketch)\nAny tensor with two indices can be decomposed uniquely into a symmetric and an antisymmetric component. This is a fundamental structural result and often the first place where symmetry in tensors becomes useful.\n\nStatement\nLet \\(T \\in V \\otimes V\\) (a bilinear form, or a \\((0,2)\\)-tensor). Then:\n\\[\nT = \\tfrac{1}{2}(T + T^\\top) \\;+\\; \\tfrac{1}{2}(T - T^\\top),\n\\]\nwhere\n\n\\(S = \\tfrac{1}{2}(T + T^\\top)\\) is symmetric,\n\\(A = \\tfrac{1}{2}(T - T^\\top)\\) is antisymmetric, and the decomposition is unique.\n\n\n\nProof Sketch\n\nSymmetrization and Antisymmetrization Operators\n\nDefine the symmetrization operator:\n\\[\n\\text{Sym}(T)(u,v) = \\tfrac{1}{2}[T(u,v) + T(v,u)].\n\\]\nDefine the antisymmetrization operator:\n\\[\n\\text{Alt}(T)(u,v) = \\tfrac{1}{2}[T(u,v) - T(v,u)].\n\\]\n\nLinearity and Decomposition\n\nBoth operators are linear.\nFor any \\(u,v\\):\n\\[\nT(u,v) = \\text{Sym}(T)(u,v) + \\text{Alt}(T)(u,v).\n\\]\n\nUniqueness\n\nSuppose \\(T = S + A\\) with \\(S\\) symmetric and \\(A\\) antisymmetric.\nThen \\(S = \\text{Sym}(T)\\) and \\(A = \\text{Alt}(T)\\).\nNo other decomposition is possible, proving uniqueness.\n\n\n\n\nExample\nMatrix form:\n\\[\nT = \\begin{bmatrix} 1 & 3 \\\\ 4 & 2 \\end{bmatrix}.\n\\]\n\nSymmetric part:\n\\[\nS = \\tfrac{1}{2}\\left(\\begin{bmatrix} 1 & 3 \\\\ 4 & 2 \\end{bmatrix} + \\begin{bmatrix} 1 & 4 \\\\ 3 & 2 \\end{bmatrix}\\right)\n= \\begin{bmatrix} 1 & 3.5 \\\\ 3.5 & 2 \\end{bmatrix}.\n\\]\nAntisymmetric part:\n\\[\nA = \\tfrac{1}{2}\\left(\\begin{bmatrix} 1 & 3 \\\\ 4 & 2 \\end{bmatrix} - \\begin{bmatrix} 1 & 4 \\\\ 3 & 2 \\end{bmatrix}\\right)\n= \\begin{bmatrix} 0 & -0.5 \\\\ 0.5 & 0 \\end{bmatrix}.\n\\]\n\nSo \\(T = S + A\\).\n\n\nWhy This Matters\n\nSymmetric tensors capture “energy-like” or metric quantities.\nAntisymmetric tensors capture orientation, area, and rotational effects.\nIn physics: stress tensor = symmetric, electromagnetic field tensor = antisymmetric.\n\n\n\nExercises\n\nCompute the Decomposition: Decompose\n\\[\nT = \\begin{bmatrix} 0 & 2 \\\\ -1 & 3 \\end{bmatrix}\n\\]\ninto symmetric and antisymmetric parts.\nUniqueness Check: Why can’t a nonzero symmetric matrix also be antisymmetric?\nDimension Argument: Show that in \\(\\mathbb{R}^n\\):\n\nDimension of symmetric 2-tensors = \\(\\tfrac{n(n+1)}{2}\\).\nDimension of antisymmetric 2-tensors = \\(\\tfrac{n(n-1)}{2}\\).\nTotal adds up to \\(n^2\\).\n\nApplication in Physics: Which parts of the strain tensor (from mechanics) and electromagnetic field tensor (from relativity) correspond to symmetric and antisymmetric parts?\nThought Experiment: Can you imagine a scenario where the antisymmetric part of a tensor carries more physical information than the symmetric part? ### Appendix B.4 Rank Decomposition for Matrices and Extension to CP Rank (Proof Sketch)\n\nThe concept of rank begins with matrices and extends to higher-order tensors. This appendix sketches the reasoning behind matrix rank decomposition and how it generalizes to the Canonical Polyadic (CP) decomposition for tensors.\n\n\nMatrix Rank Decomposition\nStatement: Any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) of rank \\(r\\) can be written as a sum of \\(r\\) rank-one matrices.\nFormally:\n\\[\nA = \\sum_{k=1}^r u^{(k)} (v^{(k)})^\\top,\n\\]\nwith \\(u^{(k)} \\in \\mathbb{R}^m\\), \\(v^{(k)} \\in \\mathbb{R}^n\\).\nSketch of Proof:\n\nColumn Space Basis:\n\nSince \\(\\text{rank}(A) = r\\), there exist \\(r\\) independent columns.\nLet \\(u^{(1)}, \\dots, u^{(r)}\\) be these basis vectors.\n\nExpansion of Columns:\n\nEach column of \\(A\\) is a linear combination of \\(\\{u^{(k)}\\}\\).\nThus, \\(A\\) can be written as a sum of outer products between \\(u^{(k)}\\) and suitable coefficient vectors \\(v^{(k)}\\).\n\nMinimality:\n\nNo fewer than \\(r\\) terms suffice: otherwise, the column space dimension would drop below \\(r\\).\n\n\nThis proves the decomposition exists and is minimal.\n\n\nExtension to Tensors: CP Decomposition\nFor a 3rd-order tensor \\(T \\in \\mathbb{R}^{I \\times J \\times K}\\):\n\\[\nT_{ijk} = \\sum_{r=1}^R a^{(r)}_i \\, b^{(r)}_j \\, c^{(r)}_k,\n\\]\nor equivalently,\n\\[\nT = \\sum_{r=1}^R a^{(r)} \\otimes b^{(r)} \\otimes c^{(r)}.\n\\]\n\n\\(R\\) = tensor rank (minimal number of rank-one tensors).\nUnlike matrices, computing \\(R\\) is much harder (NP-hard in general).\n\n\n\nKey Differences Between Matrix Rank and Tensor Rank\n\nMatrices: rank \\(\\leq \\min(m,n)\\).\nTensors: rank can exceed dimensions, and uniqueness properties are more subtle.\nMatrix SVD: always gives orthogonal decomposition.\nTensor CP: uniqueness requires conditions (e.g., Kruskal’s condition).\n\n\n\nWhy This Matters\n\nRank decomposition gives the conceptual backbone of low-rank approximation.\nCP decomposition underlies applications in data compression, chemometrics, neuroscience, and machine learning.\nShows how a simple linear algebra property grows into a rich multilinear theory.\n\n\n\nExercises\n\nMatrix Rank-One Decomposition: Decompose\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\n\\]\ninto rank-one matrices.\nMinimality Check: Why can’t the above \\(A\\) be written as a single rank-one matrix?\nTensor Example: Express the tensor \\(T_{ijk} = \\delta_{ij}\\delta_{jk}\\) (the identity cube) as a CP decomposition.\nRank Bound: Show that for \\(T \\in \\mathbb{R}^{I \\times J \\times K}\\), the rank is at most \\(\\min(IJ, JK, IK)\\).\nThought Experiment: Why might tensor rank decomposition be harder- but also more useful- than matrix rank decomposition in data science? ### Appendix B.5 Identifiability Conditions for CP Decomposition (Proof Sketch)\n\nOne of the most important questions in tensor decomposition is uniqueness: when is a CP (Canonical Polyadic) decomposition determined uniquely (up to permutation and scaling)? This property is called identifiability.\n\n\nStatement (Informal)\nA CP decomposition\n\\[\nT = \\sum_{r=1}^R a^{(r)} \\otimes b^{(r)} \\otimes c^{(r)}\n\\]\nis essentially unique (unique up to permutation and scaling of terms) if the factor matrices \\([a^{(1)} \\ \\dots \\ a^{(R)}]\\), \\([b^{(1)} \\ \\dots \\ b^{(R)}]\\), \\([c^{(1)} \\ \\dots \\ c^{(R)}]\\) satisfy certain rank conditions.\n\n\nKruskal’s Theorem (Key Result)\nLet \\(A \\in \\mathbb{R}^{I \\times R}, B \\in \\mathbb{R}^{J \\times R}, C \\in \\mathbb{R}^{K \\times R}\\). Define the Kruskal rank of a matrix \\(M\\), written \\(k_M\\), as the maximum number such that every subset of \\(k_M\\) columns is linearly independent.\nKruskal’s condition: If\n\\[\nk_A + k_B + k_C \\geq 2R + 2,\n\\]\nthen the CP decomposition is unique (up to trivial indeterminacies).\n\n\nSketch of Why This Works\n\nOvercompleteness and Mixing:\n\nWithout conditions, many different sets of factors can represent the same tensor.\n\nColumn Independence:\n\nKruskal rank ensures that subsets of columns are independent, ruling out hidden degeneracies.\n\nBalancing the Inequality:\n\nThe inequality ensures enough “spread” across modes so that factors can’t be swapped or recombined into different decompositions.\n\nResult:\n\nAny alternative decomposition must match the original one (up to permutation and scaling).\n\n\n\n\nIntuition\n\nFor matrices, SVD gives uniqueness (up to rotations) because of orthogonality.\nFor tensors, uniqueness is subtler: CP decomposition is often more unique than matrix decompositions.\nThis uniqueness makes tensors powerful in applications like blind source separation and latent-variable modeling.\n\n\n\nWhy This Matters\n\nGuarantees that discovered latent factors have meaning.\nCritical in chemometrics, neuroscience, psychometrics, and machine learning.\nExplains why tensor methods succeed where matrix methods fail: uniqueness despite underdetermined settings.\n\n\n\nExercises\n\nKruskal Rank: Compute the Kruskal rank of\n\\[\nA = \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\nUniqueness Check: Why is CP decomposition with \\(R=1\\) always unique?\nMatrix vs Tensor: Compare uniqueness of matrix rank decomposition with tensor CP decomposition. Which one is stricter?\nPractical Implication: In blind source separation, why is uniqueness of CP decomposition essential?\nThought Experiment: Why might identifiability be a blessing- in data analysis but a curse- for numerical algorithms?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-d.-identities-cookbook",
    "href": "index.html#appendix-d.-identities-cookbook",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix D. Identities & “Cookbook”",
    "text": "Appendix D. Identities & “Cookbook”\nThis appendix collects the most frequently used identities in multilinear algebra, matrix calculus, and tensor manipulation. They are written in a quick-lookup style - proofs and derivations appear in the main chapters.\n\n1. Kronecker Product & Vec Identities\n\nVectorization of matrix products:\n\\[\n\\mathrm{vec}(AXB) = (B^\\top \\otimes A)\\, \\mathrm{vec}(X).\n\\]\nVec of outer product:\n\\[\n\\mathrm{vec}(uv^\\top) = v \\otimes u.\n\\]\nMixed product property:\n\\[\n(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD),\n\\]\nif dimensions match.\n\n\n\n2. Trace Tricks\n\nCyclic property:\n\\[\n\\mathrm{tr}(AB) = \\mathrm{tr}(BA).\n\\]\nFrobenius inner product:\n\\[\n\\langle A,B \\rangle = \\mathrm{tr}(A^\\top B).\n\\]\nTrace with Kronecker:\n\\[\n\\mathrm{tr}(A \\otimes B) = \\mathrm{tr}(A)\\, \\mathrm{tr}(B).\n\\]\n\n\n\n3. Tensor Contractions\n\nInner product of tensors:\n\\[\n\\langle T, U \\rangle = \\sum_{i_1,\\dots,i_k} T_{i_1\\dots i_k} U_{i_1\\dots i_k}.\n\\]\nContraction with \\(\\delta_{ij}\\):\n\\[\nA_{ij}\\,\\delta_{ij} = \\mathrm{tr}(A).\n\\]\nContraction with Levi-Civita (\\(\\varepsilon_{ijk}\\)):\n\\[\n\\varepsilon_{ijk}\\, u_j v_k = (u \\times v)_i.\n\\]\n\n\n\n4. Determinant & Volumes\n\nDeterminant via Levi-Civita:\n\\[\n\\det(A) = \\sum_{i_1,\\dots,i_n} \\varepsilon_{i_1 \\dots i_n}\nA_{1,i_1} A_{2,i_2} \\cdots A_{n,i_n}.\n\\]\nVolume of parallelepiped (vectors \\(v_1,\\dots,v_n\\)):\n\\[\n\\text{Vol} = |\\det([v_1 \\ \\dots \\ v_n])|.\n\\]\n\n\n\n5. Differential & Gradient Identities\n\nGradient of quadratic form:\n\\[\n\\nabla_x (x^\\top A x) = (A + A^\\top)x.\n\\]\nMatrix calculus rule:\n\\[\n\\nabla_X \\,\\mathrm{tr}(A^\\top X) = A.\n\\]\nLog-det derivative:\n\\[\n\\nabla_X \\log \\det(X) = (X^{-1})^\\top.\n\\]\n\n\n\n6. Useful Einsum Patterns\n\nMatrix multiplication:\nC = np.einsum('ik,kj-&gt;ij', A, B)\nTensor contraction:\ny = np.einsum('ijk,k-&gt;ij', T, x)\nInner product:\ns = np.einsum('i,i-&gt;', u, v)\n\n\n\n7. Symmetrization / Antisymmetrization\n\nSymmetrization of 2-tensor:\n\\[\nS_{ij} = \\tfrac{1}{2}(T_{ij} + T_{ji}).\n\\]\nAntisymmetrization:\n\\[\nA_{ij} = \\tfrac{1}{2}(T_{ij} - T_{ji}).\n\\]\nGeneral \\(k\\):\n\\[\n\\text{Sym}(T) = \\frac{1}{k!} \\sum_{\\pi \\in S_k} T_{i_{\\pi(1)} \\dots i_{\\pi(k)}}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors",
    "href": "index.html#appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix E.1 Mini-Project: Implement CP Decomposition for Small 3-Way Tensors",
    "text": "Appendix E.1 Mini-Project: Implement CP Decomposition for Small 3-Way Tensors\nGoal: Learn how to compute a Canonical Polyadic (CP) decomposition of a small 3-way tensor by implementing an algorithm step-by-step. This project combines theory (tensor rank, factorization) with practice (numerical methods).\n\n1. Background\n\nRecall: a 3rd-order tensor \\(T \\in \\mathbb{R}^{I \\times J \\times K}\\) has CP form\n\\[\nT \\approx \\sum_{r=1}^R a^{(r)} \\otimes b^{(r)} \\otimes c^{(r)},\n\\]\nwhere \\(R\\) is the target rank and \\(a^{(r)} \\in \\mathbb{R}^I, b^{(r)} \\in \\mathbb{R}^J, c^{(r)} \\in \\mathbb{R}^K\\).\nCP decomposition generalizes SVD to higher-order tensors.\nUnlike SVD, exact decomposition is not guaranteed for arbitrary tensors, so we usually solve an optimization problem.\n\n\n\n2. Implementation Plan\nStep 1: Generate a Synthetic Tensor\n\nChoose dimensions, e.g., \\(I=J=K=4\\).\nPick random factor matrices \\(A \\in \\mathbb{R}^{I \\times R}, B \\in \\mathbb{R}^{J \\times R}, C \\in \\mathbb{R}^{K \\times R}\\).\nForm a tensor:\n\\[\nT_{ijk} = \\sum_{r=1}^R A_{ir} B_{jr} C_{kr}.\n\\]\n\nStep 2: Alternating Least Squares (ALS) Algorithm\n\nInitialize random factor matrices \\(\\hat{A}, \\hat{B}, \\hat{C}\\).\nRepeat until convergence:\n\nFix \\(\\hat{B}, \\hat{C}\\), update \\(\\hat{A}\\) by solving least squares.\nFix \\(\\hat{A}, \\hat{C}\\), update \\(\\hat{B}\\).\nFix \\(\\hat{A}, \\hat{B}\\), update \\(\\hat{C}\\).\n\n\nStep 3: Evaluate Error\n\nReconstruction error:\n\\[\n\\text{Error} = \\frac{\\|T - \\hat{T}\\|_F}{\\|T\\|_F}.\n\\]\nStop when error falls below threshold (e.g., \\(10^{-6}\\)) or max iterations reached.\n\n\n\n3. Coding Hints\n\nUse NumPy (or PyTorch/JAX) for efficient tensor operations.\nReshape and unfold tensors along modes for least squares updates.\nUse numpy.linalg.lstsq to solve linear systems.\nNormalize columns of factors periodically to avoid numerical instability.\n\n\n\n4. Extensions (Optional)\n\nCompare convergence for different ranks \\(R\\).\nAdd Gaussian noise to the tensor and see how well CP decomposition recovers the factors.\nTest uniqueness: permute/scaled versions of factors should still reconstruct the tensor.\nVisualize factor matrices as heatmaps.\n\n\n\n5. Deliverables\n\nWorking implementation of CP-ALS for 3-way tensors.\nPlots of error vs. iterations.\nShort report (1–2 pages) discussing:\n\nAccuracy of decomposition.\nEffect of rank choice.\nChallenges in convergence.\n\n\nBy finishing this project, you’ll understand CP decomposition at both the conceptual (tensor rank, multilinearity) and practical (numerical algorithms, implementation) levels.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-e.2-mini-project-tucker-decomposition-for-video-compression",
    "href": "index.html#appendix-e.2-mini-project-tucker-decomposition-for-video-compression",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix E.2 Mini-Project: Tucker Decomposition for Video Compression",
    "text": "Appendix E.2 Mini-Project: Tucker Decomposition for Video Compression\nGoal: Explore how Tucker decomposition can compress multi-way data efficiently, using a small grayscale video (or synthetic 3D tensor).\n\n1. Background\n\nA video clip with \\(F\\) frames, each of size \\(H \\times W\\), is naturally represented as a 3-way tensor:\n\\[\nX \\in \\mathbb{R}^{F \\times H \\times W}.\n\\]\nTucker decomposition:\n\\[\nX \\approx G \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)},\n\\]\nwhere\n\n\\(G\\) is a smaller core tensor,\n\\(U^{(1)}, U^{(2)}, U^{(3)}\\) are factor matrices (orthogonal bases along each mode).\n\nThis is a higher-order analogue of SVD (HOSVD).\n\n\n\n2. Implementation Plan\nStep 1: Data Preparation\n\nOption A: Load a small grayscale video (e.g., \\(30 \\times 64 \\times 64\\): 30 frames, 64×64 pixels).\nOption B: Generate synthetic video data (moving shapes, noise).\n\nStep 2: Compute Tucker Decomposition (HOSVD)\n\nUnfold \\(X\\) along each mode (frame, height, width).\nCompute SVD of each unfolding.\nSelect leading \\(r_1, r_2, r_3\\) singular vectors for factor matrices \\(U^{(1)}, U^{(2)}, U^{(3)}\\).\nForm core tensor:\n\\[\nG = X \\times_1 (U^{(1)})^\\top \\times_2 (U^{(2)})^\\top \\times_3 (U^{(3)})^\\top.\n\\]\n\nStep 3: Reconstruct Compressed Video\n\nApproximation:\n\\[\n\\hat{X} = G \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}.\n\\]\nCompare reconstruction with original.\n\nStep 4: Evaluate Compression\n\nCompression ratio:\n\\[\n\\text{CR} = \\frac{\\text{entries in original tensor}}{\\text{entries in core + factors}}.\n\\]\nReconstruction error:\n\\[\n\\text{Error} = \\frac{\\|X - \\hat{X}\\|_F}{\\|X\\|_F}.\n\\]\n\n\n\n3. Coding Hints\n\nUse NumPy or Tensorly (tensorly.decomposition.tucker) to avoid low-level SVD coding.\nVisualize reconstruction error by displaying frames before and after compression.\nTry varying rank choices \\((r_1,r_2,r_3)\\).\n\n\n\n4. Extensions (Optional)\n\nCompare Tucker compression with simple PCA on flattened frames.\nAdd noise to video and check whether Tucker decomposition captures underlying structure better than PCA.\nExplore color video: treat as a 4-way tensor \\(F \\times H \\times W \\times 3\\).\nImplement randomized SVD for scalability.\n\n\n\n5. Deliverables\n\nCode that performs Tucker decomposition on a small video dataset.\nPlots:\n\nError vs. compression ratio.\nExample frame (original vs. compressed reconstruction).\n\nShort writeup explaining:\n\nHow compression works.\nTrade-off between rank choice and accuracy.\n\n\nBy completing this project, you’ll see how tensor decompositions reduce storage and preserve structure in real data, and why multilinear methods are superior to naive flattening.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate",
    "href": "index.html#appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix E.3 Mini-Project: Strain Tensor for a Rotating Plate",
    "text": "Appendix E.3 Mini-Project: Strain Tensor for a Rotating Plate\nGoal: Understand how the strain tensor captures deformation by computing it for a simple rotating square plate. This bridges mechanics (stress/strain) with tensor calculus.\n\n1. Background\n\nIn continuum mechanics, the strain tensor measures local deformation of a material:\n\\[\n\\varepsilon_{ij} = \\tfrac{1}{2}\\left(\\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i}\\right),\n\\]\nwhere \\(u(x)\\) is the displacement field.\nPure rotation should produce no strain (only rigid-body motion).\nThis project demonstrates that principle.\n\n\n\n2. Setup: Rotating Plate\n\nConsider a 2D square plate centered at the origin.\nApply a small rotation by angle \\(\\theta\\).\nDisplacement of a point \\((x,y)\\):\n\\[\nu(x,y) = R(\\theta)\\begin{bmatrix} x \\\\ y \\end{bmatrix} - \\begin{bmatrix} x \\\\ y \\end{bmatrix},\n\\]\nwhere \\(R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\\).\n\n\n\n3. Compute Strain Tensor\n\nExpand \\(u(x,y)\\):\n\\[\nu_1(x,y) = (\\cos\\theta - 1)x - (\\sin\\theta)y,\n\\]\n\\[\nu_2(x,y) = (\\sin\\theta)x + (\\cos\\theta - 1)y.\n\\]\nCompute partial derivatives:\n\n\\(\\frac{\\partial u_1}{\\partial x} = \\cos\\theta - 1,\\)\n\\(\\frac{\\partial u_1}{\\partial y} = -\\sin\\theta,\\)\n\\(\\frac{\\partial u_2}{\\partial x} = \\sin\\theta,\\)\n\\(\\frac{\\partial u_2}{\\partial y} = \\cos\\theta - 1.\\)\n\nBuild strain tensor:\n\\[\n\\varepsilon = \\tfrac{1}{2}\\begin{bmatrix}\n2(\\cos\\theta - 1) & (-\\sin\\theta + \\sin\\theta) \\\\\n(\\sin\\theta - \\sin\\theta) & 2(\\cos\\theta - 1)\n\\end{bmatrix}.\n\\]\nSimplify:\n\\[\n\\varepsilon = (\\cos\\theta - 1) I,\n\\]\nwhich vanishes when \\(\\theta\\) is small (pure rotation ≈ no strain).\n\n\n\n4. Interpretation\n\nFor infinitesimal \\(\\theta\\), expand \\(\\cos\\theta \\approx 1 - \\tfrac{1}{2}\\theta^2\\).\nSo strain \\(\\varepsilon \\sim -\\tfrac{1}{2}\\theta^2 I\\), i.e., negligible for small angles.\nConfirms physical intuition: rigid rotations produce no first-order strain.\n\n\n\n5. Coding Hints\n\nImplement in Python/NumPy: define displacement field, compute gradients symbolically (SymPy) or numerically (finite differences).\nVisualize deformation arrows for a square grid of points.\nPlot strain tensor components as heatmaps.\n\n\n\n6. Extensions (Optional)\n\nApply non-uniform deformation (e.g., shear or stretching) and compute strain.\nCompare symmetric (strain) vs. antisymmetric (rotation) parts of displacement gradient.\nExtend to 3D cube rotation.\n\n\n\n7. Deliverables\n\nCode computing strain tensor for rotating plate.\nVisualization of deformation vs. strain (showing nearly zero strain for pure rotation).\nShort explanation connecting math to physical interpretation.\n\nThis project illustrates how the strain tensor isolates real deformation, distinguishing it from rigid-body motion.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere",
    "href": "index.html#appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix E.4 Mini-Project: Parallel Transport of a Vector on a Sphere",
    "text": "Appendix E.4 Mini-Project: Parallel Transport of a Vector on a Sphere\nGoal: Visualize and compute parallel transport of a vector along a closed path on a sphere, showing how curvature affects vector orientation.\n\n1. Background\n\nOn curved manifolds, moving a vector along a path while keeping it “as constant as possible” is called parallel transport.\nOn a flat plane: parallel transport around any loop returns the vector unchanged.\nOn a sphere: transporting a vector around a loop can change its orientation, revealing curvature.\n\n\n\n2. Setup: The Sphere \\(S^2\\)\n\nSphere of radius \\(R=1\\), embedded in \\(\\mathbb{R}^3\\).\nTangent space at a point \\(p \\in S^2\\): plane orthogonal to \\(p\\).\nPath: for simplicity, use a triangle on the sphere (e.g., along the equator and a meridian).\n\n\n\n3. Parallel Transport along Geodesics\n\nChoose Starting Point:\n\n\\(p_0 = (0,0,1)\\) (north pole).\nTangent vector: \\(v_0 = (1,0,0)\\).\n\nTransport Path:\n\nMove vector along a geodesic (great circle).\nKeep it tangent at each point.\nAlgorithmically: project derivative back into tangent space.\n\nClosed Loop Example:\n\nMove from north pole down to equator (longitude 0).\nTravel along equator by 90°.\nReturn to north pole.\n\nResult:\n\nVector rotates relative to original orientation.\nRotation angle equals the spherical excess of the triangle (area on sphere).\n\n\n\n\n4. Coding Hints\n\nUse Python + NumPy/Matplotlib.\nRepresent path as discrete points on sphere.\nAt each step:\n\nMove point along geodesic.\nUpdate vector by projecting back into tangent plane:\n\\[\nv \\gets v - (v \\cdot p)p, \\quad \\text{then normalize}.\n\\]\n\nVisualize trajectory of vector arrows along sphere using 3D plotting (matplotlib.pyplot.quiver).\n\n\n\n5. Extensions (Optional)\n\nExperiment with different spherical triangles.\nCompute holonomy angle = enclosed area × curvature (for unit sphere, curvature = 1).\nCompare parallel transport on sphere vs. flat plane (no change).\nExtend to numerical geodesics on other manifolds (torus, hyperbolic surface).\n\n\n\n6. Deliverables\n\nCode that simulates parallel transport on a sphere.\n3D visualization of vector transport around loop.\nMeasurement of net rotation angle (compare with theory).\nShort reflection: “What does this reveal about curvature?”\n\nThis project gives tangible insight into how curvature manifests in parallel transport, making an abstract tensorial concept geometrically vivid.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  },
  {
    "objectID": "index.html#appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data",
    "href": "index.html#appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data",
    "title": "The Little Book of Multilinear Algebra",
    "section": "Appendix E.5 Mini-Project: PCA vs. Tucker Decomposition on Real Data",
    "text": "Appendix E.5 Mini-Project: PCA vs. Tucker Decomposition on Real Data\nGoal: Compare Principal Component Analysis (PCA), which is matrix-based, with Tucker decomposition, which is tensor-based, on a real dataset. This project shows why multilinear approaches capture more structure than flattening data.\n\n1. Background\n\nPCA: finds low-rank approximations of matrices (e.g., flatten images or videos into 2D).\nTucker decomposition: generalizes PCA to tensors, preserving multi-way structure.\nKey question: Does Tucker give a better representation than PCA when applied to data with natural multi-dimensional structure (e.g., images, videos, EEG signals)?\n\n\n\n2. Dataset Options\n\nImages: MNIST digits (28×28 grayscale images).\nVideo: small grayscale clip (frames × height × width).\nMultichannel signals: EEG (time × channel × trial).\n\n\n\n3. Methodology\nStep 1: Prepare Data\n\nFor PCA: flatten each sample into a long vector.\nFor Tucker: keep data as tensor.\n\nStep 2: Apply PCA\n\nUse scikit-learn PCA.\nChoose top \\(k\\) components.\nReconstruct approximations of data.\n\nStep 3: Apply Tucker Decomposition\n\nUse tensorly.decomposition.tucker.\nChoose rank tuple \\((r_1, r_2, r_3)\\).\nReconstruct approximations.\n\nStep 4: Compare Results\n\nCompute reconstruction error:\n\\[\n\\text{Error} = \\frac{\\|X - \\hat{X}\\|_F}{\\|X\\|_F}.\n\\]\nCompare compression ratio (# of parameters stored).\nVisualize original vs reconstructed samples.\n\n\n\n4. Coding Hints\n\nLibraries: numpy, scikit-learn, tensorly.\nTo compare fairly: match number of parameters used by PCA and Tucker.\nFor MNIST: display digits reconstructed with 5, 10, 20 components.\nFor Tucker: vary ranks (e.g., (10,10) vs (5,10,5)).\n\n\n\n5. Extensions (Optional)\n\nAdd Gaussian noise to data and test denoising power of PCA vs Tucker.\nCompare runtime and scalability.\nTry CP decomposition instead of Tucker.\nExplore color images as 3-way tensors (height × width × channels).\n\n\n\n6. Deliverables\n\nCode implementing both PCA and Tucker on chosen dataset.\nPlots:\n\nError vs. compression ratio.\nSide-by-side reconstruction images (original, PCA, Tucker).\n\nShort writeup:\n\nWhich method captures structure better?\nDoes Tucker handle correlations across modes more effectively?\n\n\nThis project highlights the advantage of tensor methods over flattening approaches, making the case for multilinear models in data science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part I. Orientation & Motivation</span>"
    ]
  }
]