% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of Multilinear Algebra},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of Multilinear Algebra}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.1.0}
\author{Duc-Tam Nguyen}
\date{2025-09-05}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Part I. Orientation \&
Motivation}\label{part-i.-orientation-motivation}

\section{Chapter 1. What is
Multilinear?}\label{chapter-1.-what-is-multilinear}

\subsection{1.1 Linear vs.~Multilinear: From Lines to
Volumes}\label{linear-vs.-multilinear-from-lines-to-volumes}

When you first meet \emph{linear algebra}, the word ``linear'' carries a
specific flavor: we study maps that preserve straightness and scaling. A
linear map \(f: V \to W\) satisfies

\[
f(a v_1 + b v_2) = a f(v_1) + b f(v_2),
\]

for scalars \(a, b\) and vectors \(v_1, v_2 \in V\). The picture is of
functions that take lines to lines, planes to planes, preserving the
essential structure of addition and scaling.

\subsubsection{From Linear to
Multilinear}\label{from-linear-to-multilinear}

A multilinear map involves \emph{several vector inputs at once}. For
instance, a bilinear map takes two vectors:

\[
B : V \times W \to \mathbb{R}, \quad B(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w),
\]

and is linear in each argument separately. More generally, a
\(k\)-linear map eats \(k\) vectors-each input behaves linearly while
the others are held fixed.

\subsubsection{Geometry of the Shift}\label{geometry-of-the-shift}

\begin{itemize}
\tightlist
\item
  Linear (1-input): Think of scaling a line, stretching it along one
  axis.
\item
  Bilinear (2-input): Now imagine two directions at once. The
  determinant of a \(2 \times 2\) matrix is bilinear: it measures the
  signed *area- spanned by two vectors.
\item
  Trilinear (3-input): With three inputs, multilinearity measures a
  \emph{volume}. The scalar triple product \(u \cdot (v \times w)\) is
  trilinear, giving the volume of the parallelepiped formed by
  \(u, v, w\).
\item
  Higher multilinearity: Beyond three inputs, we can measure 4D
  hyper-volumes and higher-dimensional ``content.''
\end{itemize}

\subsubsection{Intuition: From 1D to Higher
Dimensions}\label{intuition-from-1d-to-higher-dimensions}

\begin{itemize}
\tightlist
\item
  1D (linear): One direction → length.
\item
  2D (bilinear): Two directions → area.
\item
  3D (trilinear): Three directions → volume.
\item
  kD (multilinear): \(k\) directions → hyper-volume.
\end{itemize}

Each new input adds a new dimension of measurement. In this sense,
multilinear algebra is the natural extension of linear algebra: instead
of studying transformations of single vectors, we study functions that
combine several vectors in structured ways.

\subsubsection{Everyday Examples}\label{everyday-examples}

\begin{itemize}
\tightlist
\item
  Dot product: Bilinear form producing a scalar from two vectors.
\item
  Matrix multiplication: Bilinear in its row and column inputs.
\item
  Determinant: Multilinear in its columns (or rows), encoding volume.
\item
  Cross product: Bilinear but antisymmetric, giving a vector orthogonal
  to two inputs.
\item
  Neural networks: Convolutions and tensor contractions are deeply
  multilinear operations, reshaped for computation.
\end{itemize}

Linear algebra captures what happens when you act on one direction at a
time. Multilinear algebra generalizes this, letting us measure,
transform, and compute with several directions simultaneously. It is the
leap from lines to volumes, from single-vector transformations to the
rich geometry of many interacting vectors.

\subsubsection{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Linearity Check: Let \(f:\mathbb{R}^2 \to \mathbb{R}^2\) be defined by
  \(f(x,y) = (2x, 3y)\). Show that \(f\) is linear.
\item
  Bilinear Dot Product: Verify that the dot product
  \(\langle u, v \rangle = u_1 v_1 + u_2 v_2 + u_3 v_3\) is bilinear by
  checking linearity in each argument separately.
\item
  Area via Determinant: For vectors \(u = (1,0)\) and \(v = (1,2)\) in
  \(\mathbb{R}^2\), compute the determinant of the \(2 \times 2\) matrix
  with columns \(u\) and \(v\). Interpret the result geometrically.
\item
  Volume via Scalar Triple Product: Compute the scalar triple product
  \(u \cdot (v \times w)\) for \(u = (1,0,0)\), \(v = (0,1,0)\), and
  \(w = (0,0,1)\). Explain why the result makes sense in terms of
  volume.
\item
  Generalization Thought Experiment: Imagine you have four independent
  vectors in \(\mathbb{R}^4\). What kind of geometric quantity does a
  multilinear map on these four inputs measure? (Hint: think of the
  analogy with length, area, and volume.)
\end{enumerate}

\subsection{1.2 Three Faces of a Tensor: Array, Map, and Element of a
Product
Space}\label{three-faces-of-a-tensor-array-map-and-element-of-a-product-space}

A tensor can feel slippery at first, because it wears different
``faces'' depending on how you meet it. Each face is valid and useful.
Together they form the three standard viewpoints:

\subsubsection{1. Array View: Numbers in a
Box}\label{array-view-numbers-in-a-box}

At the simplest level, a tensor looks like a multidimensional array of
numbers.

\begin{itemize}
\tightlist
\item
  A vector is a 1-dimensional array: \([v_i]\).
\item
  A matrix is a 2-dimensional array: \([a_{ij}]\).
\item
  A general tensor of order \(k\) is like a \(k\)-dimensional grid of
  numbers: \([t_{i_1 i_2 \dots i_k}]\).
\end{itemize}

This viewpoint is intuitive for computation, storage, and indexing. For
example, in machine learning an image is a 3rd-order tensor: height ×
width × color channels.

\subsubsection{2. Map View: Multilinear
Functions}\label{map-view-multilinear-functions}

Another way to see tensors is as multilinear maps.

\begin{itemize}
\item
  A bilinear form takes two vectors and outputs a number, e.g., the dot
  product.
\item
  A trilinear form takes three vectors and outputs a number, e.g., the
  scalar triple product.
\item
  In general, a tensor \(T\) of type \((0,k)\) is a map

  \[
  T: V \times V \times \cdots \times V \to \mathbb{R},
  \]

  linear in each slot separately.
\end{itemize}

This viewpoint highlights \emph{behavior}: how the tensor interacts with
vectors. It is central in geometry and physics, where tensors encode
measurable relationships.

\subsubsection{\texorpdfstring{3. Product Space View: Elements of
\(V \otimes W \otimes \cdots\)}{3. Product Space View: Elements of V \textbackslash otimes W \textbackslash otimes \textbackslash cdots}}\label{product-space-view-elements-of-v-otimes-w-otimes-cdots}

The third viewpoint places tensors as elements of tensor product spaces.

\begin{itemize}
\tightlist
\item
  Given vector spaces \(V\) and \(W\), their tensor product
  \(V \otimes W\) is a new space built to capture bilinear maps.
\item
  A simple (decomposable) tensor looks like \(v \otimes w\).
\item
  General tensors are linear combinations of these simple pieces.
\end{itemize}

This is the most abstract but also the most powerful perspective. It
makes precise statements about dimension, bases, and transformation
laws. It also unifies the array and map viewpoints: the array entries
are just the coordinates of a tensor element with respect to a basis,
and the map behavior is encoded in how the element acts under
evaluation.

\subsubsection{Reconciling the Views}\label{reconciling-the-views}

\begin{itemize}
\tightlist
\item
  Array: concrete for computation.
\item
  Map: functional and geometric meaning.
\item
  Product space: rigorous foundation.
\end{itemize}

A beginner should be comfortable switching between them: ``the same
object, three different languages.''

\subsubsection{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Array Identification: Write down a 3rd-order tensor \(T\) with entries
  \(t_{ijk}\) where \(i,j,k \in \{1,2\}\). How many numbers are needed
  to specify \(T\)?
\item
  Dot Product as a Tensor: Show that the dot product on \(\mathbb{R}^3\)
  can be viewed both as a bilinear map and as an array (matrix).
\item
  Simple Tensor Construction: Given \(u=(1,2)\in \mathbb{R}^2\) and
  \(v=(3,4)\in \mathbb{R}^2\), form the tensor \(u \otimes v\). Write
  its coordinates as a \(2 \times 2\) array.
\item
  Switching Perspectives: Consider the determinant of a \(2\times2\)
  matrix: \(\det([a_{ij}]) = a_{11}a_{22} - a_{12}a_{21}\). Explain how
  this determinant can be seen as a bilinear map of the two column
  vectors of the matrix.
\item
  Thought Experiment: Suppose you store an RGB image of size
  \(100 \times 200\).

  \begin{itemize}
  \tightlist
  \item
    In the array view, what is the order (number of indices) of the
    tensor representing the image?
  \item
    In the product space view, which vector spaces might this tensor
    live in?
  \end{itemize}
\end{enumerate}

\subsection{1.3 Why It Matters: Graphics, Physics, ML, Data
Compression}\label{why-it-matters-graphics-physics-ml-data-compression}

Tensors may sound abstract, but they appear everywhere in modern science
and technology. Understanding *why- they matter helps motivate the study
of multilinear algebra.

\subsubsection{In Computer Graphics}\label{in-computer-graphics}

\begin{itemize}
\tightlist
\item
  Transformations: 3D graphics use matrices (2nd-order tensors) to
  rotate, scale, and project objects.
\item
  Lighting Models: Many shading equations combine vectors of light,
  normal directions, and material properties in bilinear or trilinear
  ways.
\item
  Animation: Deformations of 3D models often rely on multilinear
  blending of control parameters.
\end{itemize}

Tensors let us express geometry and transformations in compact formulas
that computers can process efficiently.

\subsubsection{In Physics and
Engineering}\label{in-physics-and-engineering}

\begin{itemize}
\tightlist
\item
  Stress and Strain: The stress tensor (2nd-order) relates internal
  forces to orientations in a material. The elasticity tensor
  (4th-order) relates stress and strain in solids.
\item
  Electromagnetism: The electromagnetic field tensor encodes electric
  and magnetic fields in a relativistic framework.
\item
  Inertia Tensor: Describes how an object resists rotational
  acceleration depending on its mass distribution.
\end{itemize}

Tensors naturally capture ``laws that hold in every direction,'' which
is why they dominate in mechanics and field theories.

\subsubsection{In Machine Learning}\label{in-machine-learning}

\begin{itemize}
\tightlist
\item
  Neural Networks: Input data (images, videos, audio) are tensors of
  order 3, 4, or higher.
\item
  Convolutions: The convolution kernel is a small tensor sliding across
  a larger tensor, producing a new one.
\item
  Attention Mechanisms: Core operations in transformers are tensor
  contractions that combine multiple input arrays.
\end{itemize}

Understanding tensor decompositions helps compress models, speed up
computation, and reveal hidden structures in data.

\subsubsection{In Data Compression and Signal
Processing}\label{in-data-compression-and-signal-processing}

\begin{itemize}
\tightlist
\item
  PCA and SVD: Classic matrix decompositions are 2D tensor methods.
\item
  Tensor Decompositions (CP, Tucker, TT): These extend compression ideas
  to multidimensional data, useful for video, hyperspectral imaging, and
  big-data analysis.
\item
  Multiway Data Analysis: Tensors allow us to uncover patterns across
  several ``modes'' simultaneously-like user × time × product in
  recommendation systems.
\end{itemize}

\subsubsection{The Big Picture}\label{the-big-picture}

Linear algebra lets us describe single-direction transformations
(lines). Multilinear algebra extends this to multiple interacting
directions (areas, volumes, hyper-volumes). These structures appear
whenever we handle multi-dimensional data or physical laws.

Learning to *think tensorially- is the key to navigating modern applied
mathematics, science, and AI.

\subsubsection{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Graphics Example: A 3D rotation matrix is a 2nd-order tensor. Explain
  why it is linear in its input vector but not multilinear.
\item
  Physics Example: The stress tensor \(\sigma\) maps a direction vector
  \(n\) to a force vector \(\sigma n\). Why is this operation linear in
  \(n\)?
\item
  Machine Learning Example: An RGB image of size \(32 \times 32\) has 3
  color channels.

  \begin{itemize}
  \tightlist
  \item
    What is the order of the tensor representing this image?
  \item
    How many entries does it have in total?
  \end{itemize}
\item
  Data Compression Example: PCA reduces a data matrix (2nd-order tensor)
  to a low-rank approximation. Suggest what a ``low-rank'' tensor
  decomposition might achieve for video data (3rd-order tensor: frame ×
  width × height).
\item
  Thought Experiment: Suppose you could only use vectors and matrices,
  not higher-order tensors. Which of the following applications would be
  impossible or very awkward:

  \begin{itemize}
  \tightlist
  \item
    Representing the interaction of three forces at once.
  \item
    Compressing a color video.
  \item
    Encoding the stress-strain relationship in 3D materials. \#\#\# 1.4
    A First Walk-Through: Color Images and 3-Way Arrays
  \end{itemize}
\end{enumerate}

Let's make tensors concrete with an everyday example: digital images.

\subsubsection{From Grayscale to Color}\label{from-grayscale-to-color}

\begin{itemize}
\item
  A grayscale image of size \(100 \times 200\) can be seen as a matrix
  (2nd-order tensor). Each entry stores the brightness of a pixel.
\item
  A color image has three channels: red, green, and blue (RGB). Now
  every pixel carries three values. This naturally forms a 3rd-order
  tensor:

  \[
  I \in \mathbb{R}^{100 \times 200 \times 3}.
  \]

  The three indices correspond to row, column, color channel.
\end{itemize}

\subsubsection{The Three Index Roles}\label{the-three-index-roles}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Row (height): vertical position in the image.
\item
  Column (width): horizontal position in the image.
\item
  Channel: one of the RGB color intensities.
\end{enumerate}

Together, \((i,j,k)\) points to a single number: the intensity of color
\(k\) at pixel \((i,j)\).

\subsubsection{Operations as Tensor
Manipulations}\label{operations-as-tensor-manipulations}

\begin{itemize}
\tightlist
\item
  Flattening: We can reshape the 3D tensor into a 2D matrix, useful for
  feeding into algorithms that expect vectors or matrices.
\item
  Contraction (summing over an index): If we sum over the color channel,
  we turn an RGB image into a grayscale image.
\item
  Outer products: A colored checkerboard pattern can be constructed by
  taking tensor products of row and column vectors, then adding a color
  channel vector.
\end{itemize}

\subsubsection{Why This Example Matters}\label{why-this-example-matters}

\begin{itemize}
\tightlist
\item
  It shows how natural data can have more than two indices.
\item
  It illustrates why matrices (2D tensors) are not enough for modern
  problems.
\item
  It connects tensor operations with practical tasks: filtering,
  compression, feature extraction.
\end{itemize}

In fact, video data adds one more index: time. A video is a 4th-order
tensor: frame × height × width × channel.

\subsubsection{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Counting Entries: How many numbers are required to store a color image
  of size \(64 \times 64\)?
\item
  Slicing: For a color image tensor
  \(I \in \mathbb{R}^{100 \times 200 \times 3}\), what is the shape of:

  \begin{itemize}
  \tightlist
  \item
    a single row across all columns and channels?
  \item
    a single color channel across all pixels?
  \end{itemize}
\item
  Flattening Practice: A \(32 \times 32 \times 3\) image is flattened
  into a vector. What is the length of this vector?
\item
  Grayscale Conversion: Define a grayscale image \(G(i,j)\) from a color
  image tensor \(I(i,j,k)\) by averaging across channels:

  \[
  G(i,j) = \frac{1}{3} \sum_{k=1}^3 I(i,j,k).
  \]

  Why is this operation an example of contraction?
\item
  Video as Tensor: Suppose you have a 10-second video at 30 frames per
  second, each frame \(128 \times 128\) with 3 color channels.

  \begin{itemize}
  \tightlist
  \item
    What is the order of the video tensor?
  \item
    How many entries does it contain in total?
  \end{itemize}
\end{enumerate}

\section{Chapter 2. Minimal
Prerequisites}\label{chapter-2.-minimal-prerequisites}

\subsection{2.1 Vector Spaces, Bases,
Dimension}\label{vector-spaces-bases-dimension}

Before diving deeper into multilinear algebra, we need a short refresher
on the basic building blocks of linear algebra: vector spaces.

\subsubsection{What is a Vector Space?}\label{what-is-a-vector-space}

A vector space is a collection of objects (called \emph{vectors}) that
can be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Added: \(u + v\) is again a vector.
\item
  Scaled: \(a v\) (where \(a\) is a scalar) is again a vector.
\end{enumerate}

The rules of addition and scaling follow natural laws: associativity,
commutativity, distributivity, and the existence of a zero vector.

Examples:

\begin{itemize}
\tightlist
\item
  \(\mathbb{R}^n\): all \(n\)-tuples of real numbers.
\item
  Polynomials of degree ≤ \(d\).
\item
  Continuous functions on an interval.
\end{itemize}

\subsubsection{Bases and Coordinates}\label{bases-and-coordinates}

A basis of a vector space is a set of vectors that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are linearly independent (no one is a linear combination of the
  others).
\item
  Span the entire space (every vector can be expressed as a linear
  combination of them).
\end{enumerate}

For \(\mathbb{R}^3\), the standard basis is:

\[
e_1 = (1,0,0), \quad e_2 = (0,1,0), \quad e_3 = (0,0,1).
\]

Every vector \(v \in \mathbb{R}^3\) can be uniquely written as:

\[
v = x e_1 + y e_2 + z e_3,
\]

with coordinates \((x,y,z)\).

\subsubsection{Dimension}\label{dimension}

The dimension of a vector space is the number of vectors in any basis.

\begin{itemize}
\tightlist
\item
  \(\mathbb{R}^n\) has dimension \(n\).
\item
  Polynomials of degree ≤ \(d\) form a vector space of dimension
  \(d+1\).
\item
  A trivial space \(\{0\}\) has dimension 0.
\end{itemize}

Dimension gives the ``number of independent directions'' in the space.

\subsubsection{Why This Matters for Multilinear
Algebra}\label{why-this-matters-for-multilinear-algebra}

\begin{itemize}
\tightlist
\item
  Tensors live in spaces built from vector spaces (tensor products).
\item
  Understanding bases and dimensions is crucial for counting entries of
  tensors.
\item
  Coordinates provide the link between abstract definitions and concrete
  arrays.
\end{itemize}

\subsubsection{Exercises}\label{exercises-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Checking Vector Spaces: Decide whether each of the following is a
  vector space over \(\mathbb{R}\):

  \begin{itemize}
  \tightlist
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \tightlist
    \item
      All \(2 \times 2\) real matrices.
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      All positive real numbers.
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      All polynomials with real coefficients.
    \end{enumerate}
  \end{itemize}
\item
  Basis in \(\mathbb{R}^2\): Show that \((1,1)\) and \((1,-1)\) form a
  basis for \(\mathbb{R}^2\). Express the vector \((3,2)\) in this
  basis.
\item
  Counting Dimension: What is the dimension of the space of all real
  polynomials of degree ≤ 4? Suggest a natural basis.
\item
  Uniqueness of Representation: In \(\mathbb{R}^3\), write \((2,3,5)\)
  as a combination of \(e_1, e_2, e_3\). Why is this representation
  unique?
\item
  Application to Tensors: If \(V = \mathbb{R}^2\) and
  \(W = \mathbb{R}^3\), what is the dimension of the product space
  \(V \otimes W\)?
\end{enumerate}

\subsection{2.2 Linear Maps, Matrices, Change of
Basis}\label{linear-maps-matrices-change-of-basis}

Having reviewed vector spaces, we now turn to linear maps-the main
actors in linear algebra.

\subsubsection{Linear Maps}\label{linear-maps}

A linear map \(T: V \to W\) between vector spaces satisfies:

\[
T(av + bw) = aT(v) + bT(w),
\]

for all scalars \(a,b\) and vectors \(v,w \in V\).

Examples:

\begin{itemize}
\tightlist
\item
  Scaling: \(T(x) = 3x\).
\item
  Rotation: \(T:\mathbb{R}^2 \to \mathbb{R}^2\) rotates vectors by 90°.
\item
  Derivative: \(D: P_3 \to P_2\) (maps a polynomial of degree ≤ 3 to its
  derivative).
\end{itemize}

Linear maps preserve the structure of vector spaces.

\subsubsection{Matrices as
Representations}\label{matrices-as-representations}

Given bases of \(V\) and \(W\), a linear map \(T: V \to W\) can be
represented by a matrix.

\begin{itemize}
\tightlist
\item
  Columns of the matrix are just the images of the basis vectors of
  \(V\).
\item
  If \(T(e_i) = \sum_j a_{ji} f_j\), then the matrix entries are
  \(a_{ji}\).
\end{itemize}

Thus, matrices are coordinate-based representations of abstract linear
maps.

\subsubsection{Composition and Matrix
Multiplication}\label{composition-and-matrix-multiplication}

\begin{itemize}
\tightlist
\item
  Composing linear maps corresponds to multiplying their matrices.
\item
  The identity map corresponds to the identity matrix.
\item
  Inverse maps correspond to inverse matrices (when they exist).
\end{itemize}

This makes linear maps concrete and computable.

\subsubsection{Change of Basis}\label{change-of-basis}

Suppose we change basis in a vector space \(V\):

\begin{itemize}
\tightlist
\item
  Old basis: \(\{e_1, \dots, e_n\}\).
\item
  New basis: \(\{e'_1, \dots, e'_n\}\).
\end{itemize}

The change-of-basis matrix \(P\) expresses each new basis vector as a
combination of the old ones.

For a linear operator \(T: V \to V\):

\[
[T]_{new} = P^{-1} [T]_{old} P.
\]

This formula is fundamental for tensors, since tensors must transform
consistently under basis changes.

\subsubsection{Why This Matters for Multilinear
Algebra}\label{why-this-matters-for-multilinear-algebra-1}

\begin{itemize}
\tightlist
\item
  Linear maps are 2nd-order tensors.
\item
  Understanding how matrices change under new coordinates sets the stage
  for how higher-order tensors transform.
\item
  The concept of basis change ensures that tensors encode *intrinsic-
  information, not just numbers in an array.
\end{itemize}

\subsection{Exercises}\label{exercises-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Linear or Not? Decide whether each map is linear:

  \begin{itemize}
  \tightlist
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \tightlist
    \item
      \(T(x,y) = (2x,3y)\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      \(S(x,y) = (x^2,y)\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      \(R(x,y) = (y,x)\).
    \end{enumerate}
  \end{itemize}
\item
  Matrix Representation: Let \(T:\mathbb{R}^2 \to \mathbb{R}^2\) be
  defined by \(T(x,y) = (x+2y,3x+y)\). Find the matrix of \(T\) with
  respect to the standard basis.
\item
  Composition Practice: If
  \(A = \begin{bmatrix}1 & 2\\0 & 1\end{bmatrix}\) and
  \(B = \begin{bmatrix}0 & 1\\1 & 0\end{bmatrix}\), compute \(AB\).
  Interpret the action of \(AB\) as a linear map.
\item
  Change of Basis: In \(\mathbb{R}^2\), let the old basis be
  \(e_1 = (1,0), e_2=(0,1)\). The new basis is
  \(e'_1=(1,1), e'_2=(1,-1)\).

  \begin{itemize}
  \tightlist
  \item
    Find the change-of-basis matrix \(P\).
  \item
    Verify that \(P^{-1}\) transforms coordinates back to the old basis.
  \end{itemize}
\item
  Tensor Connection: Explain why a linear map \(T: V \to W\) can be
  viewed as an element of \(V^- \otimes W\). (Hint: it eats a vector and
  produces another vector, which can be encoded by pairing with
  covectors.)
\end{enumerate}

\subsection{2.3 Inner Products, Dual Spaces,
Adjoint}\label{inner-products-dual-spaces-adjoint}

Linear maps and vector spaces give the structure. To measure
\emph{angles, lengths, and projections}, we need inner products. To
generalize ``coordinates'' beyond a chosen basis, we need dual spaces.
These two ideas connect directly in multilinear algebra.

\subsubsection{Inner Products}\label{inner-products}

An inner product on a real vector space \(V\) is a function

\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]

satisfying:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linearity in each slot:
  \(\langle av+ bw, u\rangle = a\langle v,u\rangle + b\langle w,u\rangle\).
\item
  Symmetry: \(\langle v, w \rangle = \langle w, v \rangle\).
\item
  Positive definiteness: \(\langle v,v\rangle \geq 0\), with equality
  only when \(v=0\).
\end{enumerate}

This structure gives:

\begin{itemize}
\tightlist
\item
  Length: \(\|v\| = \sqrt{\langle v,v\rangle}\).
\item
  Angle: \(\cos\theta = \frac{\langle v,w\rangle}{\|v\|\|w\|}\).
\item
  Orthogonality: \(\langle v,w\rangle=0\).
\end{itemize}

Example: the dot product in \(\mathbb{R}^n\).

\subsubsection{Dual Spaces}\label{dual-spaces}

The dual space \(V^*\) is the set of all linear functionals
\(f: V \to \mathbb{R}\).

\begin{itemize}
\item
  Elements of \(V^*\) are called covectors.
\item
  If \(V=\mathbb{R}^n\), then \(V^- \cong \mathbb{R}^n\), but
  conceptually they are different:

  \begin{itemize}
  \tightlist
  \item
    Vectors: ``arrows'' in space.
  \item
    Covectors: ``measuring devices'' that output numbers when fed a
    vector.
  \end{itemize}
\end{itemize}

The dual basis:

\begin{itemize}
\item
  If \(\{e_1,\dots,e_n\}\) is a basis of \(V\), then there is a unique
  dual basis \(\{e^1,\dots,e^n\}\) in \(V^*\) with

  \[
  e^i(e_j) = \delta^i_j.
  \]
\end{itemize}

This duality underpins how tensor indices ``live up or down''
(contravariant vs.~covariant).

\subsubsection{Adjoint of a Linear Map}\label{adjoint-of-a-linear-map}

Given a linear map \(T: V \to V\) on an inner product space, the adjoint
\(T^*\) is defined by:

\[
\langle Tv, w \rangle = \langle v, T^*w \rangle \quad \forall v,w \in V.
\]

\begin{itemize}
\tightlist
\item
  If \(T\) is represented by a matrix \(A\) in an orthonormal basis,
  then \(T^*\) corresponds to the transpose \(A^\top\).
\item
  Adjoint maps generalize the idea of ``transpose'' to arbitrary inner
  product spaces.
\end{itemize}

\subsubsection{Why This Matters for Multilinear
Algebra}\label{why-this-matters-for-multilinear-algebra-2}

\begin{itemize}
\tightlist
\item
  Inner products allow us to raise or lower indices (switch between
  vectors and covectors).
\item
  Dual spaces are essential for defining general tensors (mixing vectors
  and covectors).
\item
  Adjoint operators appear everywhere in applications: projections,
  least squares, and symmetry in physical laws.
\end{itemize}

\subsubsection{Exercises}\label{exercises-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inner Product Verification: Show that
  \(\langle (x_1,y_1),(x_2,y_2)\rangle = 2x_1x_2 + y_1y_2\) defines an
  inner product on \(\mathbb{R}^2\).
\item
  Length and Angle: For \(u=(1,2,2)\) and \(v=(2,0,1)\) in
  \(\mathbb{R}^3\), compute:

  \begin{itemize}
  \tightlist
  \item
    \(\|u\|\), \(\|v\|\).
  \item
    The cosine of the angle between them.
  \end{itemize}
\item
  Dual Basis: Let \(V=\mathbb{R}^2\) with basis
  \(e_1=(1,0), e_2=(0,1)\).

  \begin{itemize}
  \tightlist
  \item
    Write the dual basis \(e^1, e^2\).
  \item
    Compute \(e^1(3,4)\) and \(e^2(3,4)\).
  \end{itemize}
\item
  Adjoint Map: Let \(T:\mathbb{R}^2 \to \mathbb{R}^2\) with matrix

  \[
  A = \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix}.
  \]

  Find the matrix of \(T^*\) under the standard dot product.
\item
  Tensor Connection: Explain why an element of \(V^- \otimes V\) can be
  interpreted as a matrix, and why adjointness naturally appears when
  working with inner products.
\end{enumerate}

\subsection{2.4 Bilinear Forms and Quadratic
Forms}\label{bilinear-forms-and-quadratic-forms}

Now that we have inner products and dual spaces, we can introduce two
important types of multilinear maps that already appear in basic linear
algebra: bilinear forms and quadratic forms.

\subsubsection{Bilinear Forms}\label{bilinear-forms}

A bilinear form on a vector space \(V\) is a function

\[
B: V \times V \to \mathbb{R}
\]

that is linear in each argument separately:

\begin{itemize}
\tightlist
\item
  \(B(av_1 + bv_2, w) = aB(v_1, w) + bB(v_2, w)\),
\item
  \(B(v, aw_1 + bw_2) = aB(v, w_1) + bB(v, w_2)\).
\end{itemize}

Examples:

\begin{itemize}
\item
  Dot product: \(\langle v,w\rangle\) is symmetric and bilinear.
\item
  Matrix form: Any matrix \(A\) defines a bilinear form by

  \[
  B(v,w) = v^\top A w.
  \]
\end{itemize}

Properties:

\begin{itemize}
\tightlist
\item
  Symmetric: if \(B(v,w) = B(w,v)\).
\item
  Skew-symmetric: if \(B(v,w) = -B(w,v)\).
\end{itemize}

\subsubsection{Quadratic Forms}\label{quadratic-forms}

A quadratic form is a special case obtained by feeding the *same- vector
into both slots of a bilinear form:

\[
Q(v) = B(v,v).
\]

In coordinates, with a matrix \(A\):

\[
Q(v) = v^\top A v.
\]

Examples:

\begin{itemize}
\tightlist
\item
  In \(\mathbb{R}^2\), \(Q(x,y) = x^2 + y^2\) corresponds to the
  identity matrix.
\item
  In optimization, quadratic forms represent energy functions, error
  functions, or cost functions.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning}

\begin{itemize}
\tightlist
\item
  Quadratic forms define conic sections (ellipses, hyperbolas,
  parabolas) in 2D and quadric surfaces in higher dimensions.
\item
  They also measure ``curvature'' locally in multivariable functions
  (via the Hessian matrix).
\end{itemize}

\subsubsection{Why This Matters for Multilinear
Algebra}\label{why-this-matters-for-multilinear-algebra-3}

\begin{itemize}
\tightlist
\item
  Bilinear forms are 2nd-order tensors: one covector for each input.
\item
  Quadratic forms connect multilinearity with geometry (shapes, volumes,
  energy).
\item
  The distinction between symmetric and skew-symmetric bilinear forms
  leads to exterior algebra and inner product structures later.
\end{itemize}

\subsubsection{Exercises}\label{exercises-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Bilinear Form: Let

  \[
  A = \begin{bmatrix}2 & 1 \\ 1 & 3\end{bmatrix}.
  \]

  Define \(B(v,w) = v^\top A w\).

  \begin{itemize}
  \tightlist
  \item
    Compute \(B((1,0),(0,1))\).
  \item
    Compute \(B((1,2),(3,4))\).
  \end{itemize}
\item
  Symmetry Check: For the above \(B\), show that \(B(v,w) = B(w,v)\).
\item
  Quadratic Form: Compute \(Q(x,y) = [x \; y] A [x \; y]^\top\) for the
  same matrix \(A\). Write the explicit formula.
\item
  Geometric Interpretation: Consider \(Q(x,y) = 4x^2 + y^2\). Sketch (or
  describe) the curve \(Q(x,y)=1\). What kind of conic section is it?
\item
  Tensor Connection: Explain why a bilinear form
  \(B: V \times V \to \mathbb{R}\) can be seen as an element of
  \(V^- \otimes V^*\). What does this mean in terms of indices
  (covariant slots)?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part II. Tensors}\label{part-ii.-tensors}

\section{Chapter 3. Tensors as Indexed
Arrays}\label{chapter-3.-tensors-as-indexed-arrays}

\subsection{3.1 Order (Arity), Shape, and
Indices}\label{order-arity-shape-and-indices}

We now begin exploring tensors directly, starting with the array
viewpoint. This perspective treats tensors as multi-dimensional
generalizations of matrices.

\subsubsection{Order (Arity) of a Tensor}\label{order-arity-of-a-tensor}

The order (or arity) of a tensor is the number of indices needed to
locate one of its entries.

\begin{itemize}
\tightlist
\item
  0th-order: A scalar, e.g.~\(5\).
\item
  1st-order: A vector \(v_i\), with one index.
\item
  2nd-order: A matrix \(A_{ij}\), with two indices.
\item
  3rd-order: A block of numbers \(T_{ijk}\).
\item
  kth-order: An array with \(k\) indices.
\end{itemize}

Thus, the order tells us how many ``directions'' or ``modes'' the tensor
has.

\subsubsection{Shape (Dimensions of Each
Mode)}\label{shape-dimensions-of-each-mode}

Each index ranges over some set of values, defining the shape of the
tensor.

\begin{itemize}
\tightlist
\item
  A vector in \(\mathbb{R}^n\) has shape \((n)\).
\item
  A matrix of size \(m \times n\) has shape \((m,n)\).
\item
  A color image of size \(100 \times 200\) with 3 channels has shape
  \((100,200,3)\).
\end{itemize}

The shape is just the list of sizes of each mode.

\subsubsection{Indices and Notation}\label{indices-and-notation}

Indices label positions in the tensor:

\[
T_{i_1 i_2 \dots i_k}.
\]

Example: A 3rd-order tensor \(T_{ijk}\) with shape \((2,3,4)\) has:

\begin{itemize}
\tightlist
\item
  \(i \in \{1,2\}\),
\item
  \(j \in \{1,2,3\}\),
\item
  \(k \in \{1,2,3,4\}\).
\end{itemize}

So it contains \(2 \times 3 \times 4 = 24\) entries.

\subsubsection{Visual Intuition}\label{visual-intuition}

\begin{itemize}
\tightlist
\item
  Scalars are points.
\item
  Vectors are arrows (1D arrays).
\item
  Matrices are grids (2D arrays).
\item
  Higher-order tensors are cubes, hypercubes, or higher-dimensional
  arrays, which we can't fully draw but can still manipulate
  symbolically.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters}

\begin{itemize}
\tightlist
\item
  The array view is the most concrete: you can store tensors in memory,
  index them, and manipulate them in code.
\item
  It provides the language of shapes that data science, ML, and physics
  use constantly.
\item
  Later, we will see that these indices correspond to *slots- in
  multilinear maps and tensor product spaces.
\end{itemize}

\subsubsection{Exercises}\label{exercises-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Counting Entries: How many entries does a tensor of shape \((3,4,5)\)
  have?
\item
  Order Identification: Identify the order and shape of each object:

  \begin{itemize}
  \tightlist
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \tightlist
    \item
      A grayscale image \(64 \times 64\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      A video: 30 frames, each \(128 \times 128\) RGB.
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      A dataset with 1000 samples, each a \(20 \times 20\) grayscale
      image.
    \end{enumerate}
  \end{itemize}
\item
  Index Practice: For a tensor \(T_{ijk}\) with shape \((2,2,2)\), list
  explicitly all the index triples \((i,j,k)\).
\item
  Shape Transformation: Flatten a tensor with shape \((5,4,3)\) into a
  matrix. What two possible shapes could the matrix have (depending on
  how you group the indices)?
\item
  Thought Experiment: Why is a scalar sometimes called a ``0th-order
  tensor''? How does this viewpoint help unify the hierarchy of scalars,
  vectors, matrices, and higher-order tensors?
\end{enumerate}

\subsection{3.2 Covariant vs.~Contravariant
Indices}\label{covariant-vs.-contravariant-indices}

So far, we've treated indices as simple ``positions in an array.'' But
in multilinear algebra, indices carry roles. Some belong to vectors
(contravariant), others to covectors (covariant). Understanding this
distinction is crucial for how tensors behave under change of basis.

\subsubsection{Vectors vs.~Covectors}\label{vectors-vs.-covectors}

\begin{itemize}
\tightlist
\item
  Vectors are elements of a space \(V\). They transform with the basis.
\item
  Covectors (linear functionals) are elements of the dual space \(V^*\).
  They transform with the *inverse transpose- of the basis change.
\end{itemize}

This leads to two kinds of indices:

\begin{itemize}
\tightlist
\item
  Contravariant indices (upper): \(v^i\), coordinates of a vector.
\item
  Covariant indices (lower): \(\omega_j\), coordinates of a covector.
\end{itemize}

\subsubsection{Tensors Mixing Both}\label{tensors-mixing-both}

A tensor may have both types of indices:

\[
T^{i_1 i_2 \dots i_p}_{j_1 j_2 \dots j_q},
\]

which means it accepts \(q\) vectors and \(p\) covectors as inputs (or
outputs, depending on interpretation).

Examples:

\begin{itemize}
\tightlist
\item
  A vector \(v^i\) → contravariant (upper index).
\item
  A covector \(\omega_j\) → covariant (lower index).
\item
  A bilinear form \(B_{ij}\) → two covariant indices.
\item
  A linear map \(A^i{}_j\) → one up and one down (it eats a vector,
  gives back a vector).
\end{itemize}

\subsubsection{Why Two Types?}\label{why-two-types}

This distinction is not cosmetic:

\begin{itemize}
\tightlist
\item
  When we change basis, vectors and covectors transform in ``opposite''
  ways.
\item
  Having both ensures that tensor equations describe intrinsic
  relationships, independent of coordinates.
\end{itemize}

Example: Inner product

\[
\langle v, w \rangle = g_{ij} v^i w^j.
\]

Here \(g_{ij}\) is a metric tensor (covariant), combining two
contravariant vectors into a scalar.

\subsubsection{Pictures Before Symbols}\label{pictures-before-symbols}

\begin{itemize}
\tightlist
\item
  Contravariant: arrows pointing ``outward'' (directions in space).
\item
  Covariant: measuring devices pointing ``inward'' (hyperplanes that
  assign numbers to arrows).
\item
  Tensors: diagrams with arrows in and out, representing how they
  connect inputs to outputs.
\end{itemize}

This picture-based intuition helps prevent index mistakes when writing
formulas.

\subsubsection{Why This Matters}\label{why-this-matters-1}

\begin{itemize}
\tightlist
\item
  Covariant vs.~contravariant indices explain the geometry of tensors,
  not just their array form.
\item
  It prepares us for raising and lowering indices with inner products.
\item
  It ensures we can handle basis changes correctly (Chapter 12 will
  revisit this in detail).
\end{itemize}

\subsubsection{Exercises}\label{exercises-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Identify Index Type: For each object, say whether its indices are
  covariant, contravariant, or mixed:

  \begin{itemize}
  \tightlist
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \tightlist
    \item
      \(v^i\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      \(\omega_j\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      \(A^i{}_j\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\alph{enumii})}
    \setcounter{enumii}{3}
    \tightlist
    \item
      \(B_{ij}\).
    \end{enumerate}
  \end{itemize}
\item
  Dual Basis Practice: In \(\mathbb{R}^2\) with basis \(e_1, e_2\) and
  dual basis \(e^1, e^2\):

  \begin{itemize}
  \tightlist
  \item
    Write a vector \(v = 3e_1 + 4e_2\) in coordinates \(v^i\).
  \item
    Evaluate \(\omega(v)\) for \(\omega = 2e^1 - e^2\).
  \end{itemize}
\item
  Basis Change Intuition: Suppose we scale the basis of \(\mathbb{R}^2\)
  by 2: \(e'_i = 2 e_i\).

  \begin{itemize}
  \tightlist
  \item
    How do the contravariant coordinates \(v^i\) of a vector change?
  \item
    How do the covariant coordinates \(\omega_i\) of a covector change?
  \end{itemize}
\item
  Mixed Tensor Example: Interpret the meaning of a tensor \(T^i{}_j\)
  acting on a vector \(v^j\). What kind of object is the result?
\item
  Thought Experiment: Why do we need both contravariant and covariant
  indices to describe something like the dot product? What would go
  wrong if we only allowed one type?
\end{enumerate}

\subsection{3.3 Change of Basis Rules in
Coordinates}\label{change-of-basis-rules-in-coordinates}

So far we have seen that indices can be contravariant (upper) or
covariant (lower). The key difference appears when we change basis.
Multilinear algebra is all about writing rules that remain valid
regardless of coordinates, and basis transformations reveal why the
distinction is essential.

\subsubsection{Vectors Under Change of
Basis}\label{vectors-under-change-of-basis}

Let \(V = \mathbb{R}^n\). Suppose we change from an old basis
\(\{e_i\}\) to a new basis \(\{e'_i\}\):

\[
e'_i = P^j{}_i e_j,
\]

where \(P\) is the change-of-basis matrix.

\begin{itemize}
\tightlist
\item
  A vector \(v\) has coordinates \(v^i\) in the old basis and \(v'^i\)
  in the new basis.
\item
  The relation is:
\end{itemize}

\[
v'^i = (P^{-1})^i{}_j v^j.
\]

Thus, contravariant components transform with the inverse of the basis
change.

\subsubsection{Covectors Under Change of
Basis}\label{covectors-under-change-of-basis}

Now consider a covector \(\omega \in V^*\), expressed in the old basis
as \(\omega_i\). Under the same change of basis:

\[
\omega'_i = P^j{}_i \, \omega_j.
\]

Thus, covariant components transform directly with the basis change
matrix.

\subsubsection{General Tensor
Transformation}\label{general-tensor-transformation}

A tensor with both covariant and contravariant indices transforms by
applying these rules to each index:

\[
T'^{i_1 \dots i_p}{}_{j_1 \dots j_q} =
(P^{-1})^{i_1}{}_{k_1} \cdots (P^{-1})^{i_p}{}_{k_p}
\, P^{\ell_1}{}_{j_1} \cdots P^{\ell_q}{}_{j_q}
\, T^{k_1 \dots k_p}{}_{\ell_1 \dots \ell_q}.
\]

\begin{itemize}
\tightlist
\item
  Upper indices get \(P^{-1}\).
\item
  Lower indices get \(P\).
\item
  This ensures tensor equations remain coordinate-independent.
\end{itemize}

\subsubsection{Simple Example: Linear
Maps}\label{simple-example-linear-maps}

A linear map \(A: V \to V\) has components \(A^i{}_j\). Under change of
basis:

\[
A'^i{}_j = (P^{-1})^i{}_k \, A^k{}_\ell \, P^\ell{}_j.
\]

This is the familiar similarity transformation:

\[
[A]_{new} = P^{-1} [A]_{old} P.
\]

\subsubsection{Why This Matters}\label{why-this-matters-2}

\begin{itemize}
\tightlist
\item
  Basis changes test whether your formulas are intrinsic or just
  coordinate artifacts.
\item
  The distinction between covariant and contravariant is precisely what
  makes tensor equations survive basis changes intact.
\item
  In physics, this explains why laws (like Maxwell's equations or
  stress-strain relations) remain valid no matter what coordinates we
  choose.
\end{itemize}

\subsubsection{Exercises}\label{exercises-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vector Transformation: Let
  \(P = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}\).

  \begin{itemize}
  \tightlist
  \item
    If \(v = (4,6)\) in the old basis, what are its coordinates in the
    new basis?
  \end{itemize}
\item
  Covector Transformation: With the same \(P\), let \(\omega = (1,2)\)
  in the old basis. What are its coordinates in the new basis?
\item
  Matrix Transformation: Let

  \[
  A = \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix}, \quad
  P = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}.
  \]

  Compute \(A' = P^{-1} A P\).
\item
  Tensor Component Count: How many transformation matrices \(P\) and
  \(P^{-1}\) appear in the formula for a tensor of type \((2,1)\)?
\item
  Thought Experiment: Why would formulas break if we treated all indices
  as the same type (ignoring covariant vs.~contravariant)? Consider the
  dot product as an example.
\end{enumerate}

\subsection{3.4 Einstein Summation and Index
Hygiene}\label{einstein-summation-and-index-hygiene}

When working with tensors, writing every summation explicitly quickly
becomes messy. To keep formulas clean, mathematicians and physicists use
the Einstein summation convention and a set of informal ``index
hygiene'' rules.

\subsubsection{Einstein Summation
Convention}\label{einstein-summation-convention}

The rule is simple: whenever an index appears once up and once down, you
sum over it.

Example in \(\mathbb{R}^3\):

\[
y^i = A^i{}_j x^j
\]

means

\[
y^i = \sum_{j=1}^3 A^i{}_j x^j.
\]

This compact notation hides the summation symbol but makes multilinear
expressions much easier to read.

\subsubsection{Free vs.~Dummy Indices}\label{free-vs.-dummy-indices}

\begin{itemize}
\tightlist
\item
  Free indices: appear only once in a term; they label the components of
  the result.
\item
  Dummy indices: appear exactly twice (once up, once down); they are
  summed over and can be renamed arbitrarily.
\end{itemize}

Example:

\[
z^i = B^i{}_j x^j
\]

Here \(i\) is free (labels components of \(z\)), and \(j\) is a dummy
index (summed).

\subsubsection{Index Hygiene Rules}\label{index-hygiene-rules}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Never use the same index more than twice in a term.
\item
  Never mix up free and dummy indices.
\item
  Rename dummy indices freely if it helps clarity.
\end{enumerate}

Example of bad hygiene:

\[
A^i{}_i \, x^i
\]

This is ambiguous, because \(i\) appears three times. Correct it by
renaming:

\[
(A^i{}_i) \, x^j.
\]

\subsubsection{Examples of Einstein
Notation}\label{examples-of-einstein-notation}

\begin{itemize}
\tightlist
\item
  Dot product: \(\langle v,w \rangle = v^i w_i\).
\item
  Matrix-vector product: \(y^i = A^i{}_j x^j\).
\item
  Bilinear form: \(B(v,w) = B_{ij} v^i w^j\).
\item
  Trace of a matrix: \(\mathrm{tr}(A) = A^i{}_i\).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-3}

\begin{itemize}
\tightlist
\item
  Einstein summation is the language of tensors: concise, unambiguous,
  and basis-independent.
\item
  It allows formulas to be read structurally, focusing on how indices
  connect rather than on summation symbols.
\item
  Practicing good index hygiene prevents mistakes when manipulating
  complicated expressions.
\end{itemize}

\subsubsection{Exercises}\label{exercises-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Summation Practice: Expand the Einstein-summed expression

  \[
  y^i = A^i{}_j x^j
  \]

  explicitly for \(i=1,2\) when

  \[
  A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}, \quad x=(5,6).
  \]
\item
  Dot Product Check: Show that \(v^i w_i\) is invariant under a change
  of dummy index name (e.g.~rewrite with \(j\) instead of \(i\)).
\item
  Trace Calculation: For

  \[
  A = \begin{bmatrix}2 & 1 \\ 0 & -3\end{bmatrix},
  \]

  compute \(\mathrm{tr}(A)\) using Einstein notation.
\item
  Index Hygiene: Identify the mistake in the following expression and
  correct it:

  \[
  C^i = A^i{}_j B^j{}_j x^j.
  \]
\item
  Thought Experiment: Why does Einstein notation require one index up
  and one down to imply summation? What would go wrong if we summed
  whenever indices simply repeated, regardless of position?
\end{enumerate}

\section{Chapter 4. Tensors as Multilinear
Maps}\label{chapter-4.-tensors-as-multilinear-maps}

\subsection{4.1 Multilinearity and
Currying}\label{multilinearity-and-currying}

So far, we treated tensors as arrays of numbers. Now we switch to the
map viewpoint: tensors as functions that are linear in each argument
separately. This is the most natural way to see how tensors \emph{act}.

\subsubsection{What Does Multilinear
Mean?}\label{what-does-multilinear-mean}

A map \(T: V_1 \times V_2 \times \cdots \times V_k \to \mathbb{R}\) (or
to another vector space) is multilinear if it is linear in each slot,
while the others are fixed.

Example (bilinear):

\[
B(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w).
\]

The same rule holds in each argument.

\begin{itemize}
\tightlist
\item
  Linear → 1 slot.
\item
  Bilinear → 2 slots.
\item
  Trilinear → 3 slots.
\item
  k-linear → k slots.
\end{itemize}

\subsubsection{Examples of Multilinear
Maps}\label{examples-of-multilinear-maps}

\begin{itemize}
\tightlist
\item
  Dot product: \(\langle v,w\rangle\), bilinear.
\item
  Matrix-vector action: \(A(v)\), linear (1 slot).
\item
  Determinant in \(\mathbb{R}^2\): \(\det(u,v) = u_1 v_2 - u_2 v_1\),
  bilinear.
\item
  Scalar triple product: \(u \cdot (v \times w)\), trilinear.
\end{itemize}

These all satisfy linearity in each argument separately.

\subsubsection{Currying Viewpoint}\label{currying-viewpoint}

Another way to think about multilinear maps is through currying:

\begin{itemize}
\item
  A bilinear map \(B: V \times W \to \mathbb{R}\) can be seen as a
  function

  \[
  B(v,-): W \to \mathbb{R}, \quad w \mapsto B(v,w).
  \]

  So, fixing one input gives a linear map in the remaining argument.
\item
  In general, a \(k\)-linear map can be seen as a nested sequence of
  linear maps, each taking one input at a time.
\end{itemize}

This perspective helps connect multilinear maps with ordinary linear
maps, by viewing them as ``linear maps into linear maps.''

\subsubsection{Why This Matters}\label{why-this-matters-4}

\begin{itemize}
\tightlist
\item
  This viewpoint clarifies how tensors can be ``evaluated'' by plugging
  in vectors.
\item
  It connects with functional programming ideas (currying and partial
  application).
\item
  It bridges between arrays (coordinates) and abstract multilinear
  functionals.
\end{itemize}

\subsubsection{Exercises}\label{exercises-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bilinearity Check: Verify directly that the dot product
  \(\langle (x_1,y_1),(x_2,y_2)\rangle = x_1x_2 + y_1y_2\) is bilinear.
\item
  Currying Example: Let \(B(u,v) = u_1v_1 + 2u_2v_2\). For a fixed
  \(u=(1,2)\), write the resulting linear functional on \(v\).
\item
  Determinant as Bilinear Form: Show that
  \(\det(u,v) = u_1v_2 - u_2v_1\) is bilinear in \(\mathbb{R}^2\).
\item
  Trilinear Example: Prove that the scalar triple product
  \(u \cdot (v \times w)\) is trilinear by checking linearity in \(u\).
\item
  Thought Experiment: Why might it be useful to think of a bilinear map
  as a linear map into the dual space, i.e.~\(B: V \to W^*\)?
\end{enumerate}

\subsection{4.2 Evaluation with Vectors and
Covectors}\label{evaluation-with-vectors-and-covectors}

In the map viewpoint, tensors are best understood by how they act on
inputs. Depending on their type (covariant or contravariant indices),
tensors expect vectors, covectors, or both.

\subsubsection{Feeding Vectors into Covariant
Slots}\label{feeding-vectors-into-covariant-slots}

A purely covariant tensor \(T_{ij}\) is a multilinear map

\[
T: V \times V \to \mathbb{R}.
\]

\begin{itemize}
\tightlist
\item
  You feed in two vectors \(u, v \in V\).
\item
  The output is a scalar: \(T(u,v) = T_{ij} u^i v^j\).
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  A bilinear form (like an inner product) is a covariant 2-tensor.
\end{itemize}

\subsubsection{Feeding Covectors into Contravariant
Slots}\label{feeding-covectors-into-contravariant-slots}

A purely contravariant tensor \(T^{ij}\) is a multilinear map

\[
T: V^- \times V^- \to \mathbb{R}.
\]

\begin{itemize}
\tightlist
\item
  You feed in two covectors \(\alpha, \beta \in V^*\).
\item
  The output is a scalar: \(T(\alpha,\beta) = T^{ij}\alpha_i \beta_j\).
\end{itemize}

\subsubsection{Mixed Tensors: Both Types of
Inputs}\label{mixed-tensors-both-types-of-inputs}

A mixed tensor \(T^i{}_j\) acts as a map:

\[
T: V \times V^- \to \mathbb{R}.
\]

But more naturally, it can be seen as:

\begin{itemize}
\tightlist
\item
  taking a vector and giving back a vector,
\item
  or taking a covector and giving back a covector.
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  A linear operator \(A: V \to V\) has components \(A^i{}_j\). Given
  \(v^j\), it produces another vector \(w^i = A^i{}_j v^j\).
\end{itemize}

\subsubsection{Evaluating Step by Step}\label{evaluating-step-by-step}

\begin{itemize}
\tightlist
\item
  Pick a tensor.
\item
  Plug in the right kind of inputs (vector or covector) into the right
  slots.
\item
  Contract the matching indices (up with down).
\item
  The result is either a scalar, a vector, or another tensor (depending
  on how many free indices remain).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-5}

\begin{itemize}
\tightlist
\item
  This clarifies the action of tensors, not just their coordinates.
\item
  Evaluation explains why indices are placed up or down: they indicate
  what kind of input the tensor expects.
\item
  It connects naturally with Einstein summation: every evaluation is
  just an index contraction.
\end{itemize}

\subsubsection{Exercises}\label{exercises-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Evaluation of Bilinear Form: Let
  \(B_{ij} = \begin{bmatrix}1 & 2 \\ 2 & 3\end{bmatrix}\). Compute
  \(B(u,v)\) for \(u=(1,1), v=(2,3)\).
\item
  Linear Operator Action: Let
  \(A^i{}_j = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\).

  \begin{itemize}
  \tightlist
  \item
    Apply \(A\) to \(v=(4,5)\).
  \item
    Interpret the result.
  \end{itemize}
\item
  Covector Evaluation: If \(\omega = (2, -1)\) and \(v=(3,4)\), compute
  \(\omega(v)\).
\item
  Mixed Tensor Evaluation: For
  \(T^i{}_j = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}\), find the
  output vector when applied to \(v=(1,2)\).
\item
  Thought Experiment: Why does the distinction between covariant and
  contravariant slots matter when evaluating tensors? What would go
  wrong if we treated them the same?
\end{enumerate}

\subsection{4.3 From Maps to Arrays (and Back) via a
Basis}\label{from-maps-to-arrays-and-back-via-a-basis}

So far, we've looked at tensors as multilinear maps (abstract) and as
arrays of numbers (concrete). The bridge between these viewpoints is a
choice of basis.

\subsubsection{Step 1: Start with a Multilinear
Map}\label{step-1-start-with-a-multilinear-map}

Suppose \(T: V \times W \to \mathbb{R}\) is bilinear.

\begin{itemize}
\tightlist
\item
  On its own, \(T\) is just a rule for combining vectors into a number.
\item
  Example: in \(\mathbb{R}^2\),
  \(T((x_1,x_2),(y_1,y_2)) = x_1 y_1 + 2x_2 y_2\).
\end{itemize}

\subsubsection{Step 2: Choose Bases}\label{step-2-choose-bases}

Let \(\{e_i\}\) be a basis for \(V\) and \(\{f_j\}\) a basis for \(W\).

\begin{itemize}
\tightlist
\item
  We can evaluate \(T(e_i,f_j)\) for each pair of basis vectors.
\item
  These numbers form an array of components \(T_{ij}\).
\end{itemize}

So in a basis, the multilinear map has a coordinate table.

\subsubsection{Step 3: Using the Array to Reconstruct the
Map}\label{step-3-using-the-array-to-reconstruct-the-map}

For general vectors \(v = v^i e_i\) and \(w = w^j f_j\):

\[
T(v,w) = T_{ij} v^i w^j.
\]

\begin{itemize}
\tightlist
\item
  Here, the coefficients \(v^i, w^j\) are the coordinates of \(v, w\).
\item
  The array entries \(T_{ij}\) tell us how the map acts on basis
  vectors.
\end{itemize}

Thus:

\begin{itemize}
\tightlist
\item
  Abstract definition: multilinear rule.
\item
  Concrete representation: an array of numbers (depends on basis).
\end{itemize}

\subsubsection{Step 4: General Tensors}\label{step-4-general-tensors}

For a tensor of type \((p,q)\):

\begin{itemize}
\tightlist
\item
  Choose a basis for \(V\).
\item
  Evaluate the tensor on all combinations of \(q\) basis vectors and
  \(p\) dual basis covectors.
\item
  The results form a multidimensional array
  \(T^{i_1 \dots i_p}{}_{j_1 \dots j_q}\).
\end{itemize}

In coordinates:

\[
T(v_1, \dots, v_q, \omega^1, \dots, \omega^p) =
T^{i_1 \dots i_p}{}_{j_1 \dots j_q}
\, v_1^{j_1} \cdots v_q^{j_q} \, \omega^1_{i_1} \cdots \omega^p_{i_p}.
\]

\subsubsection{Why This Matters}\label{why-this-matters-6}

\begin{itemize}
\tightlist
\item
  It explains why tensors can be stored as arrays of numbers: those
  numbers are just evaluations on basis elements.
\item
  It shows why basis choice matters for components but not for the
  tensor itself.
\item
  It unifies the map viewpoint and the array viewpoint into one
  consistent framework.
\end{itemize}

\subsubsection{Exercises}\label{exercises-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix from Bilinear Form: Let
  \(T((x_1,x_2),(y_1,y_2)) = 3x_1y_1 + 2x_1y_2 + x_2y_1\).

  \begin{itemize}
  \tightlist
  \item
    Write down the matrix \([T_{ij}]\).
  \item
    Compute \(T((1,2),(3,4))\) using both the formula and the matrix.
  \end{itemize}
\item
  Basis Choice Effect: In \(\mathbb{R}^2\), let \(T\) be the dot
  product.

  \begin{itemize}
  \tightlist
  \item
    What is the matrix of \(T\) in the standard basis?
  \item
    What is the matrix of \(T\) in the basis \((1,1),(1,-1)\)?
  \end{itemize}
\item
  Coordinate Reconstruction: Let
  \(T_{ij} = \begin{bmatrix}1 & 2 \\ 0 & 3\end{bmatrix}\). Compute
  \(T(v,w)\) for \(v=(2,1), w=(1,4)\).
\item
  Higher Order Example: Suppose \(T\) is a 3rd-order tensor with
  components \(T_{ijk}\). Write the formula for \(T(u,v,w)\) in terms of
  \(T_{ijk}\), \(u^i\), \(v^j\), and \(w^k\).
\item
  Thought Experiment: Why can the same abstract tensor have different
  component arrays depending on the basis? What stays the same across
  all choices?
\end{enumerate}

\subsection{4.4 Universal Examples: Bilinear Forms, Trilinear
Mixing}\label{universal-examples-bilinear-forms-trilinear-mixing}

To make the map viewpoint concrete, let's look at a few universal
examples. These are classic multilinear maps that keep reappearing in
mathematics, physics, and data science.

\subsubsection{Example 1: Bilinear
Forms}\label{example-1-bilinear-forms}

A bilinear form is a covariant 2-tensor:

\[
B: V \times V \to \mathbb{R}, \quad B(u,v) = B_{ij} u^i v^j.
\]

\begin{itemize}
\tightlist
\item
  Dot product: \(\langle u,v \rangle = \delta_{ij} u^i v^j\).
\item
  General quadratic form: \(Q(v) = v^\top A v\).
\item
  Applications: measuring angles, energy, distance.
\end{itemize}

\subsubsection{Example 2: Determinant (Alternating Multilinear
Form)}\label{example-2-determinant-alternating-multilinear-form}

The determinant in \(\mathbb{R}^n\) is an n-linear map of the column
vectors:

\[
\det(v_1,\dots,v_n).
\]

It is:

\begin{itemize}
\tightlist
\item
  Multilinear: linear in each column separately.
\item
  Alternating: if two inputs are the same, the determinant vanishes.
\item
  Geometric meaning: volume of the parallelepiped spanned by the
  vectors.
\end{itemize}

\subsubsection{Example 3: Scalar Triple Product
(Trilinear)}\label{example-3-scalar-triple-product-trilinear}

In \(\mathbb{R}^3\):

\[
[u,v,w] = u \cdot (v \times w).
\]

\begin{itemize}
\tightlist
\item
  Trilinear: linear in each of \(u,v,w\).
\item
  Geometric meaning: signed volume of the parallelepiped formed by
  \(u,v,w\).
\item
  Appears in mechanics (torques, volumes, orientation tests).
\end{itemize}

\subsubsection{Example 4: Tensor Contraction in
Applications}\label{example-4-tensor-contraction-in-applications}

Suppose \(T: V \times W \times X \to \mathbb{R}\) with components
\(T_{ijk}\).

\begin{itemize}
\tightlist
\item
  Fixing one input reduces \(T\) to a bilinear form.
\item
  Fixing two inputs reduces \(T\) to a linear functional.
\end{itemize}

This is how general multilinear maps can be ``partially evaluated.''

\subsubsection{Example 5: Mixing Signals (Trilinear
Mixing)}\label{example-5-mixing-signals-trilinear-mixing}

In signal processing and ML, a trilinear map appears naturally:

\[
M(u,v,w) = \sum_{i,j,k} T_{ijk} u^i v^j w^k.
\]

\begin{itemize}
\tightlist
\item
  Example: a 3D convolution kernel.
\item
  Applications: image processing, tensor regression, multiway data
  analysis.
\end{itemize}

\subsubsection{Why These Examples
Matter}\label{why-these-examples-matter}

\begin{itemize}
\tightlist
\item
  They show how multilinear maps generalize familiar ideas (dot product
  → determinant → triple product).
\item
  They illustrate how multilinearity encodes geometry (lengths, areas,
  volumes).
\item
  They connect abstract tensor notation to real applications (ML,
  physics, graphics).
\end{itemize}

\subsubsection{Exercises}\label{exercises-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dot Product as Bilinear Form: Show that the dot product
  \(\langle u,v \rangle = u^i v^i\) is bilinear.
\item
  Determinant in 2D: For \(u=(1,0), v=(1,2)\), compute \(\det(u,v)\).
  Interpret geometrically.
\item
  Triple Product: Compute \([u,v,w]\) for
  \(u=(1,0,0), v=(0,1,0), w=(0,0,1)\).
\item
  Signal Mixing Example: Let \(T_{ijk} = 1\) if \(i=j=k\), and \(0\)
  otherwise, for indices \(1 \leq i,j,k \leq 2\). Compute \(M(u,v,w)\)
  for \(u=(1,2), v=(3,4), w=(5,6)\).
\item
  Thought Experiment: Why do determinants and triple products count as
  multilinear maps, not just algebraic formulas?
\end{enumerate}

\section{Chapter 5. Tensors as Elements of Tensor
Products}\label{chapter-5.-tensors-as-elements-of-tensor-products}

\subsection{\texorpdfstring{5.1 Constructing \(V \otimes W\): Intuition
and
Goals}{5.1 Constructing V \textbackslash otimes W: Intuition and Goals}}\label{constructing-v-otimes-w-intuition-and-goals}

So far, we've looked at tensors as arrays and as multilinear maps. The
third viewpoint places them as elements of tensor product spaces. This
perspective is abstract, but it unifies everything: it explains
\emph{why- multilinear maps correspond to arrays and }how- they
transform consistently.

\subsubsection{Motivation}\label{motivation}

Suppose you want to encode a bilinear map
\(B: V \times W \to \mathbb{R}\).

\begin{itemize}
\tightlist
\item
  You could record all values of \(B(e_i, f_j)\) on basis elements.
\item
  You could write it as an array \(B_{ij}\).
\item
  Or you could find a new space where a single element encodes all the
  bilinear behavior.
\end{itemize}

That new space is the tensor product space \(V \otimes W\).

\subsubsection{Intuition: Building
Blocks}\label{intuition-building-blocks}

\begin{itemize}
\item
  Given \(v \in V\) and \(w \in W\), we form a simple tensor
  \(v \otimes w\).
\item
  Think of \(v \otimes w\) as a ``formal symbol'' that remembers both
  vectors at once.
\item
  Then we allow linear combinations:

  \[
  u = \sum_k v_k \otimes w_k.
  \]
\item
  These combinations form a new vector space: \(V \otimes W\).
\end{itemize}

\subsubsection{Universal Property
(informal)}\label{universal-property-informal}

The tensor product is defined so that:

\begin{itemize}
\tightlist
\item
  Every bilinear map \(B: V \times W \to U\) factors uniquely through a
  linear map \(\tilde{B}: V \otimes W \to U\).
\item
  In plain words: ``Bilinear maps are the same thing as linear maps out
  of a tensor product.''
\end{itemize}

This property makes \(V \otimes W\) the *right- space to house tensors.

\subsubsection{Example}\label{example}

If \(V = \mathbb{R}^2, W = \mathbb{R}^3\), then \(V \otimes W\) has
dimension \(2 \times 3 = 6\).

\begin{itemize}
\item
  Basis: \(e_i \otimes f_j\) with \(i=1,2; j=1,2,3\).
\item
  A general element:

  \[
  u = \sum_{i=1}^2 \sum_{j=1}^3 c_{ij} (e_i \otimes f_j).
  \]
\item
  Coordinates \(c_{ij}\) are exactly the array components of a bilinear
  map.
\end{itemize}

\subsubsection{Goals of This Viewpoint}\label{goals-of-this-viewpoint}

\begin{itemize}
\tightlist
\item
  Provide a basis-independent foundation for tensors.
\item
  Unify maps, arrays, and algebra into one framework.
\item
  Generalize easily: \(V_1 \otimes \cdots \otimes V_k\) encodes
  \(k\)-linear maps.
\end{itemize}

\subsubsection{Exercises}\label{exercises-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dimension Count: If \(\dim V = 3\) and \(\dim W = 4\), what is
  \(\dim(V \otimes W)\)?
\item
  Simple Tensor Example: In \(\mathbb{R}^2 \otimes \mathbb{R}^2\), write
  the simple tensor \((1,2) \otimes (3,4)\) as a linear combination of
  basis elements \(e_i \otimes e_j\).
\item
  Array to Tensor: For the bilinear form
  \(B((x_1,x_2),(y_1,y_2)) = 2x_1y_1 + 3x_2y_2\), find the corresponding
  tensor in \(\mathbb{R}^2 \otimes \mathbb{R}^2\).
\item
  Universal Property Check: Suppose
  \(B: \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}\) is given by
  \(B(u,v) = u^\top v\).

  \begin{itemize}
  \tightlist
  \item
    Show how \(B\) factors through a linear map
    \(\tilde{B}: \mathbb{R}^2 \otimes \mathbb{R}^2 \to \mathbb{R}\).
  \end{itemize}
\item
  Thought Experiment: Why is it useful to replace ``bilinear maps'' with
  ``linear maps from a bigger space''? How does this simplify reasoning
  and computation?
\end{enumerate}

\subsection{5.2 Simple vs.~General
Tensors}\label{simple-vs.-general-tensors}

Now that we've seen how to build the tensor product space, we need to
distinguish between its basic ``atoms'' and more complicated elements.

\subsubsection{Simple (Decomposable)
Tensors}\label{simple-decomposable-tensors}

A simple tensor (also called pure or decomposable) is one that can be
written as a single tensor product:

\[
u = v \otimes w.
\]

Examples:

\begin{itemize}
\item
  In \(\mathbb{R}^2 \otimes \mathbb{R}^3\), \((1,2) \otimes (0,1,1)\) is
  simple.
\item
  Its array of components is an outer product:

  \[
  \begin{bmatrix}1 \\ 2\end{bmatrix}
  \otimes
  \begin{bmatrix}0 & 1 & 1\end{bmatrix}
  =
  \begin{bmatrix}0 & 1 & 1 \\ 0 & 2 & 2\end{bmatrix}.
  \]
\end{itemize}

So simple tensors correspond to rank-one arrays (outer products).

\subsubsection{General Tensors}\label{general-tensors}

A general tensor in \(V \otimes W\) is a linear combination of simple
tensors:

\[
T = \sum_{k=1}^m v_k \otimes w_k.
\]

\begin{itemize}
\tightlist
\item
  Some tensors can be expressed as a single simple tensor.
\item
  Most cannot; they require a sum of several simple tensors.
\item
  The minimal number of terms in such a sum is called the tensor rank
  (analogous to matrix rank for order-2 tensors).
\end{itemize}

\subsubsection{Matrix Analogy}\label{matrix-analogy}

\begin{itemize}
\tightlist
\item
  Simple tensors ↔ rank-one matrices (outer products of a column and a
  row).
\item
  General tensors ↔ arbitrary matrices (sums of rank-one pieces).
\end{itemize}

For higher-order tensors, the situation is the same:

\begin{itemize}
\tightlist
\item
  Simple tensor = one outer product of vectors.
\item
  General tensor = sum of several such products.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-7}

\begin{itemize}
\tightlist
\item
  The distinction underlies tensor decomposition methods (CP, Tucker,
  TT).
\item
  Simple tensors capture the ``building blocks'' of tensor spaces.
\item
  General tensors encode richer structure, but often we approximate them
  by a small number of simple ones.
\end{itemize}

\subsubsection{Exercises}\label{exercises-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simple or Not? In \(\mathbb{R}^2 \otimes \mathbb{R}^2\), determine
  whether

  \[
  T = e_1 \otimes e_1 + e_2 \otimes e_2
  \]

  is a simple tensor.
\item
  Outer Product Calculation: Compute the outer product of \(u=(1,2)\)
  and \(v=(3,4,5)\). Write the resulting \(2 \times 3\) matrix.
\item
  Rank-One Check: Given the matrix

  \[
  M = \begin{bmatrix}2 & 4 \\ 3 & 6\end{bmatrix},
  \]

  show that \(M\) is a simple tensor (rank one) by writing it as
  \(u \otimes v\).
\item
  General Tensor Example: Express

  \[
  \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}
  \]

  as a sum of two simple tensors in
  \(\mathbb{R}^2 \otimes \mathbb{R}^2\).
\item
  Thought Experiment: Why are most tensors not simple? What does this
  imply about the usefulness of tensor decompositions in applications
  like machine learning?
\end{enumerate}

\subsection{5.3 Dimension and Bases of Tensor
Products}\label{dimension-and-bases-of-tensor-products}

Having distinguished simple tensors from general ones, let's now
describe the structure of a tensor product space: its dimension and
basis.

\subsubsection{Dimension Formula}\label{dimension-formula}

If \(V\) and \(W\) are finite-dimensional vector spaces, then:

\[
\dim(V \otimes W) = \dim(V) \cdot \dim(W).
\]

More generally, for \(V_1, V_2, \dots, V_k\):

\[
\dim(V_1 \otimes V_2 \otimes \cdots \otimes V_k) = \prod_{i=1}^k \dim(V_i).
\]

This matches our intuition that each index multiplies the number of
``slots'' in the array representation.

\subsubsection{Basis of a Tensor
Product}\label{basis-of-a-tensor-product}

Suppose \(\{e_i\}\) is a basis for \(V\) and \(\{f_j\}\) is a basis for
\(W\).

\begin{itemize}
\item
  Then the set

  \[
  \{e_i \otimes f_j : 1 \leq i \leq \dim V, \, 1 \leq j \leq \dim W \}
  \]

  forms a basis for \(V \otimes W\).
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  If \(\dim V = 2, \dim W = 3\):
  \(\{e_1 \otimes f_1, e_1 \otimes f_2, e_1 \otimes f_3, e_2 \otimes f_1, e_2 \otimes f_2, e_2 \otimes f_3\}\)
  is a basis (6 elements).
\end{itemize}

\subsubsection{Coordinates in the Basis}\label{coordinates-in-the-basis}

A general tensor can be expressed as:

\[
T = \sum_{i,j} c_{ij} \, (e_i \otimes f_j),
\]

where the coefficients \(c_{ij}\) form the familiar matrix (array)
representation.

For higher-order tensor products, the same logic applies:

\[
T = \sum_{i,j,k} c_{ijk} \, (e_i \otimes f_j \otimes g_k).
\]

\subsubsection{Analogy with Arrays}\label{analogy-with-arrays}

\begin{itemize}
\tightlist
\item
  Basis elements correspond to array positions.
\item
  Coefficients are the entries.
\item
  The dimension formula tells us the total number of independent
  entries.
\end{itemize}

Thus, the product-space viewpoint and the array viewpoint are perfectly
aligned.

\subsubsection{Why This Matters}\label{why-this-matters-8}

\begin{itemize}
\tightlist
\item
  The basis description explains why tensor product spaces have the same
  ``shape'' as arrays.
\item
  It provides a rigorous foundation for the component representation of
  tensors.
\item
  It prepares us to reconcile all three viewpoints (array, map, product
  space).
\end{itemize}

\subsubsection{Exercises}\label{exercises-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dimension Count: Compute \(\dim(\mathbb{R}^2 \otimes \mathbb{R}^3)\).
  List a basis explicitly.
\item
  Basis Expansion: Express \(T = (1,2) \otimes (3,4,5)\) in terms of the
  standard basis \(e_i \otimes f_j\).
\item
  Higher-Order Example: If \(\dim U = 2, \dim V = 2, \dim W = 2\), what
  is \(\dim(U \otimes V \otimes W)\)?
\item
  Coefficient Identification: Let
  \(T = e_1 \otimes f_1 + 2 e_2 \otimes f_3\). What are the nonzero
  coefficients \(c_{ij}\)?
\item
  Thought Experiment: Why does the formula
  \(\dim(V \otimes W) = \dim V \cdot \dim W\) make sense if you think of
  arrays? How does this match the earlier idea of ``shape''?
\end{enumerate}

\subsection{5.4 Three Viewpoints
Reconciled}\label{three-viewpoints-reconciled}

We now have three different ways to understand tensors: as arrays, as
multilinear maps, and as elements of tensor product spaces. Each looks
different, but they are all equivalent perspectives on the same object.

\subsubsection{1. Array View (Concrete)}\label{array-view-concrete}

\begin{itemize}
\tightlist
\item
  A tensor is a multi-dimensional array of numbers.
\item
  Its entries depend on a choice of basis.
\item
  Operations are done by indexing and summing (Einstein notation).
\item
  Example: a \(2 \times 3 \times 4\) tensor is just a 3D block of \(24\)
  numbers.
\end{itemize}

Strengths: intuitive, easy to compute. Limitations: depends heavily on
coordinates.

\subsubsection{2. Map View (Functional)}\label{map-view-functional}

\begin{itemize}
\item
  A tensor is a multilinear function that takes vectors and covectors as
  inputs and produces scalars or other tensors.
\item
  Example:

  \begin{itemize}
  \tightlist
  \item
    Dot product: bilinear map.
  \item
    Determinant: alternating multilinear map.
  \item
    Linear operators: mixed tensors.
  \end{itemize}
\end{itemize}

Strengths: coordinate-free, emphasizes action. Limitations: abstract,
less computational.

\subsubsection{3. Product Space View
(Foundational)}\label{product-space-view-foundational}

\begin{itemize}
\tightlist
\item
  A tensor is an element of a tensor product space
  \(V^{\otimes p} \otimes (V^*)^{\otimes q}\).
\item
  Basis choice identifies this element with an array of components.
\item
  Universal property: multilinear maps from vectors ↔ linear maps out of
  tensor products.
\end{itemize}

Strengths: rigorous, unifying, handles all cases. Limitations: abstract
at first, harder to visualize.

\subsubsection{Reconciling Them}\label{reconciling-them}

\begin{itemize}
\tightlist
\item
  Array = product-space + basis.
\item
  Map = product-space + evaluation on inputs.
\item
  Product-space = abstract ``home'' that makes the other two consistent.
\end{itemize}

So:

\begin{itemize}
\tightlist
\item
  If you want computation, use arrays.
\item
  If you want geometry or physics intuition, use maps.
\item
  If you want rigor and generality, use tensor products.
\end{itemize}

They are not rivals but complementary languages describing the same
object.

\subsubsection{Why This Matters}\label{why-this-matters-9}

Understanding all three viewpoints and moving fluidly between them is
the hallmark of real mastery. It's like being fluent in three languages:
you choose the one that fits the context, but you know they describe the
same reality.

\subsection{Exercises}\label{exercises-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Array ↔ Map: Let
  \(T_{ij} = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}\).

  \begin{itemize}
  \tightlist
  \item
    Write \(T(u,v) = T_{ij} u^i v^j\).
  \item
    Evaluate for \(u=(1,0), v=(0,1)\).
  \end{itemize}
\item
  Map ↔ Product-Space: Show that the bilinear form
  \(B(u,v) = u_1v_1 + 2u_2v_2\) corresponds to an element of
  \(\mathbb{R}^2 \otimes \mathbb{R}^2\).
\item
  Array ↔ Product-Space: If
  \(T = 3 e_1 \otimes f_2 + 5 e_2 \otimes f_3\), what are the nonzero
  array components \(T_{ij}\)?
\item
  Three Languages: Write the dot product of \(u,v \in \mathbb{R}^2\) in
  each of the three viewpoints:

  \begin{itemize}
  \tightlist
  \item
    Array form
  \item
    Multilinear map
  \item
    Tensor product element
  \end{itemize}
\item
  Thought Experiment: Why is it valuable to know all three viewpoints
  instead of just one? Give an application where each viewpoint is the
  most natural.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part III. Core Operations on
Tensors}\label{part-iii.-core-operations-on-tensors}

\section{Chapter 6. Building Blocks}\label{chapter-6.-building-blocks}

\subsection{6.1 Tensor (Outer) Product}\label{tensor-outer-product}

Now that we've reconciled the three viewpoints, we can explore the core
operations on tensors. The first and most fundamental is the tensor
product itself, also known as the outer product in the array viewpoint.

\subsubsection{Definition}\label{definition}

Given two vectors \(u \in V\) and \(v \in W\), their tensor product is

\[
u \otimes v \in V \otimes W.
\]

\begin{itemize}
\tightlist
\item
  It is bilinear in \(u,v\).
\item
  In a basis, it looks like an array of rank one (outer product).
\end{itemize}

\subsubsection{Example with Vectors}\label{example-with-vectors}

If \(u = (1,2)\) and \(v = (3,4,5)\):

\[
u \otimes v =
\begin{bmatrix}1 \\ 2\end{bmatrix}
\otimes
\begin{bmatrix}3 & 4 & 5\end{bmatrix}
=
\begin{bmatrix}
3 & 4 & 5 \\
6 & 8 & 10
\end{bmatrix}.
\]

This is a \(2 \times 3\) array - exactly the outer product.

\subsubsection{Higher-Order Outer
Products}\label{higher-order-outer-products}

We can extend this:

\[
u \otimes v \otimes w,
\]

produces a 3rd-order tensor, with entries

\[
(u \otimes v \otimes w)_{ijk} = u_i v_j w_k.
\]

In general, the outer product of \(k\) vectors is a simple tensor of
order \(k\).

\subsubsection{Properties}\label{properties}

\begin{itemize}
\tightlist
\item
  Bilinearity:
  \((au + bu') \otimes v = a(u \otimes v) + b(u' \otimes v)\).
\item
  Noncommutativity: \(u \otimes v \neq v \otimes u\) (unless dimensions
  align and we impose symmetry).
\item
  Rank-One Structure: Outer products produce the simplest tensors,
  forming the building blocks of general tensors.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-10}

\begin{itemize}
\tightlist
\item
  The tensor product (outer product) is the construction rule for
  higher-order tensors.
\item
  It connects vector multiplication with multidimensional arrays.
\item
  Many decompositions (CP, Tucker, tensor train) rely on sums of outer
  products.
\end{itemize}

\subsubsection{Exercises}\label{exercises-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basic Outer Product: Compute \((2,1) \otimes (1,3)\). Write it as a
  \(2 \times 2\) matrix.
\item
  Higher Order: Compute \((1,2) \otimes (0,1) \otimes (3,4)\). What is
  the shape of the resulting tensor, and how many entries does it have?
\item
  Linearity Check: Verify that
  \((u+u') \otimes v = u \otimes v + u' \otimes v\) for
  \(u=(1,0), u'=(0,1), v=(2,3)\).
\item
  Rank-One Matrix: Show that every outer product \(u \otimes v\) is a
  rank-one matrix. Give an example.
\item
  Thought Experiment: Why does the outer product produce the
  ``simplest'' tensors? How does this idea generalize from matrices to
  higher-order arrays?
\end{enumerate}

\subsection{6.2 Contraction: Summing Paired
Indices}\label{contraction-summing-paired-indices}

If the tensor product (outer product) builds bigger tensors, then
contraction is the operation that reduces them by ``pairing up'' an
upper and a lower index and summing over it. This is the natural
generalization of the dot product and the trace.

\subsubsection{Definition}\label{definition-1}

Given a tensor \(T^{i}{}_{j}\), contracting on the index \(i\) and \(j\)
means:

\[
\mathrm{contr}(T) = T^{i}{}_{i}.
\]

\begin{itemize}
\tightlist
\item
  You match one upper and one lower index.
\item
  You sum over that index.
\item
  The result is a new tensor of lower order.
\end{itemize}

\subsubsection{Familiar Examples}\label{familiar-examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dot Product: For vectors \(u^i\) and \(v_i\):

  \[
  u^i v_i
  \]

  is a contraction, producing a scalar.
\item
  Matrix-Vector Multiplication:

  \[
  y^i = A^i{}_j x^j
  \]

  contracts on \(j\), leaving a vector \(y^i\).
\item
  Trace of a Matrix:

  \[
  \mathrm{tr}(A) = A^i{}_i
  \]

  contracts on the row and column indices, producing a scalar.
\end{enumerate}

\subsubsection{General Contraction}\label{general-contraction}

If \(T^{i_1 i_2 \dots i_p}{}_{j_1 j_2 \dots j_q}\) is a tensor,
contracting on \(i_k\) and \(j_\ell\) yields a tensor of type
\((p-1, q-1)\).

This is how tensor calculus generalizes dot products, matrix
multiplication, and traces into one operation.

\subsubsection{Why This Matters}\label{why-this-matters-11}

\begin{itemize}
\tightlist
\item
  Contraction is the dual operation to the tensor product.
\item
  Outer product increases order, contraction decreases order.
\item
  Together, they generate most tensor operations in mathematics,
  physics, and machine learning.
\end{itemize}

\subsubsection{Exercises}\label{exercises-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dot Product as Contraction: Show explicitly that
  \(u \cdot v = u^i v_i\) is a contraction.
\item
  Matrix-Vector Multiplication: Let
  \(A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}\), \(x=(5,6)\).
  Compute \(y^i = A^i{}_j x^j\).
\item
  Trace Calculation: For
  \(A = \begin{bmatrix}2 & 1 \\ 0 & -3\end{bmatrix}\), compute
  \(\mathrm{tr}(A)\) via contraction.
\item
  Higher-Order Contraction: Suppose \(T^{ij}{}_{k\ell}\) is a 4th-order
  tensor. What is the order and type of the tensor obtained by
  contracting on \(j\) and \(\ell\)?
\item
  Thought Experiment: Why can contraction only happen between one upper
  and one lower index? What would break if you tried to contract two
  uppers or two lowers?
\end{enumerate}

\subsection{6.3 Permutations of Modes: Transpose, Unfold,
Matricize}\label{permutations-of-modes-transpose-unfold-matricize}

Tensors often need to be rearranged so that their structure fits the
operation we want to perform. This section introduces three related
operations: permutation of modes, transpose, and matricization
(unfolding).

\subsubsection{Permuting Modes}\label{permuting-modes}

A tensor of order \(k\) has \(k\) indices:

\[
T_{i_1 i_2 \dots i_k}.
\]

A permutation of modes reorders these indices.

\begin{itemize}
\tightlist
\item
  Example: if \(T\) is order 3, with entries \(T_{ijk}\), then a
  permutation might yield \(T_{kij}\).
\item
  This doesn't change the data, just how we view the axes.
\end{itemize}

\subsubsection{Transpose (Matrix Case)}\label{transpose-matrix-case}

For a 2nd-order tensor (matrix), permutation of the two indices
corresponds to the familiar transpose:

\[
A_{ij} \mapsto A_{ji}.
\]

This is just a special case of permuting modes.

\subsubsection{Matricization (Unfolding)}\label{matricization-unfolding}

For higher-order tensors, it is often useful to flatten them into
matrices.

\begin{itemize}
\tightlist
\item
  Choose one index (or a group of indices) to form the rows.
\item
  The remaining indices form the columns.
\end{itemize}

Example: For a 3rd-order tensor \(T_{ijk}\):

\begin{itemize}
\tightlist
\item
  Mode-1 unfolding: rows indexed by \(i\), columns by \((j,k)\).
\item
  Mode-2 unfolding: rows indexed by \(j\), columns by \((i,k)\).
\item
  Mode-3 unfolding: rows indexed by \(k\), columns by \((i,j)\).
\end{itemize}

This is widely used in numerical linear algebra and machine learning.

\subsubsection{Why Permutations Matter}\label{why-permutations-matter}

\begin{itemize}
\tightlist
\item
  They allow us to reinterpret tensors for algorithms (e.g., apply
  matrix methods to unfolded tensors).
\item
  They help align indices correctly before contraction.
\item
  They reveal symmetry or structure hidden in the raw array.
\end{itemize}

\subsubsection{Exercises}\label{exercises-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Permutation Practice: If \(T_{ijk}\) has shape \((2,3,4)\), what is
  the shape of the permuted tensor \(T_{kij}\)?
\item
  Matrix Transpose: Write the transpose of

  \[
  A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}.
  \]
\item
  Mode-1 Unfolding: For a tensor \(T_{ijk}\) of shape \((2,2,2)\), what
  is the shape of its mode-1 unfolding?
\item
  Reversibility: Show that permuting modes twice with inverse
  permutations returns the original tensor.
\item
  Thought Experiment: Why might unfolding a tensor into a matrix help in
  data analysis (e.g., PCA, SVD)? What do we lose by unfolding?
\end{enumerate}

\subsection{6.4 Kronecker Product vs.~Tensor
Product}\label{kronecker-product-vs.-tensor-product}

The tensor product and the Kronecker product are closely related, but
they live in slightly different worlds. Beginners often confuse them
because they both ``blow up'' dimensions in similar ways. Let's
carefully separate them.

\subsubsection{Tensor Product (Abstract)}\label{tensor-product-abstract}

\begin{itemize}
\tightlist
\item
  Given two vector spaces \(V, W\), their tensor product \(V \otimes W\)
  is a new vector space.
\item
  If \(\dim V = m\) and \(\dim W = n\), then \(\dim(V \otimes W) = mn\).
\item
  Basis: \(e_i \otimes f_j\).
\item
  Purpose: encodes bilinear maps as linear maps.
\end{itemize}

Example: For \(u=(u_1,u_2)\), \(v=(v_1,v_2,v_3)\):

\[
u \otimes v =
\begin{bmatrix}
u_1v_1 & u_1v_2 & u_1v_3 \\
u_2v_1 & u_2v_2 & u_2v_3
\end{bmatrix}.
\]

\subsubsection{Kronecker Product (Matrix
Operation)}\label{kronecker-product-matrix-operation}

\begin{itemize}
\tightlist
\item
  Given two matrices \(A \in \mathbb{R}^{m \times n}\),
  \(B \in \mathbb{R}^{p \times q}\), the Kronecker product is:
\end{itemize}

\[
A \otimes B =
\begin{bmatrix}
a_{11}B & a_{12}B & \cdots & a_{1n}B \\
a_{21}B & a_{22}B & \cdots & a_{2n}B \\
\vdots  & \vdots  & \ddots & \vdots \\
a_{m1}B & a_{m2}B & \cdots & a_{mn}B
\end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  This produces a block matrix of size \((mp) \times (nq)\).
\item
  Used heavily in linear algebra identities, signal processing, and ML.
\end{itemize}

\subsubsection{Relation Between the Two}\label{relation-between-the-two}

\begin{itemize}
\item
  The Kronecker product is the matrix representation of the tensor
  product once bases are chosen.
\item
  In other words:

  \begin{itemize}
  \tightlist
  \item
    Tensor product = basis-independent, abstract.
  \item
    Kronecker product = concrete array form.
  \end{itemize}
\end{itemize}

\subsubsection{Examples}\label{examples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vector Tensor Product:
  \((1,2) \otimes (3,4) = \begin{bmatrix}3 & 4 \\ 6 & 8\end{bmatrix}.\)
\item
  Matrix Kronecker Product:

  \[
  \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} \otimes
  \begin{bmatrix}0 & 5 \\ 6 & 7\end{bmatrix}
  =
  \begin{bmatrix}
  1\cdot B & 2\cdot B \\
  3\cdot B & 4\cdot B
  \end{bmatrix}.
  \]
\end{enumerate}

\subsubsection{Why This Matters}\label{why-this-matters-12}

\begin{itemize}
\tightlist
\item
  Tensor product explains the theory of multilinearity.
\item
  Kronecker product is the computational tool, letting us work with
  actual arrays.
\item
  Keeping them distinct prevents confusion between \emph{concept- and
  }representation*.
\end{itemize}

\subsubsection{Exercises}\label{exercises-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basic Kronecker: Compute

  \[
  \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} \otimes
  \begin{bmatrix}a & b \\ c & d\end{bmatrix}.
  \]
\item
  Dimension Check: If \(A\) is \(2 \times 3\) and \(B\) is
  \(3 \times 4\), what is the size of \(A \otimes B\)?
\item
  Tensor vs.~Kronecker: Show explicitly that for vectors \(u,v\), the
  tensor product \(u \otimes v\) corresponds to the Kronecker product of
  \(u\) and \(v^\top\).
\item
  Outer Product Relation: Express the outer product \(u \otimes v\) as a
  Kronecker product when \(u,v\) are vectors.
\item
  Thought Experiment: Why is it useful to separate the abstract tensor
  product from the concrete Kronecker product? What problems might arise
  if we conflated them?
\end{enumerate}

\section{Chapter 7. Symmetry and
(Anti)Symmetry}\label{chapter-7.-symmetry-and-antisymmetry}

\subsection{7.1 Symmetrization
Operators}\label{symmetrization-operators}

One of the most important features of tensors is how they behave under
permutations of indices. Some tensors stay the same when you swap
indices (symmetric), others change sign (antisymmetric). To formalize
this, we introduce symmetrization operators.

\subsubsection{Symmetric Tensors}\label{symmetric-tensors}

A tensor \(T_{ij}\) is symmetric if swapping indices does not change it:

\[
T_{ij} = T_{ji}.
\]

More generally, a \(k\)-tensor \(T_{i_1 i_2 \dots i_k}\) is symmetric
if:

\[
T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}} = T_{i_1 i_2 \dots i_k}
\]

for every permutation \(\pi\).

Example:

\begin{itemize}
\tightlist
\item
  The Hessian matrix of a scalar function is symmetric.
\item
  A covariance matrix is symmetric.
\end{itemize}

\subsubsection{Symmetrization Operator}\label{symmetrization-operator}

Given any tensor \(T\), its symmetrized version is obtained by averaging
over all permutations of indices:

\[
\mathrm{Sym}(T)_{i_1 i_2 \dots i_k} = \frac{1}{k!} \sum_{\pi \in S_k} T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}.
\]

\begin{itemize}
\tightlist
\item
  \(S_k\) is the symmetric group on \(k\) indices.
\item
  This ensures the result is symmetric, even if the original tensor was
  not.
\end{itemize}

\subsubsection{Example: Symmetrizing a 2nd-Order
Tensor}\label{example-symmetrizing-a-2nd-order-tensor}

Given a matrix \(A\), its symmetrization is:

\[
\mathrm{Sym}(A) = \frac{1}{2}(A + A^\top).
\]

This extracts the symmetric part of a matrix.

\subsubsection{Properties}\label{properties-1}

\begin{itemize}
\tightlist
\item
  Symmetrization is a projection operator: applying it twice gives the
  same result.
\item
  The space of symmetric tensors is a subspace of the full tensor space.
\item
  Symmetric tensors are often much smaller in dimension than general
  tensors.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-13}

\begin{itemize}
\tightlist
\item
  Symmetrization leads to symmetric tensor spaces, central in polynomial
  algebra and statistics.
\item
  It prepares us for the study of alternating tensors (exterior algebra)
  in the next part.
\item
  Many applications (covariance, stress, Hessians) naturally produce
  symmetric tensors.
\end{itemize}

\subsubsection{Exercises}\label{exercises-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Symmetrization: Compute the symmetrized version of

  \[
  A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}.
  \]
\item
  Check Symmetry: Is the tensor \(T_{ij}\) with entries
  \(T_{12}=1, T_{21}=2, T_{11}=0, T_{22}=3\) symmetric?
\item
  Symmetrizing a 3rd-Order Tensor: Write the formula for the symmetrized
  version of a 3rd-order tensor \(T_{ijk}\).
\item
  Projection Property: Show that if \(T\) is already symmetric, then
  \(\mathrm{Sym}(T) = T\).
\item
  Thought Experiment: Why might symmetrization be important in
  statistics (e.g., covariance estimation) or in physics (e.g., stress
  tensors)?
\end{enumerate}

\subsection{7.2 Alternation
(Antisymmetrization)}\label{alternation-antisymmetrization}

If symmetrization enforces invariance under index swaps, alternation (or
antisymmetrization) enforces the opposite: swapping two indices flips
the sign. This construction is central to exterior algebra and geometric
concepts like orientation and volume.

\subsubsection{Antisymmetric Tensors}\label{antisymmetric-tensors}

A tensor \(T_{ij}\) is antisymmetric if

\[
T_{ij} = -T_{ji}.
\]

\begin{itemize}
\item
  In particular, \(T_{ii} = 0\).
\item
  For higher orders:

  \[
  T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}} = \mathrm{sgn}(\pi) \, T_{i_1 i_2 \dots i_k},
  \]

  where \(\mathrm{sgn}(\pi)\) is the sign of the permutation \(\pi\).
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  The determinant is based on a fully antisymmetric tensor.
\item
  The cross product in \(\mathbb{R}^3\) can be expressed using an
  antisymmetric tensor.
\end{itemize}

\subsubsection{Alternation Operator}\label{alternation-operator}

Given any tensor \(T\), its alternated version is obtained by summing
with signs:

\[
\mathrm{Alt}(T)_{i_1 i_2 \dots i_k} = \frac{1}{k!} \sum_{\pi \in S_k} \mathrm{sgn}(\pi) \, T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}.
\]

\begin{itemize}
\tightlist
\item
  This forces antisymmetry.
\item
  If \(T\) was already antisymmetric, \(\mathrm{Alt}(T) = T\).
\end{itemize}

\subsubsection{Example: Alternating a 2nd-Order
Tensor}\label{example-alternating-a-2nd-order-tensor}

For a matrix \(A\), alternation gives:

\[
\mathrm{Alt}(A) = \frac{1}{2}(A - A^\top).
\]

This extracts the skew-symmetric part of a matrix.

\subsubsection{Properties}\label{properties-2}

\begin{itemize}
\tightlist
\item
  Alternation is a projection operator:
  \(\mathrm{Alt}(\mathrm{Alt}(T)) = \mathrm{Alt}(T)\).
\item
  The space of antisymmetric \(k\)-tensors is called \(\Lambda^k(V)\),
  the exterior power.
\item
  Its dimension is \(\binom{n}{k}\), much smaller than the full tensor
  space.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-14}

\begin{itemize}
\tightlist
\item
  Alternating tensors represent oriented areas, volumes, and
  higher-dimensional analogues.
\item
  They are the foundation of differential forms and exterior algebra.
\item
  Physics applications: angular momentum, electromagnetism,
  determinants.
\end{itemize}

\subsubsection{Exercises}\label{exercises-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Skew-Symmetric Matrix: Compute the alternated version of

  \[
  A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}.
  \]
\item
  Check Antisymmetry: Is the tensor \(T_{ij}\) with
  \(T_{12}=1, T_{21}=-1, T_{11}=0, T_{22}=0\) antisymmetric?
\item
  Alternating a 3rd-Order Tensor: Write the formula for
  \(\mathrm{Alt}(T)_{ijk}\) explicitly in terms of the six permutations
  of \((i,j,k)\).
\item
  Zero Property: Show that if two indices are equal in an antisymmetric
  tensor, the component must vanish.
\item
  Thought Experiment: Why is antisymmetry essential for defining
  oriented volume (via determinants) and exterior algebra?
\end{enumerate}

\subsection{7.3 Decompositions by Symmetry
Type}\label{decompositions-by-symmetry-type}

So far we've seen how to symmetrize and antisymmetrize tensors. The next
step is to notice that *any- tensor can be decomposed into symmetric and
antisymmetric parts. This is very similar to how every matrix can be
written as the sum of a symmetric and a skew-symmetric matrix.

\subsubsection{The 2nd-Order Case
(Matrices)}\label{the-2nd-order-case-matrices}

Given a matrix \(A\):

\[
A = \tfrac{1}{2}(A + A^\top) + \tfrac{1}{2}(A - A^\top).
\]

\begin{itemize}
\tightlist
\item
  The first term is symmetric.
\item
  The second term is antisymmetric.
\item
  Together, they reconstruct the original matrix.
\end{itemize}

This is the simplest case of decomposition by symmetry type.

\subsubsection{Higher-Order
Generalization}\label{higher-order-generalization}

For a tensor \(T_{i_1 \dots i_k}\):

\begin{itemize}
\tightlist
\item
  Apply the symmetrization operator to get the symmetric part.
\item
  Apply the alternation operator to get the antisymmetric part.
\end{itemize}

But for \(k > 2\), there are many possible symmetry types (not just
symmetric vs.~antisymmetric).

\subsubsection{Young Diagrams and Symmetry Types
(Preview)}\label{young-diagrams-and-symmetry-types-preview}

In general, the ways indices can be symmetrized/antisymmetrized
correspond to representations of the symmetric group.

\begin{itemize}
\tightlist
\item
  Fully symmetric: indices invariant under any swap.
\item
  Fully antisymmetric: indices change sign under swaps.
\item
  Mixed types: some indices symmetric among themselves, others
  antisymmetric.
\end{itemize}

These patterns can be visualized with Young diagrams (a deeper subject
in representation theory).

\subsubsection{Why Decomposition
Matters}\label{why-decomposition-matters}

\begin{itemize}
\item
  Symmetry often reduces the effective dimension of tensor spaces.
\item
  Many natural tensors fall into specific symmetry classes (e.g., metric
  tensors are symmetric, differential forms are antisymmetric).
\item
  In applications, splitting into symmetric/antisymmetric parts
  clarifies the geometric meaning:

  \begin{itemize}
  \tightlist
  \item
    Symmetric part → stretching, scaling (like stress/strain).
  \item
    Antisymmetric part → rotation, orientation (like angular momentum).
  \end{itemize}
\end{itemize}

\subsubsection{Exercises}\label{exercises-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Decomposition: Decompose

  \[
  A = \begin{bmatrix}1 & 3 \\ 2 & 4\end{bmatrix}
  \]

  into symmetric and antisymmetric parts.
\item
  Sym + Anti Check: Verify that every \(2 \times 2\) matrix can be
  written uniquely as \(S + K\), with \(S\) symmetric and \(K\)
  antisymmetric.
\item
  Symmetric Projection: For \(T_{ijk}\), write the formula for its
  symmetric part \(\mathrm{Sym}(T)_{ijk}\).
\item
  Antisymmetric Projection: For \(T_{ijk}\), write the formula for its
  antisymmetric part \(\mathrm{Alt}(T)_{ijk}\).
\item
  Thought Experiment: Why does decomposing tensors by symmetry type help
  in physics and engineering (e.g., stress tensors, electromagnetic
  fields)? \#\#\# 7.4 Applications: Determinants and Oriented Volumes
\end{enumerate}

Now we put the ideas of symmetry and antisymmetry into practice. Some of
the most important geometric and physical quantities are built from
alternating tensors - especially the determinant and oriented volumes.

\subsubsection{Determinant as an Antisymmetric Multilinear
Map}\label{determinant-as-an-antisymmetric-multilinear-map}

The determinant of an \(n \times n\) matrix can be seen as an
alternating \(n\)-linear form:

\[
\det(v_1, v_2, \dots, v_n),
\]

where \(v_i\) are the column vectors of the matrix.

\begin{itemize}
\tightlist
\item
  Multilinear: linear in each column.
\item
  Alternating: if two columns are equal, the determinant vanishes.
\item
  Geometric meaning: signed volume of the parallelepiped spanned by the
  columns.
\end{itemize}

\subsubsection{\texorpdfstring{Oriented Areas in
\(\mathbb{R}^2\)}{Oriented Areas in \textbackslash mathbb\{R\}\^{}2}}\label{oriented-areas-in-mathbbr2}

Given two vectors \(u, v \in \mathbb{R}^2\), the area of the
parallelogram they span is:

\[
\mathrm{Area}(u,v) = | \det(u,v) |.
\]

The sign of \(\det(u,v)\) indicates orientation (clockwise
vs.~counterclockwise).

\subsubsection{\texorpdfstring{Oriented Volumes in
\(\mathbb{R}^3\)}{Oriented Volumes in \textbackslash mathbb\{R\}\^{}3}}\label{oriented-volumes-in-mathbbr3}

Given three vectors \(u,v,w \in \mathbb{R}^3\), the scalar triple
product

\[
[u,v,w] = u \cdot (v \times w)
\]

is the determinant of the \(3 \times 3\) matrix with columns \(u,v,w\).

\begin{itemize}
\tightlist
\item
  Magnitude: volume of the parallelepiped.
\item
  Sign: orientation (right-handed vs.~left-handed system).
\end{itemize}

\subsubsection{Exterior Powers and General
Volumes}\label{exterior-powers-and-general-volumes}

The antisymmetric space \(\Lambda^k(V)\) encodes oriented
\(k\)-dimensional volumes:

\begin{itemize}
\tightlist
\item
  For \(k=2\): oriented areas (parallelograms).
\item
  For \(k=3\): oriented volumes (parallelepipeds).
\item
  For general \(k\): higher-dimensional ``hyper-volumes.''
\end{itemize}

Thus, alternating tensors generalize determinants and orientation to any
dimension.

\subsubsection{Why This Matters}\label{why-this-matters-15}

\begin{itemize}
\tightlist
\item
  Determinants, areas, and volumes are not just numbers - they are built
  from antisymmetric tensors.
\item
  This viewpoint makes clear why determinants vanish when vectors are
  linearly dependent: antisymmetry forces collapse.
\item
  Orientation is encoded algebraically, which is crucial in physics
  (torques, flux) and geometry (orientation of manifolds).
\end{itemize}

\subsubsection{Exercises}\label{exercises-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  2D Determinant: Compute \(\det\big((1,2),(3,4)\big)\). Interpret the
  result geometrically.
\item
  Area from Determinant: Find the area of the parallelogram spanned by
  \(u=(2,0)\) and \(v=(1,3)\) using determinants.
\item
  Triple Product: Compute \([u,v,w]\) for \(u=(1,0,0)\), \(v=(0,2,0)\),
  \(w=(0,0,3)\). What volume does this represent?
\item
  Linear Dependence: Show that if \(v_1, v_2, v_3\) are linearly
  dependent in \(\mathbb{R}^3\), then \([v_1,v_2,v_3] = 0\).
\item
  Thought Experiment: Why does antisymmetry naturally encode the
  geometric idea of orientation? How does this link back to the sign of
  a determinant?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part IV. Exterior Algebra}\label{part-iv.-exterior-algebra}

\section{Chapter 8. Wedge Products and
k-Vectors}\label{chapter-8.-wedge-products-and-k-vectors}

\subsection{8.1 The Wedge Product and Geometric
Meaning}\label{the-wedge-product-and-geometric-meaning}

We now begin Part IV: Exterior Algebra, which formalizes antisymmetric
tensors into a powerful algebraic system. The key operation here is the
wedge product (\(\wedge\)).

\subsubsection{Definition of the Wedge
Product}\label{definition-of-the-wedge-product}

Given two vectors \(u, v \in V\), their wedge product is:

\[
u \wedge v = u \otimes v - v \otimes u.
\]

\begin{itemize}
\tightlist
\item
  It is bilinear: linear in each input.
\item
  It is antisymmetric: \(u \wedge v = -v \wedge u\).
\item
  It vanishes if \(u\) and \(v\) are linearly dependent.
\end{itemize}

\subsubsection{Geometric Meaning}\label{geometric-meaning-1}

\begin{itemize}
\item
  \(u \wedge v\) represents the oriented parallelogram spanned by \(u\)
  and \(v\).
\item
  The magnitude corresponds to the area:

  \[
  \|u \wedge v\| = \|u\| \, \|v\| \, \sin\theta,
  \]

  where \(\theta\) is the angle between \(u\) and \(v\).
\item
  The sign encodes orientation.
\end{itemize}

\subsubsection{Higher Wedge Products}\label{higher-wedge-products}

For \(k\) vectors \(u_1, u_2, \dots, u_k\):

\[
u_1 \wedge u_2 \wedge \cdots \wedge u_k
\]

represents the oriented \(k\)-dimensional volume element
(parallelepiped).

\begin{itemize}
\item
  Antisymmetry ensures the wedge vanishes if the vectors are linearly
  dependent.
\item
  Thus, wedge products capture the idea of dimension of span:

  \begin{itemize}
  \tightlist
  \item
    Two vectors → area.
  \item
    Three vectors → volume.
  \item
    More vectors → higher-dimensional volume.
  \end{itemize}
\end{itemize}

\subsubsection{Algebraic Properties}\label{algebraic-properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Anticommutativity: \(u \wedge v = -v \wedge u\).
\item
  Associativity (up to sign):
  \((u \wedge v) \wedge w = u \wedge (v \wedge w)\).
\item
  Alternating Property: \(u \wedge u = 0\).
\item
  Distributivity: \((u+v) \wedge w = u \wedge w + v \wedge w\).
\end{enumerate}

Together, these rules define the exterior algebra \(\Lambda(V)\).

\subsubsection{Why This Matters}\label{why-this-matters-16}

\begin{itemize}
\tightlist
\item
  The wedge product is the algebraic foundation of determinants, areas,
  and volumes.
\item
  It provides a coordinate-free way to work with oriented geometry.
\item
  In advanced topics, wedge products lead directly to differential forms
  and integration on manifolds.
\end{itemize}

\subsubsection{Exercises}\label{exercises-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basic Wedge: Compute \((1,0) \wedge (0,1)\) in \(\mathbb{R}^2\). What
  does it represent?
\item
  Antisymmetry Check: Show that
  \((1,2,3) \wedge (4,5,6) = - (4,5,6) \wedge (1,2,3)\).
\item
  Area via Wedge: Use wedge products to find the area of the
  parallelogram spanned by \(u=(2,0)\) and \(v=(1,3)\).
\item
  Zero Wedge: Show that \(u \wedge v = 0\) when \(u=(1,2)\),
  \(v=(2,4)\). Interpret geometrically.
\item
  Thought Experiment: Why does antisymmetry make wedge products vanish
  for linearly dependent vectors? How does this encode the idea of
  dimension reduction?
\end{enumerate}

\subsection{8.2 Areas, Volumes, Orientation,
Determinant}\label{areas-volumes-orientation-determinant}

Now that we've introduced the wedge product, we can connect it directly
to geometry: areas, volumes, and orientation. The wedge product is the
algebraic language for these concepts.

\subsubsection{2D: Areas from the Wedge
Product}\label{d-areas-from-the-wedge-product}

For vectors \(u, v \in \mathbb{R}^2\), the wedge product \(u \wedge v\)
represents the oriented area of the parallelogram they span.

\begin{itemize}
\item
  Algebraically:

  \[
  u \wedge v = \det \begin{bmatrix} u_1 & v_1 \\ u_2 & v_2 \end{bmatrix}.
  \]
\item
  Geometric meaning: \(|u \wedge v|\) is the area, the sign encodes
  clockwise vs.~counterclockwise orientation.
\end{itemize}

\subsubsection{3D: Volumes via Triple
Wedge}\label{d-volumes-via-triple-wedge}

For \(u, v, w \in \mathbb{R}^3\):

\[
u \wedge v \wedge w
\]

represents the oriented volume of the parallelepiped spanned by them.

\begin{itemize}
\item
  Algebraically:

  \[
  u \wedge v \wedge w = \det \begin{bmatrix} u_1 & v_1 & w_1 \\ u_2 & v_2 & w_2 \\ u_3 & v_3 & w_3 \end{bmatrix}.
  \]
\item
  Geometric meaning: \(|u \wedge v \wedge w|\) is the volume, the sign
  encodes right- vs.~left-handed orientation.
\end{itemize}

\subsubsection{\texorpdfstring{General \(n\)-Dimensional
Case}{General n-Dimensional Case}}\label{general-n-dimensional-case}

For \(n\) vectors \(v_1, \dots, v_n \in \mathbb{R}^n\):

\[
v_1 \wedge v_2 \wedge \cdots \wedge v_n = \det(v_1, v_2, \dots, v_n),
\]

the determinant of the matrix with those vectors as columns.

\begin{itemize}
\tightlist
\item
  Magnitude: \(n\)-dimensional volume of the parallelepiped.
\item
  Sign: orientation (positive if vectors preserve orientation, negative
  if they reverse it).
\end{itemize}

\subsubsection{Orientation}\label{orientation}

\begin{itemize}
\tightlist
\item
  Orientation is the choice of ``handedness'' (right-hand vs.~left-hand
  rule).
\item
  The wedge product encodes orientation automatically via antisymmetry.
\item
  Swapping two vectors flips the sign, meaning the orientation is
  reversed.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-17}

\begin{itemize}
\tightlist
\item
  Wedge products generalize determinants to all dimensions.
\item
  They provide a uniform way to compute areas, volumes, and
  higher-dimensional volumes.
\item
  Orientation, crucial in geometry and physics, is naturally handled
  through antisymmetry.
\end{itemize}

\subsubsection{Exercises}\label{exercises-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Area in 2D: Compute the oriented area of the parallelogram spanned by
  \(u=(1,2)\), \(v=(3,4)\) using \(u \wedge v\).
\item
  Volume in 3D: Compute \(u \wedge v \wedge w\) for
  \(u=(1,0,0), v=(0,2,0), w=(0,0,3)\). Interpret geometrically.
\item
  Orientation Flip: Show that \(u \wedge v = -v \wedge u\) changes the
  orientation of the area.
\item
  Determinant Link: Verify that \(u \wedge v \wedge w\) in
  \(\mathbb{R}^3\) equals \(\det[u,v,w]\).
\item
  Thought Experiment: Why does the wedge product vanish if the set of
  vectors is linearly dependent? What does this mean geometrically for
  areas/volumes?
\end{enumerate}

\subsection{8.3 Basis, Dimension, Grassmann
Algebra}\label{basis-dimension-grassmann-algebra}

We've seen that wedge products describe oriented areas, volumes, and
higher-dimensional analogues. To make this systematic, we construct
exterior powers of a vector space, also known as Grassmann algebras.

\subsubsection{\texorpdfstring{Exterior Powers
\(\Lambda^k(V)\)}{Exterior Powers \textbackslash Lambda\^{}k(V)}}\label{exterior-powers-lambdakv}

For a vector space \(V\):

\begin{itemize}
\tightlist
\item
  \(\Lambda^k(V)\) is the space of all antisymmetric \(k\)-tensors
  (wedge products of \(k\) vectors).
\item
  \(\Lambda^0(V) \cong \mathbb{R}\) (scalars).
\item
  \(\Lambda^1(V) \cong V\) (vectors).
\item
  \(\Lambda^2(V)\): oriented areas.
\item
  \(\Lambda^3(V)\): oriented volumes.
\item
  In general, \(\Lambda^k(V)\) encodes oriented \(k\)-dimensional
  ``volume elements.''
\end{itemize}

\subsubsection{\texorpdfstring{Basis of
\(\Lambda^k(V)\)}{Basis of \textbackslash Lambda\^{}k(V)}}\label{basis-of-lambdakv}

Suppose \(V\) has dimension \(n\) with basis \(e_1, e_2, \dots, e_n\).

\begin{itemize}
\item
  A basis for \(\Lambda^k(V)\) is given by wedge products:

  \[
  e_{i_1} \wedge e_{i_2} \wedge \cdots \wedge e_{i_k}, \quad i_1 < i_2 < \cdots < i_k.
  \]
\item
  These are linearly independent and span all of \(\Lambda^k(V)\).
\end{itemize}

\subsubsection{Dimension Formula}\label{dimension-formula-1}

The dimension of \(\Lambda^k(V)\) is:

\[
\dim \Lambda^k(V) = \binom{n}{k}.
\]

\begin{itemize}
\tightlist
\item
  Example: If \(n=4\), then \(\dim \Lambda^2(V) = \binom{4}{2} = 6\).
\item
  This matches the number of independent ways to choose \(k\) basis
  vectors from \(n\).
\end{itemize}

\subsubsection{Grassmann (Exterior)
Algebra}\label{grassmann-exterior-algebra}

The direct sum of all exterior powers:

\[
\Lambda(V) = \Lambda^0(V) \oplus \Lambda^1(V) \oplus \cdots \oplus \Lambda^n(V),
\]

is called the exterior algebra (or Grassmann algebra).

\begin{itemize}
\tightlist
\item
  Multiplication in this algebra is given by the wedge product.
\item
  Anticommutativity ensures \(u \wedge u = 0\).
\item
  This makes \(\Lambda(V)\) into a graded algebra, organized by degree.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-18}

\begin{itemize}
\tightlist
\item
  Exterior algebra gives a systematic framework for working with
  antisymmetric tensors.
\item
  The dimension formula explains why areas, volumes, etc. fit neatly
  into subspaces.
\item
  Grassmann algebra underlies modern geometry, topology, and physics
  (differential forms, integration on manifolds).
\end{itemize}

\subsubsection{Exercises}\label{exercises-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basis in \(\Lambda^2\): For \(V = \mathbb{R}^3\) with basis
  \((e_1,e_2,e_3)\), list a basis of \(\Lambda^2(V)\).
\item
  Dimension Count: If \(\dim V = 5\), compute \(\dim \Lambda^2(V)\),
  \(\dim \Lambda^3(V)\), and \(\dim \Lambda^4(V)\).
\item
  Simple Wedge Expansion: In \(\mathbb{R}^3\), expand
  \((e_1 + e_2) \wedge e_3\).
\item
  Grassmann Algebra Structure: Identify which degrees (\(k\)) of
  \(\Lambda^k(V)\) correspond to:

  \begin{itemize}
  \tightlist
  \item
    scalars,
  \item
    vectors,
  \item
    oriented areas,
  \item
    oriented volumes.
  \end{itemize}
\item
  Thought Experiment: Why does the formula
  \(\dim \Lambda^k(V) = \binom{n}{k}\) match the combinatorial idea of
  ``choosing \(k\) directions from \(n\)''?
\end{enumerate}

\subsection{8.4 Differential Forms (Gentle
Preview)}\label{differential-forms-gentle-preview}

We close Part IV with a glimpse of how wedge products extend beyond
linear algebra into calculus on manifolds. This leads to differential
forms, the language of modern geometry and physics.

\subsubsection{Differential 1-Forms}\label{differential-1-forms}

\begin{itemize}
\item
  A 1-form is a covector field: at each point, it eats a vector and
  gives a number.
\item
  Example in \(\mathbb{R}^2\):

  \[
  \omega = x \, dy - y \, dx,
  \]

  where \(dx, dy\) are basis 1-forms.
\end{itemize}

\subsubsection{Higher-Degree Forms}\label{higher-degree-forms}

\begin{itemize}
\item
  A k-form is an antisymmetric multilinear map that takes \(k\) vectors
  (directions) and outputs a scalar.
\item
  Built using wedge products of 1-forms:

  \begin{itemize}
  \tightlist
  \item
    \(dx \wedge dy\) is a 2-form (measures oriented area in the
    \(xy\)-plane).
  \item
    \(dx \wedge dy \wedge dz\) is a 3-form (measures oriented volume in
    \(\mathbb{R}^3\)).
  \end{itemize}
\end{itemize}

\subsubsection{\texorpdfstring{Exterior Derivative
\(d\)}{Exterior Derivative d}}\label{exterior-derivative-d}

Differential forms come with a derivative operator \(d\):

\begin{itemize}
\item
  Takes a \(k\)-form to a \((k+1)\)-form.
\item
  Generalizes gradient, curl, and divergence in one unified framework.
\item
  Example:

  \begin{itemize}
  \item
    For \(f(x,y)\) a scalar function (0-form):

    \[
    df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy,
    \]

    which is the gradient written as a 1-form.
  \end{itemize}
\end{itemize}

\subsubsection{Integration of Forms}\label{integration-of-forms}

\begin{itemize}
\tightlist
\item
  Differential forms can be integrated over curves, surfaces, and
  higher-dimensional manifolds.
\item
  Example: integrating a 1-form along a curve gives the work done by a
  force field.
\item
  Integrating a 2-form over a surface gives the flux through the
  surface.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-19}

\begin{itemize}
\tightlist
\item
  Differential forms unify multivariable calculus (gradient, curl,
  divergence) into one elegant theory.
\item
  They are the foundation of Stokes' theorem, which generalizes all the
  fundamental theorems of calculus.
\item
  Physics: electromagnetism, fluid dynamics, and general relativity all
  use differential forms.
\end{itemize}

\subsubsection{Exercises}\label{exercises-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  1-Form Action: Let \(\omega = 3dx + 2dy\). Evaluate \(\omega(v)\) for
  \(v = (1,4)\).
\item
  Wedge of 1-Forms: Compute \(dx \wedge dy(v,w)\) for
  \(v=(1,0), w=(0,2)\).
\item
  Exterior Derivative: For \(f(x,y) = x^2 y\), compute \(df\).
\item
  Integration Example: Interpret \(\int_\gamma (y \, dx)\) along a path
  \(\gamma\) in \(\mathbb{R}^2\). What does it represent physically?
\item
  Thought Experiment: How does the wedge product in linear algebra
  naturally lead into differential forms in calculus?
\end{enumerate}

\section{Chapter 9. Hodge Dual and Metrics (Optional but
Useful)}\label{chapter-9.-hodge-dual-and-metrics-optional-but-useful}

\subsection{9.1 Inner Products on Exterior
Powers}\label{inner-products-on-exterior-powers}

To use wedge products effectively, we need a way to measure their size
and compare them. This is where inner products on exterior powers come
in. They extend the familiar dot product on vectors to areas, volumes,
and higher-dimensional elements.

\subsubsection{Induced Inner Product}\label{induced-inner-product}

Suppose \(V\) has an inner product \(\langle \cdot, \cdot \rangle\). We
can extend it to \(\Lambda^k(V)\) by declaring wedge products of basis
vectors to be orthonormal:

\[
\langle e_{i_1} \wedge \cdots \wedge e_{i_k}, \, e_{j_1} \wedge \cdots \wedge e_{j_k} \rangle =
\begin{cases}
\det(\delta_{i_a j_b}) & \text{if sets are equal}, \\
0 & \text{otherwise}.
\end{cases}
\]

In simpler words:

\begin{itemize}
\tightlist
\item
  Take all ordered \(k\)-tuples of basis vectors.
\item
  The wedge products of these are orthonormal if the underlying sets
  match.
\end{itemize}

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^2\)}{Example in \textbackslash mathbb\{R\}\^{}2}}\label{example-in-mathbbr2}

Let \(e_1, e_2\) be an orthonormal basis. Then

\[
\langle e_1 \wedge e_2, e_1 \wedge e_2 \rangle = 1.
\]

So \(e_1 \wedge e_2\) has unit length, representing a unit square area.

\subsubsection{\texorpdfstring{Example in
\(\mathbb{R}^3\)}{Example in \textbackslash mathbb\{R\}\^{}3}}\label{example-in-mathbbr3}

Let \(e_1, e_2, e_3\) be orthonormal. Then

\begin{itemize}
\tightlist
\item
  \(e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3\) form an orthonormal
  basis of \(\Lambda^2(\mathbb{R}^3)\).
\item
  \(e_1 \wedge e_2 \wedge e_3\) is the unit element of
  \(\Lambda^3(\mathbb{R}^3)\), representing a unit cube volume.
\end{itemize}

\subsubsection{Norms of Wedge Products}\label{norms-of-wedge-products}

For \(u,v \in V\):

\[
\|u \wedge v\|^2 = \|u\|^2 \|v\|^2 - \langle u,v \rangle^2.
\]

This is the formula for the area of a parallelogram spanned by \(u,v\).

\begin{itemize}
\tightlist
\item
  If \(u,v\) are orthogonal: area = product of lengths.
\item
  If \(u,v\) are parallel: area = 0.
\end{itemize}

For higher \(k\), the squared norm of \(u_1 \wedge \cdots \wedge u_k\)
equals the determinant of the Gram matrix \([\langle u_i,u_j \rangle]\).

\subsubsection{Why This Matters}\label{why-this-matters-20}

\begin{itemize}
\tightlist
\item
  Inner products on exterior powers connect wedge products to geometry
  quantitatively (areas, volumes).
\item
  They unify length, area, and volume measurement into one consistent
  framework.
\item
  This lays the groundwork for the Hodge star operator, which depends on
  having an inner product.
\end{itemize}

\subsubsection{Exercises}\label{exercises-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Area Formula: Compute \(\|u \wedge v\|\) for \(u=(1,0), v=(1,1)\) in
  \(\mathbb{R}^2\).
\item
  Gram Determinant: Show that
  \(\|u \wedge v\|^2 = \det \begin{bmatrix} \langle u,u \rangle & \langle u,v \rangle \\ \langle v,u \rangle & \langle v,v \rangle \end{bmatrix}\).
\item
  3D Basis Check: Verify that
  \(e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3\) are orthonormal in
  \(\Lambda^2(\mathbb{R}^3)\).
\item
  Volume Norm: Compute the norm of \(e_1 \wedge e_2 \wedge (e_1+e_3)\).
  Interpret geometrically.
\item
  Thought Experiment: Why does the Gram determinant naturally appear in
  wedge product norms? How does this connect to the idea of measuring
  volume via linear independence?
\end{enumerate}

\subsection{9.2 Hodge Star, Pseudo-Vectors, Cross
Products}\label{hodge-star-pseudo-vectors-cross-products}

With an inner product defined on exterior powers, we can now introduce
the Hodge star operator (\(\star\)), which creates a bridge between
\(k\)-forms and \((n-k)\)-forms. This operator encodes geometric
duality, and in 3D it gives rise to the familiar cross product.

\subsubsection{The Hodge Star Operator}\label{the-hodge-star-operator}

Let \(V\) be an \(n\)-dimensional inner product space with an
orientation. The Hodge star is a linear map:

\[
\star : \Lambda^k(V) \to \Lambda^{n-k}(V).
\]

It is defined so that for all \(\alpha, \beta \in \Lambda^k(V)\):

\[
\alpha \wedge \star \beta = \langle \alpha, \beta \rangle \, \mathrm{vol},
\]

where \(\mathrm{vol}\) is the unit volume element in \(\Lambda^n(V)\).

\subsubsection{\texorpdfstring{Examples in
\(\mathbb{R}^3\)}{Examples in \textbackslash mathbb\{R\}\^{}3}}\label{examples-in-mathbbr3}

With orthonormal basis \(e_1, e_2, e_3\):

\begin{itemize}
\tightlist
\item
  \(\star e_1 = e_2 \wedge e_3\).
\item
  \(\star (e_1 \wedge e_2) = e_3\).
\item
  \(\star (e_1 \wedge e_2 \wedge e_3) = 1\).
\end{itemize}

So the star turns 1-vectors into 2-forms, 2-forms into 1-vectors, and
the 3-form into a scalar.

\subsubsection{Cross Product as Hodge
Dual}\label{cross-product-as-hodge-dual}

In \(\mathbb{R}^3\), the cross product \(u \times v\) can be expressed
using the Hodge star:

\[
u \times v = \star (u \wedge v).
\]

\begin{itemize}
\tightlist
\item
  \(u \wedge v\) represents the oriented area spanned by \(u,v\).
\item
  The Hodge star converts that 2-form into the unique vector
  perpendicular to the plane, with magnitude equal to the area.
\end{itemize}

Thus, the cross product is not a primitive concept - it is the Hodge
star of a wedge product.

\subsubsection{Pseudo-Vectors}\label{pseudo-vectors}

\begin{itemize}
\tightlist
\item
  Objects like angular momentum or magnetic field transform like
  vectors, but with an orientation twist.
\item
  These are pseudo-vectors, naturally arising from antisymmetric 2-forms
  in \(\mathbb{R}^3\).
\item
  The Hodge star provides the link between antisymmetric 2-forms and
  pseudo-vectors.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-21}

\begin{itemize}
\tightlist
\item
  The Hodge star encodes geometric duality between subspaces and their
  complements.
\item
  It explains the origin of the cross product in 3D and why it does not
  generalize to higher dimensions.
\item
  It sets the stage for physics applications: electromagnetism, fluid
  dynamics, and general relativity all use the Hodge dual.
\end{itemize}

\subsubsection{Exercises}\label{exercises-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basic Star: In \(\mathbb{R}^3\), compute \(\star (e_1)\),
  \(\star (e_2)\), and \(\star (e_3)\).
\item
  Star on 2-Forms: Verify that \(\star (e_1 \wedge e_2) = e_3\) in
  \(\mathbb{R}^3\).
\item
  Cross Product via Star: Compute \(u \times v\) using
  \(u \times v = \star (u \wedge v)\) for \(u=(1,0,0), v=(0,1,0)\).
\item
  Star Squared: Show that in \(\mathbb{R}^3\), applying the star twice
  gives \(\star \star \alpha = \alpha\) for 1-forms.
\item
  Thought Experiment: Why does the cross product only exist in
  \(\mathbb{R}^3\) (and in a special sense in \(\mathbb{R}^7\))? How
  does the Hodge star viewpoint clarify this?
\end{enumerate}

\subsection{9.3 Volume Forms and Integration
Glimpses}\label{volume-forms-and-integration-glimpses}

We now tie together the Hodge star, wedge products, and inner products
to introduce the concept of volume forms - the mathematical tools that
let us integrate over spaces of any dimension.

\subsubsection{Volume Form}\label{volume-form}

In an \(n\)-dimensional oriented inner product space \(V\), the volume
form is the unit \(n\)-form:

\[
\mathrm{vol} = e_1 \wedge e_2 \wedge \cdots \wedge e_n,
\]

where \(\{e_i\}\) is an orthonormal positively oriented basis.

\begin{itemize}
\item
  It represents the ``unit \(n\)-dimensional volume element.''
\item
  Any \(n\) vectors \(v_1,\dots,v_n\) give

  \[
  \mathrm{vol}(v_1,\dots,v_n) = \det[v_1 \ \cdots \ v_n],
  \]

  i.e.~their oriented volume.
\end{itemize}

\subsubsection{Volume via Hodge Star}\label{volume-via-hodge-star}

For a \(k\)-form \(\alpha\), its dual \(\star \alpha\) is an
\((n-k)\)-form.

\begin{itemize}
\tightlist
\item
  The wedge \(\alpha \wedge \star \alpha\) is proportional to
  \(\mathrm{vol}\).
\item
  This expresses how much ``volume'' \(\alpha\) contributes in
  \(n\)-dimensions.
\end{itemize}

Example in \(\mathbb{R}^3\):

\begin{itemize}
\item
  If \(\alpha = u \wedge v\), then

  \[
  \alpha \wedge \star \alpha = \|u \wedge v\|^2 \, \mathrm{vol},
  \]

  encoding the squared area of the parallelogram spanned by \(u,v\).
\end{itemize}

\subsubsection{Integration of Differential Forms
(Glimpse)}\label{integration-of-differential-forms-glimpse}

The volume form allows us to integrate over manifolds:

\begin{itemize}
\item
  In \(\mathbb{R}^n\), integrating \(\mathrm{vol}\) over a region gives
  its volume.
\item
  A \(k\)-form can be integrated over a \(k\)-dimensional surface
  (curve, area, etc.).
\item
  Example:

  \begin{itemize}
  \tightlist
  \item
    In \(\mathbb{R}^2\), \(\mathrm{vol} = dx \wedge dy\).
  \item
    The integral \(\int_\Omega dx \wedge dy\) is the area of region
    \(\Omega\).
  \end{itemize}
\end{itemize}

This is the seed of Stokes' theorem, which unifies the fundamental
theorem of calculus, Green's theorem, and the divergence theorem.

\subsubsection{Why This Matters}\label{why-this-matters-22}

\begin{itemize}
\tightlist
\item
  Volume forms make precise the link between algebra (determinants) and
  geometry (volume).
\item
  They are the foundation for integration on curved spaces (manifolds).
\item
  In physics, volume forms appear in conservation laws, flux integrals,
  and field theories.
\end{itemize}

\subsubsection{Exercises}\label{exercises-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  2D Volume Form: Show that for \(u=(1,0), v=(0,2)\) in
  \(\mathbb{R}^2\):
  \(\mathrm{vol}(u,v) = \det \begin{bmatrix}1 & 0 \\ 0 & 2\end{bmatrix}\).
\item
  3D Example: Compute \(\mathrm{vol}(u,v,w)\) for
  \(u=(1,0,0), v=(0,1,0), w=(0,0,2)\).
\item
  Star Relation: In \(\mathbb{R}^3\), verify that
  \((u \wedge v) \wedge \star (u \wedge v) = \|u \wedge v\|^2 \, \mathrm{vol}\).
\item
  Integration Preview: Evaluate
  \(\int_{[0,1]\times [0,1]} dx \wedge dy\). Interpret the result.
\item
  Thought Experiment: Why is orientation essential when defining a
  volume form? What would go wrong in integration if we ignored
  orientation?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part V. Symmetric Tensors and Polynomial
View}\label{part-v.-symmetric-tensors-and-polynomial-view}

\section{Chapter 10. Symmetric Powers and Homogeneous
Polynomials}\label{chapter-10.-symmetric-powers-and-homogeneous-polynomials}

\subsection{10.1 Symmetric Tensors as Polynomial
Coefficients}\label{symmetric-tensors-as-polynomial-coefficients}

So far, we have studied antisymmetric tensors and exterior algebra. Now
we turn to the opposite case: symmetric tensors. These are equally
important, and they connect directly to polynomials.

\subsubsection{Symmetric Tensors}\label{symmetric-tensors-1}

A symmetric tensor of order \(k\) satisfies:

\[
T_{i_1 i_2 \dots i_k} = T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}
\]

for any permutation \(\pi\).

Example:

\begin{itemize}
\tightlist
\item
  A quadratic form \(Q(x) = x^\top A x\) comes from a symmetric
  2-tensor.
\item
  Higher-order symmetric tensors generalize this to cubic, quartic, etc.
\end{itemize}

\subsubsection{From Symmetric Tensors to
Polynomials}\label{from-symmetric-tensors-to-polynomials}

Every symmetric \(k\)-tensor corresponds to a homogeneous polynomial of
degree \(k\).

\begin{itemize}
\item
  Given a symmetric tensor \(T_{i_1 \dots i_k}\):

  \[
  p(x) = T_{i_1 \dots i_k} x^{i_1} \cdots x^{i_k}.
  \]
\item
  Example: In \(\mathbb{R}^2\), let
  \(T_{11}=1, T_{12}=T_{21}=2, T_{22}=3\). Then

  \[
  p(x,y) = x^2 + 4xy + 3y^2.
  \]
\end{itemize}

This is a homogeneous polynomial of degree 2.

\subsubsection{From Polynomials to Symmetric
Tensors}\label{from-polynomials-to-symmetric-tensors}

Conversely, every homogeneous polynomial defines a symmetric tensor:

\begin{itemize}
\tightlist
\item
  Example: \(p(x,y,z) = 2x^2z + 3yz^2\).
\item
  Its coefficients directly correspond to entries of a symmetric
  3-tensor.
\end{itemize}

Thus, symmetric tensors and homogeneous polynomials are two sides of the
same coin.

\subsubsection{Why This Matters}\label{why-this-matters-23}

\begin{itemize}
\tightlist
\item
  This correspondence makes symmetric tensors central in algebraic
  geometry and statistics.
\item
  Moments of probability distributions are symmetric tensors.
\item
  Polynomial optimization problems can be rephrased in terms of
  symmetric tensors.
\end{itemize}

\subsubsection{Exercises}\label{exercises-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Quadratic Form: Show that the polynomial \(p(x,y) = 3x^2 + 2xy + y^2\)
  corresponds to a symmetric 2-tensor. Write its matrix.
\item
  Cubic Example: Write the symmetric 3-tensor for
  \(p(x,y) = x^3 + 3x^2y + 2y^3\).
\item
  Backwards: Given the symmetric tensor with entries
  \(T_{111}=1, T_{112}=2, T_{122}=3, T_{222}=4\), write the polynomial
  in two variables.
\item
  Homogeneity Check: Explain why the polynomial \(x^2 + xy + y^2\) is
  homogeneous of degree 2, but \(x^2 + y + 1\) is not.
\item
  Thought Experiment: Why might it be useful to switch between tensor
  and polynomial viewpoints in machine learning or physics? \#\#\# 10.2
  Polarization Identities
\end{enumerate}

We saw that symmetric tensors and homogeneous polynomials are two sides
of the same coin. But how exactly do we go from one to the other in a
precise way? The tool for this is the polarization identity, which
allows us to reconstruct a symmetric multilinear map (or tensor) from
its associated polynomial.

\subsubsection{From Symmetric Tensor to
Polynomial}\label{from-symmetric-tensor-to-polynomial}

If \(T\) is a symmetric \(k\)-linear form on \(V\), we define the
polynomial

\[
p(x) = T(x,x,\dots,x).
\]

This is a homogeneous polynomial of degree \(k\).

Example:

\begin{itemize}
\tightlist
\item
  If \(T(x,y) = \langle x,y \rangle\), then
  \(p(x) = \langle x,x \rangle = \|x\|^2\).
\end{itemize}

\subsubsection{From Polynomial to Symmetric
Tensor}\label{from-polynomial-to-symmetric-tensor}

Given a homogeneous polynomial \(p(x)\) of degree \(k\), we want to
recover the symmetric tensor \(T\).

The polarization identity says:

\[
T(x_1, \dots, x_k) = \frac{1}{k! \, 2^k} \sum_{\epsilon_1,\dots,\epsilon_k = \pm 1} \epsilon_1 \cdots \epsilon_k \, p(\epsilon_1 x_1 + \cdots + \epsilon_k x_k).
\]

This formula extracts the multilinear coefficients hidden inside the
polynomial.

\subsubsection{Example: Quadratic Forms}\label{example-quadratic-forms}

For \(p(x) = \|x\|^2\):

\begin{itemize}
\item
  Using polarization, we recover the bilinear form:

  \[
  T(x,y) = \tfrac{1}{2} \big( \|x+y\|^2 - \|x\|^2 - \|y\|^2 \big) = \langle x,y \rangle.
  \]
\end{itemize}

This is the classical polarization identity for inner products.

\subsubsection{Why This Matters}\label{why-this-matters-24}

\begin{itemize}
\item
  The polarization identity ensures that the correspondence between
  symmetric tensors and homogeneous polynomials is exact and invertible.
\item
  It provides a way to compute multilinear structure from polynomial
  data.
\item
  In applications:

  \begin{itemize}
  \tightlist
  \item
    In physics, energy polynomials give rise to symmetric stress
    tensors.
  \item
    In statistics, cumulants and moments correspond to symmetric tensors
    recovered from generating functions.
  \end{itemize}
\end{itemize}

\subsubsection{Exercises}\label{exercises-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inner Product Recovery: Use the quadratic polarization identity to
  show that

  \[
  \langle x,y \rangle = \tfrac{1}{4} \big( \|x+y\|^2 - \|x-y\|^2 \big).
  \]
\item
  Cubic Case: Let \(p(x) = x_1^3\) in \(\mathbb{R}^2\). Use the
  polarization formula to compute \(T(e_1,e_1,e_1)\) and
  \(T(e_1,e_1,e_2)\).
\item
  Verification: For \(p(x,y) = x^2 + y^2\), verify that the polarization
  identity recovers the symmetric bilinear form
  \(T(x,y) = x_1y_1 + x_2y_2\).
\item
  Homogeneity Check: Show why polarization fails if \(p(x)\) is not
  homogeneous.
\item
  Thought Experiment: Why is it important that the tensor be symmetric
  in order for the polynomial ↔ tensor correspondence to work smoothly?
\end{enumerate}

\subsection{10.2 Polarization Identities}\label{polarization-identities}

We saw that symmetric tensors and homogeneous polynomials are two sides
of the same coin. But how exactly do we go from one to the other in a
precise way? The tool for this is the polarization identity, which
allows us to reconstruct a symmetric multilinear map (or tensor) from
its associated polynomial.

\subsubsection{From Symmetric Tensor to
Polynomial}\label{from-symmetric-tensor-to-polynomial-1}

If \(T\) is a symmetric \(k\)-linear form on \(V\), we define the
polynomial

\[
p(x) = T(x,x,\dots,x).
\]

This is a homogeneous polynomial of degree \(k\).

Example:

\begin{itemize}
\tightlist
\item
  If \(T(x,y) = \langle x,y \rangle\), then
  \(p(x) = \langle x,x \rangle = \|x\|^2\).
\end{itemize}

\subsubsection{From Polynomial to Symmetric
Tensor}\label{from-polynomial-to-symmetric-tensor-1}

Given a homogeneous polynomial \(p(x)\) of degree \(k\), we want to
recover the symmetric tensor \(T\).

The polarization identity says:

\[
T(x_1, \dots, x_k) = \frac{1}{k! \, 2^k} \sum_{\epsilon_1,\dots,\epsilon_k = \pm 1} \epsilon_1 \cdots \epsilon_k \, p(\epsilon_1 x_1 + \cdots + \epsilon_k x_k).
\]

This formula extracts the multilinear coefficients hidden inside the
polynomial.

\subsubsection{Example: Quadratic
Forms}\label{example-quadratic-forms-1}

For \(p(x) = \|x\|^2\):

\begin{itemize}
\item
  Using polarization, we recover the bilinear form:

  \[
  T(x,y) = \tfrac{1}{2} \big( \|x+y\|^2 - \|x\|^2 - \|y\|^2 \big) = \langle x,y \rangle.
  \]
\end{itemize}

This is the classical polarization identity for inner products.

\subsubsection{Why This Matters}\label{why-this-matters-25}

\begin{itemize}
\item
  The polarization identity ensures that the correspondence between
  symmetric tensors and homogeneous polynomials is exact and invertible.
\item
  It provides a way to compute multilinear structure from polynomial
  data.
\item
  In applications:

  \begin{itemize}
  \tightlist
  \item
    In physics, energy polynomials give rise to symmetric stress
    tensors.
  \item
    In statistics, cumulants and moments correspond to symmetric tensors
    recovered from generating functions.
  \end{itemize}
\end{itemize}

\subsubsection{Exercises}\label{exercises-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inner Product Recovery: Use the quadratic polarization identity to
  show that

  \[
  \langle x,y \rangle = \tfrac{1}{4} \big( \|x+y\|^2 - \|x-y\|^2 \big).
  \]
\item
  Cubic Case: Let \(p(x) = x_1^3\) in \(\mathbb{R}^2\). Use the
  polarization formula to compute \(T(e_1,e_1,e_1)\) and
  \(T(e_1,e_1,e_2)\).
\item
  Verification: For \(p(x,y) = x^2 + y^2\), verify that the polarization
  identity recovers the symmetric bilinear form
  \(T(x,y) = x_1y_1 + x_2y_2\).
\item
  Homogeneity Check: Show why polarization fails if \(p(x)\) is not
  homogeneous.
\item
  Thought Experiment: Why is it important that the tensor be symmetric
  in order for the polynomial ↔ tensor correspondence to work smoothly?
\end{enumerate}

\subsection{10.3 Moments and Cumulants}\label{moments-and-cumulants}

Symmetric tensors are not just abstract objects - they naturally appear
in probability and statistics. Two key concepts, moments and cumulants,
can be represented as symmetric tensors, providing a multilinear view of
random variables and their dependencies.

\subsubsection{Moments as Symmetric
Tensors}\label{moments-as-symmetric-tensors}

For a random vector \(X \in \mathbb{R}^n\), the \(k\)-th moment tensor
is:

\[
M^{(k)} = \mathbb{E}[X^{\otimes k}],
\]

where

\[
X^{\otimes k} = X \otimes X \otimes \cdots \otimes X \quad (k \text{ times}).
\]

\begin{itemize}
\item
  Entries:

  \[
  (M^{(k)})_{i_1 i_2 \dots i_k} = \mathbb{E}[X_{i_1} X_{i_2} \cdots X_{i_k}].
  \]
\item
  Since multiplication is commutative, \(M^{(k)}\) is a symmetric
  tensor.
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  \(M^{(1)}\) = mean vector.
\item
  \(M^{(2)}\) = second moment matrix (related to covariance).
\item
  Higher-order moments capture skewness, kurtosis, etc.
\end{itemize}

\subsubsection{Cumulants as Symmetric
Tensors}\label{cumulants-as-symmetric-tensors}

Cumulants refine moments by removing redundancy:

\begin{itemize}
\item
  Defined via the log of the moment-generating function.
\item
  The \(k\)-th cumulant tensor \(C^{(k)}\) is also symmetric.
\item
  Example:

  \begin{itemize}
  \tightlist
  \item
    \(C^{(1)}\) = mean.
  \item
    \(C^{(2)}\) = covariance matrix.
  \item
    \(C^{(3)}\) = skewness tensor.
  \item
    \(C^{(4)}\) = kurtosis tensor.
  \end{itemize}
\end{itemize}

\subsubsection{Why Tensors?}\label{why-tensors}

\begin{itemize}
\item
  Moments and cumulants encode multi-way dependencies between variables.
\item
  As tensors, they can be studied with linear algebra and
  decompositions.
\item
  Applications:

  \begin{itemize}
  \tightlist
  \item
    Signal processing: blind source separation via cumulant tensors.
  \item
    Statistics: identifying distributions via moment tensors.
  \item
    Machine learning: feature extraction from higher-order statistics.
  \end{itemize}
\end{itemize}

\subsubsection{Connection to Symmetric
Polynomials}\label{connection-to-symmetric-polynomials}

\begin{itemize}
\tightlist
\item
  Moments correspond to coefficients of homogeneous polynomials (moment
  generating functions).
\item
  Polarization identities let us move between the polynomial
  representation and the tensor form.
\end{itemize}

\subsubsection{Exercises}\label{exercises-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Second Moment: Let \(X = (X_1,X_2)\) with
  \(\mathbb{E}[X_1^2]=2, \mathbb{E}[X_1X_2]=1, \mathbb{E}[X_2^2]=3\).
  Write the 2nd moment tensor \(M^{(2)}\).
\item
  Symmetry Check: Show that
  \((M^{(3)})_{ijk} = \mathbb{E}[X_i X_j X_k]\) is symmetric in all
  indices.
\item
  Covariance as Cumulant: Explain why the covariance matrix is the 2nd
  cumulant tensor \(C^{(2)}\).
\item
  Higher Cumulants: What does \(C^{(3)}\) measure that is not captured
  by \(C^{(2)}\)?
\item
  Thought Experiment: Why might cumulants be more useful than moments in
  separating independent signals or detecting non-Gaussianity?
\end{enumerate}

\subsection{10.4 Low-Rank Symmetric
Decompositions}\label{low-rank-symmetric-decompositions}

Symmetric tensors, like general tensors, can be decomposed into simpler
parts. In the symmetric case, this often means writing a symmetric
tensor as a sum of outer products of vectors with themselves. These
low-rank symmetric decompositions play a central role in data science,
signal processing, and machine learning.

\subsubsection{Symmetric Rank}\label{symmetric-rank}

For a symmetric tensor \(T \in \mathrm{Sym}^k(V)\), the symmetric rank
is the smallest \(r\) such that

\[
T = \sum_{i=1}^r \lambda_i \, v_i^{\otimes k},
\]

where \(v_i \in V\) and \(\lambda_i \in \mathbb{R}\).

\begin{itemize}
\tightlist
\item
  Each term
  \(v_i^{\otimes k} = v_i \otimes v_i \otimes \cdots \otimes v_i\) (k
  times) is a simple symmetric tensor.
\item
  This is the symmetric analogue of the rank-one decomposition for
  matrices.
\end{itemize}

\subsubsection{Example: Quadratic
Forms}\label{example-quadratic-forms-2}

A quadratic form (symmetric 2-tensor) can be decomposed as:

\[
Q(x) = \sum_{i=1}^r \lambda_i (v_i^\top x)^2.
\]

This is the spectral decomposition of a symmetric matrix.

\subsubsection{Higher-Order Case}\label{higher-order-case}

For cubic or quartic symmetric tensors:

\begin{itemize}
\item
  Example in 3rd order:

  \[
  T(x,y,z) = \sum_{i=1}^r \lambda_i (v_i^\top x)(v_i^\top y)(v_i^\top z).
  \]
\item
  This is the CP decomposition restricted to the symmetric case.
\end{itemize}

\subsubsection{Applications}\label{applications}

\begin{itemize}
\tightlist
\item
  Statistics: cumulant tensors often admit low-rank symmetric
  decompositions, useful in blind source separation.
\item
  Machine learning: compressing polynomial kernels, tensor regression,
  deep learning model compression.
\item
  Algebraic geometry: the study of Waring decompositions of polynomials.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-26}

\begin{itemize}
\tightlist
\item
  Low-rank symmetric decompositions reduce complexity, making huge
  tensors manageable.
\item
  They connect tensor algebra with classical spectral theory and
  polynomial factorization.
\item
  They explain why symmetric tensors are a natural language for
  representing data with latent low-dimensional structure.
\end{itemize}

\subsubsection{Exercises}\label{exercises-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Case: Decompose

  \[
  A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}
  \]

  as a sum of symmetric rank-one matrices.
\item
  Simple Symmetric Tensor: Write explicitly \(v^{\otimes 3}\) for
  \(v=(1,2)\).
\item
  Symmetric Decomposition: Express the quadratic form
  \(Q(x,y) = 4x^2 + 2xy + y^2\) as a sum of rank-one terms
  \(\lambda_i (a_i x + b_i y)^2\).
\item
  Interpretation: For a 3rd-order symmetric tensor in \(\mathbb{R}^2\),
  how many coefficients does it have? Compare this with how many are
  needed to specify a rank-1 symmetric decomposition.
\item
  Thought Experiment: Why might low-rank symmetric decompositions be
  especially valuable in machine learning (hint: think compression and
  interpretability)?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part VI. Linear Maps Between Tensor
Spaces}\label{part-vi.-linear-maps-between-tensor-spaces}

\section{Chapter 11. Reshaping, Vectorization, and
Commutation}\label{chapter-11.-reshaping-vectorization-and-commutation}

\subsection{11.1 Mode-n Unfolding and
Matricization}\label{mode-n-unfolding-and-matricization}

When working with higher-order tensors, it is often useful to
``flatten'' them into matrices so that standard linear algebra tools can
be applied. This process is called unfolding or matricization.

\subsubsection{Definition: Mode-n
Unfolding}\label{definition-mode-n-unfolding}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\), the
mode-n unfolding arranges all entries into a matrix where:

\begin{itemize}
\tightlist
\item
  The row index corresponds to the \(n\)-th mode.
\item
  The column index corresponds to all other modes combined.
\end{itemize}

Formally:

\[
T_{i_1 i_2 \dots i_N} \quad \mapsto \quad T^{(n)}_{i_n, j},
\]

where \(j\) encodes the multi-index
\((i_1,\dots,i_{n-1},i_{n+1},\dots,i_N)\).

\subsubsection{Example (3rd-Order
Tensor)}\label{example-3rd-order-tensor}

If \(T\) has shape \((I_1,I_2,I_3)\):

\begin{itemize}
\tightlist
\item
  Mode-1 unfolding: size \(I_1 \times (I_2 I_3)\).
\item
  Mode-2 unfolding: size \(I_2 \times (I_1 I_3)\).
\item
  Mode-3 unfolding: size \(I_3 \times (I_1 I_2)\).
\end{itemize}

This reshaping preserves all entries, only changing their layout.

\subsubsection{Why Unfold?}\label{why-unfold}

\begin{itemize}
\tightlist
\item
  Brings tensors into a familiar matrix framework.
\item
  Enables the use of matrix decompositions (SVD, QR, eigenvalue
  methods).
\item
  Used in algorithms for tensor decompositions (Tucker, CP, HOSVD).
\item
  Useful in machine learning for feature extraction, low-rank
  approximations, and compression.
\end{itemize}

\subsubsection{Connection to Linear
Maps}\label{connection-to-linear-maps}

Mode-\(n\) unfolding allows us to represent how a linear operator acts
on the \(n\)-th mode of a tensor.

\begin{itemize}
\item
  For example, applying a matrix \(A \in \mathbb{R}^{J \times I_n}\) to
  mode-\(n\) corresponds to left-multiplying the unfolded tensor:

  \[
  (T \times_n A)^{(n)} = A \, T^{(n)}.
  \]
\end{itemize}

This makes tensor-matrix products consistent with matrix multiplication.

\subsubsection{Why This Matters}\label{why-this-matters-27}

\begin{itemize}
\tightlist
\item
  Mode-n unfolding bridges the gap between multilinear algebra and
  classical linear algebra.
\item
  It provides the foundation for defining multilinear rank and tensor
  decompositions.
\item
  Without unfolding, many computational algorithms for tensors would be
  infeasible.
\end{itemize}

\subsubsection{Exercises}\label{exercises-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Shape Check: If \(T\) has dimensions \((2,3,4)\), what are the shapes
  of its mode-1, mode-2, and mode-3 unfoldings?
\item
  Matrix Form: Write out explicitly the mode-1 unfolding of a tensor
  \(T \in \mathbb{R}^{2 \times 2 \times 2}\).
\item
  Operator Action: Let \(T \in \mathbb{R}^{2 \times 3 \times 4}\). If
  \(A \in \mathbb{R}^{5 \times 2}\), what is the shape of
  \((T \times_1 A)\)?
\item
  Reversibility: Explain why unfolding is reversible (i.e., why no
  information is lost).
\item
  Thought Experiment: Why is it advantageous to analyze tensors via
  unfoldings rather than directly in their multi-index form?
\end{enumerate}

\subsection{11.2 Vec Operator and Kronecker
Identities}\label{vec-operator-and-kronecker-identities}

Once a tensor has been unfolded into a matrix, a common next step is to
``flatten'' that matrix into a vector. This operation is called
vectorization, or the vec operator. It provides a bridge between
multilinear algebra and standard linear algebra identities involving
Kronecker products.

\subsubsection{The Vec Operator}\label{the-vec-operator}

For a matrix \(A \in \mathbb{R}^{m \times n}\):

\[
\mathrm{vec}(A) \in \mathbb{R}^{mn}
\]

is obtained by stacking the columns of \(A\) into a single vector.

Example:

\[
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, \quad
\mathrm{vec}(A) = \begin{bmatrix} a \\ c \\ b \\ d \end{bmatrix}.
\]

For higher-order tensors, we apply vec after unfolding into a matrix.

\subsubsection{Key Identity (Matrix
Multiplication)}\label{key-identity-matrix-multiplication}

For matrices \(A, X, B\) of compatible sizes:

\[
\mathrm{vec}(AXB^\top) = (B \otimes A)\, \mathrm{vec}(X).
\]

\begin{itemize}
\tightlist
\item
  Here, \(\otimes\) is the Kronecker product.
\item
  This identity rewrites a two-sided matrix multiplication as a single
  linear transformation.
\end{itemize}

\subsubsection{Tensor Extension}\label{tensor-extension}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\), linear
maps applied along each mode can be expressed in vec form using
Kronecker products.

If \(A_n \in \mathbb{R}^{J_n \times I_n}\), then

\[
(T \times_1 A_1 \times_2 A_2 \cdots \times_N A_N)^{\mathrm{vec}} = (A_N \otimes \cdots \otimes A_2 \otimes A_1) \, \mathrm{vec}(T).
\]

This is the multilinear analogue of the matrix vec identity.

\subsubsection{Applications}\label{applications-1}

\begin{itemize}
\tightlist
\item
  Efficient coding of multilinear transformations.
\item
  Proofs of tensor decomposition algorithms.
\item
  Numerical linear algebra (solving tensor equations).
\item
  Machine learning: efficient representation of Kronecker-structured
  layers.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-28}

\begin{itemize}
\tightlist
\item
  The vec operator turns multilinear problems into linear ones.
\item
  Kronecker products capture structure in large transformations
  compactly.
\item
  These tools make computation with tensors more systematic and
  efficient.
\end{itemize}

\subsubsection{Exercises}\label{exercises-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vec of a Matrix: Compute
  \(\mathrm{vec}\left(\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}\right)\).
\item
  Matrix Identity Check: Let
  \(A = \begin{bmatrix}1 & 0 \\ 0 & 2\end{bmatrix}, X = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}, B = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\).
  Verify that
  \(\mathrm{vec}(AXB^\top) = (B \otimes A) \, \mathrm{vec}(X)\).
\item
  Tensor Shape: If \(T \in \mathbb{R}^{2 \times 3 \times 4}\), what is
  the dimension of \(\mathrm{vec}(T)\)?
\item
  Kronecker Ordering: Explain why the order of factors in
  \((A_N \otimes \cdots \otimes A_1)\) matters.
\item
  Thought Experiment: Why does vectorization make multilinear algebra
  easier to connect with existing linear algebra tools?
\end{enumerate}

\subsection{11.3 Linear Operators Acting on
Tensors}\label{linear-operators-acting-on-tensors}

So far, we have unfolded tensors and vectorized them to connect with
matrix operations. Now we look at tensors as natural domains and
codomains for linear operators. This viewpoint is powerful because many
tensor operations are simply linear maps on tensor product spaces.

\subsubsection{Operators on Tensor Product
Spaces}\label{operators-on-tensor-product-spaces}

If \(A: V \to V'\) and \(B: W \to W'\) are linear maps, then their
tensor product operator

\[
A \otimes B : V \otimes W \to V' \otimes W'
\]

is defined by

\[
(A \otimes B)(v \otimes w) = (Av) \otimes (Bw).
\]

This definition extends linearly to all of \(V \otimes W\).

\subsubsection{Example with Matrices}\label{example-with-matrices}

If \(A \in \mathbb{R}^{m \times n}\), \(B \in \mathbb{R}^{p \times q}\),
and \(X \in \mathbb{R}^{n \times q}\):

\[
(A \otimes B)\,\mathrm{vec}(X) = \mathrm{vec}(BXA^\top).
\]

This links tensor product operators to Kronecker product identities.

\subsubsection{Mode-n Multiplication as Operator
Action}\label{mode-n-multiplication-as-operator-action}

For a tensor \(T \in \mathbb{R}^{I_1 \times \cdots \times I_N}\),
multiplying along the \(n\)-th mode by a matrix
\(A \in \mathbb{R}^{J \times I_n}\):

\[
T' = T \times_n A,
\]

is equivalent to applying the operator
\(I \otimes \cdots \otimes A \otimes \cdots \otimes I\) to
\(\mathrm{vec}(T)\).

Thus, tensor--matrix products are just specialized cases of linear
operators on tensor spaces.

\subsubsection{Higher-Order Operators}\label{higher-order-operators}

More generally, a linear operator can act on multiple modes
simultaneously:

\[
T' = T \times_1 A_1 \times_2 A_2 \cdots \times_N A_N.
\]

This corresponds to the operator

\[
A_N \otimes \cdots \otimes A_2 \otimes A_1
\]

on the vectorized tensor.

\subsubsection{Why This Matters}\label{why-this-matters-29}

\begin{itemize}
\tightlist
\item
  Many ``complicated'' tensor operations are just linear maps in
  disguise.
\item
  Operator language clarifies the connection between abstract
  multilinear algebra and concrete matrix computations.
\item
  This sets the stage for defining tensor rank, decompositions, and
  algorithms.
\end{itemize}

\subsubsection{Exercises}\label{exercises-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Operator on Product: Let
  \(A = \begin{bmatrix}1 & 0 \\ 0 & 2\end{bmatrix}\),
  \(B = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\). Compute
  \((A \otimes B)(e_1 \otimes e_2)\).
\item
  Mode-1 Action: For \(T \in \mathbb{R}^{2 \times 3}\), let
  \(A = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}\). Write explicitly
  how \(T \times_1 A\) transforms rows.
\item
  Vec Form: Verify that
  \((A \otimes B)\,\mathrm{vec}(X) = \mathrm{vec}(BXA^\top)\) for
  \(A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}, B = I\).
\item
  Composed Operators: Show that
  \((A_1 \otimes B_1)(A_2 \otimes B_2) = (A_1A_2) \otimes (B_1B_2)\).
\item
  Thought Experiment: Why is it useful to think of tensor--matrix
  multiplication as applying a linear operator on a tensor space,
  instead of just as array reshaping?
\end{enumerate}

\section{Chapter 12. Metrics, Forms, and Raising/Lowering
Indices}\label{chapter-12.-metrics-forms-and-raisinglowering-indices}

\subsection{12.1 Using Inner Products to Move
Indices}\label{using-inner-products-to-move-indices}

In tensor calculus, indices can appear as upper (contravariant) or lower
(covariant). Inner products give us a way to convert between them. This
process is called raising and lowering indices.

\subsubsection{Covariant vs.~Contravariant
Indices}\label{covariant-vs.-contravariant-indices-1}

\begin{itemize}
\tightlist
\item
  A vector \(v\) has components \(v^i\) (upper index).
\item
  A covector (linear functional) \(\omega\) has components \(\omega_i\)
  (lower index).
\item
  A tensor may mix both kinds, e.g.~\(T^{i}{}_{j}\).
\end{itemize}

Without an inner product, upper and lower indices live in different
spaces (\(V\) and \(V^*\)).

\subsubsection{Using the Metric Tensor}\label{using-the-metric-tensor}

An inner product (or metric) \(g\) provides a natural way to link
vectors and covectors.

\begin{itemize}
\tightlist
\item
  Metric tensor: \(g_{ij} = \langle e_i, e_j \rangle\).
\item
  Inverse metric: \(g^{ij}\) satisfies \(g^{ij} g_{jk} = \delta^i_k\).
\end{itemize}

\subsubsection{Lowering an Index}\label{lowering-an-index}

Given a vector \(v^i\), define its covector by:

\[
v_i = g_{ij} v^j.
\]

This maps \(V \to V^*\).

\subsubsection{Raising an Index}\label{raising-an-index}

Given a covector \(\omega_i\), define its vector by:

\[
\omega^i = g^{ij} \omega_j.
\]

This maps \(V^- \to V\).

\subsubsection{\texorpdfstring{Example in \(\mathbb{R}^2\) with
Euclidean
Metric}{Example in \textbackslash mathbb\{R\}\^{}2 with Euclidean Metric}}\label{example-in-mathbbr2-with-euclidean-metric}

Let \(g = I\) (the identity matrix). Then raising and lowering indices
does nothing:

\begin{itemize}
\tightlist
\item
  \(v_i = v^i\).
\item
  \(\omega^i = \omega_i\).
\end{itemize}

But in a non-Euclidean metric (e.g., relativity), raising/lowering
changes components significantly.

\subsubsection{Why This Matters}\label{why-this-matters-30}

\begin{itemize}
\tightlist
\item
  Index manipulation allows us to move seamlessly between vector and
  covector viewpoints.
\item
  In geometry and physics, metrics define lengths, angles, and duality.
\item
  In relativity, raising and lowering indices with the Minkowski metric
  distinguishes between time and space components.
\end{itemize}

\subsubsection{Exercises}\label{exercises-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Lowering Indices: In \(\mathbb{R}^2\) with metric

  \[
  g = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix},
  \]

  compute the covector \(v_i\) for \(v^j = (1,4)\).
\item
  Raising Indices: Using the same metric, compute \(\omega^i\) from
  \(\omega_j = (2,6)\).
\item
  Check Consistency: Verify that raising and then lowering an index
  returns the original vector.
\item
  Physics Example: In special relativity, the Minkowski metric is

  \[
  g = \mathrm{diag}(-1,1,1,1).
  \]

  Show how lowering the time component of a 4-vector changes its sign.
\item
  Thought Experiment: Why is an inner product (metric) essential for
  connecting vectors and covectors? What would break if we tried to
  raise/lower indices without one? \#\#\# 12.2 Dualizations and Adjoint
  Operations
\end{enumerate}

Raising and lowering indices with a metric doesn't just apply to vectors
and covectors - it also extends naturally to linear maps and tensors.
This leads to the notions of duals and adjoints, which are central in
multilinear algebra, physics, and functional analysis.

\subsubsection{Dual of a Linear Map}\label{dual-of-a-linear-map}

Given a linear map \(A: V \to W\), its dual map is

\[
A^*: W^- \to V^*
\]

defined by

\[
(A^- \omega)(v) = \omega(Av),
\]

for all \(v \in V, \, \omega \in W^*\).

\begin{itemize}
\tightlist
\item
  In matrix form: If \(A\) has matrix \(M\), then \(A^*\) has matrix
  \(M^\top\).
\end{itemize}

\subsubsection{Adjoint of a Linear Map}\label{adjoint-of-a-linear-map-1}

If \(V\) and \(W\) have inner products, we can define the adjoint
\(A^\dagger: W \to V\) by the property:

\[
\langle Av, w \rangle_W = \langle v, A^\dagger w \rangle_V.
\]

\begin{itemize}
\item
  In Euclidean space with the standard dot product,
  \(A^\dagger = A^\top\).
\item
  With a general metric tensor \(g\), the adjoint depends on
  raising/lowering indices:

  \[
  (A^\dagger)^i{}_j = g^{ik} A^l{}_k g_{lj}.
  \]
\end{itemize}

\subsubsection{Extension to Tensors}\label{extension-to-tensors}

\begin{itemize}
\item
  For tensors with mixed indices, dualization flips upper ↔ lower
  indices.
\item
  Example: For \(T^i{}_j\), the dual acts as

  \[
  (T^*)_i{}^j = g_{ik} g^{jl} T^k{}_l.
  \]
\end{itemize}

This systematically moves indices while preserving linear relationships.

\subsubsection{Why This Matters}\label{why-this-matters-31}

\begin{itemize}
\tightlist
\item
  Dualization is purely algebraic (map between spaces and their duals).
\item
  Adjoint operations involve the metric and carry geometric meaning
  (orthogonality, length).
\item
  In physics, adjoints appear in energy conservation laws, quantum
  mechanics (Hermitian operators), and relativity.
\end{itemize}

\subsubsection{Exercises}\label{exercises-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dual Map: Let \(A = \begin{bmatrix}1 & 2 \\ 0 & 3\end{bmatrix}\).
  Compute its dual \(A^*\).
\item
  Adjoint in Euclidean Space: Show that in \(\mathbb{R}^2\) with the
  standard dot product, the adjoint of \(A\) is just \(A^\top\).
\item
  Adjoint with Metric: In \(\mathbb{R}^2\) with metric

  \[
  g = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix},
  \]

  compute the adjoint of
  \(A = \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\).
\item
  Tensor Dualization: For a tensor \(T^i{}_j = \delta^i_j\) (the
  identity), compute its dual under the Euclidean metric.
\item
  Thought Experiment: Why does the adjoint depend on the choice of inner
  product, while the dual does not? What does this tell us about algebra
  vs.~geometry? \#\#\# 12.3 Coordinate Rules Made Simple
\end{enumerate}

So far, we've seen raising/lowering of indices, dual maps, and adjoints.
These can look abstract, but in practice they reduce to straightforward
coordinate rules once a basis and metric are fixed. This section
summarizes the ``index gymnastics'' in a beginner-friendly way.

\subsubsection{Raising and Lowering}\label{raising-and-lowering}

\begin{itemize}
\item
  Lowering:

  \[
  v_i = g_{ij} v^j
  \]
\item
  Raising:

  \[
  \omega^i = g^{ij} \omega_j
  \]
\end{itemize}

Here \(g_{ij}\) is the metric, \(g^{ij}\) its inverse.

Tip: Think of lowering as ``applying \(g\)'' and raising as ``applying
\(g^{-1}\).''

\subsubsection{Dual Maps}\label{dual-maps}

For a matrix \(A\) representing a linear map:

\[
(A^*)_{ij} = A_{ji}.
\]

So the dual corresponds to a transpose in coordinates.

\subsubsection{Adjoint Maps}\label{adjoint-maps}

With a general metric \(g\), the adjoint of a matrix \(A\) is:

\[
A^\dagger = g^{-1} A^\top g.
\]

\begin{itemize}
\tightlist
\item
  If \(g = I\) (Euclidean metric), then \(A^\dagger = A^\top\).
\item
  This formula generalizes to any inner product space.
\end{itemize}

\subsubsection{Tensors with Mixed
Indices}\label{tensors-with-mixed-indices}

For a tensor \(T^{i}{}_j\), indices can be moved by contracting with
\(g_{ij}\) or \(g^{ij}\):

\begin{itemize}
\item
  To lower an upper index:

  \[
  T_{ij} = g_{ik} T^k{}_j.
  \]
\item
  To raise a lower index:

  \[
  T^{ij} = g^{jk} T^i{}_k.
  \]
\end{itemize}

This bookkeeping ensures correct transformations under basis changes.

\subsubsection{Why This Matters}\label{why-this-matters-32}

\begin{itemize}
\tightlist
\item
  These coordinate rules are the practical toolkit for working with
  metrics and adjoints.
\item
  In relativity, this is exactly how time and space components are
  manipulated.
\item
  In machine learning and physics, these rules underlie how gradients,
  covariances, and bilinear forms are expressed.
\end{itemize}

\subsubsection{Exercises}\label{exercises-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Lower an Index: In \(\mathbb{R}^2\) with metric
  \(g = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}\), compute \(v_i\)
  for \(v^j = (1,2)\).
\item
  Raise an Index: With the same metric, compute \(\omega^i\) for
  \(\omega_j = (4,6)\).
\item
  Adjoint Check: For \(A = \begin{bmatrix}1 & 2 \\ 0 & 3\end{bmatrix}\),
  compute \(A^\dagger\) under \(g = I\).
\item
  Mixed Tensor: Given
  \(T^i{}_j = \begin{bmatrix}1 & 0 \\ 2 & 3\end{bmatrix}\), compute
  \(T_{ij}\) using \(g = I\).
\item
  Thought Experiment: Why do physicists insist on keeping track of upper
  vs.~lower indices, while engineers often ignore the distinction in
  Euclidean spaces?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part VII. Tensor Ranks and
Decompositions}\label{part-vii.-tensor-ranks-and-decompositions}

\section{Chapter 13. Ranks for
Tensors}\label{chapter-13.-ranks-for-tensors}

\subsection{13.1 Matrix Rank vs.~Tensor
Rank}\label{matrix-rank-vs.-tensor-rank}

Before diving into advanced tensor decompositions, we need to understand
what rank means for tensors. Unlike matrices, where rank is simple and
unique, tensors have several different notions of rank. This section
starts with the familiar matrix rank and then introduces tensor rank.

\subsubsection{Matrix Rank (Review)}\label{matrix-rank-review}

For a matrix \(A \in \mathbb{R}^{m \times n}\):

\begin{itemize}
\item
  The rank is the dimension of its column space (or row space).
\item
  Equivalently, the smallest \(r\) such that

  \[
  A = \sum_{i=1}^r u_i v_i^\top,
  \]

  where \(u_i \in \mathbb{R}^m, v_i \in \mathbb{R}^n\).
\item
  Each term is a rank-one matrix.
\end{itemize}

Example:

\[
\begin{bmatrix}1 & 2 \\ 2 & 4\end{bmatrix}
= \begin{bmatrix}1 \\ 2\end{bmatrix} \begin{bmatrix}1 & 2\end{bmatrix},
\]

so its rank is 1.

\subsubsection{Tensor Rank}\label{tensor-rank}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\), the
tensor rank (sometimes called CP rank) is the smallest \(r\) such that

\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]

where each \(u^{(k)}_i \in \mathbb{R}^{I_k}\).

\begin{itemize}
\tightlist
\item
  Each term is a rank-one tensor (outer product of vectors).
\item
  Rank measures how many simple pieces are needed to build the tensor.
\end{itemize}

\subsubsection{Key Differences: Matrix
vs.~Tensor}\label{key-differences-matrix-vs.-tensor}

\begin{itemize}
\tightlist
\item
  Uniqueness: Matrix rank is uniquely defined; tensor rank can vary
  depending on the notion (CP rank, Tucker rank, etc.).
\item
  Computation: Matrix rank is easy to compute (via SVD); tensor rank is
  NP-hard to compute in general.
\item
  Upper Bound: For a matrix \(m \times n\), rank ≤ min(m,n). For a
  tensor, the maximum possible rank is often not obvious.
\end{itemize}

\subsubsection{Example: 3-Way Tensor}\label{example-3-way-tensor}

Let

\[
T_{ijk} = 1 \quad \text{if } i=j=k, \quad 0 \text{ otherwise}.
\]

This ``diagonal'' tensor has tensor rank = dimension \(n\).

\subsubsection{Why This Matters}\label{why-this-matters-33}

\begin{itemize}
\tightlist
\item
  Tensor rank generalizes the concept of complexity from matrices to
  higher-order data.
\item
  Low-rank structure is crucial in applications: compression, latent
  factor models, signal separation.
\item
  Understanding tensor rank lays the foundation for CP, Tucker, and TT
  decompositions.
\end{itemize}

\subsubsection{Exercises}\label{exercises-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Rank: Compute the rank of

  \[
  A = \begin{bmatrix}1 & 1 \\ 2 & 2\end{bmatrix}.
  \]
\item
  Tensor Rank-1: Show that the tensor \(T_{ijk} = a_i b_j c_k\) has rank
  1.
\item
  Decomposition Practice: Express

  \[
  T_{ij} = \begin{bmatrix}1 & 2 \\ 3 & 6\end{bmatrix}
  \]

  as a sum of rank-one matrices.
\item
  Diagonal Tensor: For \(T_{ijk}\) with \(T_{111}=1, T_{222}=1\), all
  others 0, determine its rank.
\item
  Thought Experiment: Why might tensor rank be harder to compute and
  less well-behaved than matrix rank?
\end{enumerate}

\subsection{13.2 Multilinear (Tucker) Rank and Mode
Ranks}\label{multilinear-tucker-rank-and-mode-ranks}

Besides CP rank, tensors have another important notion of rank: the
multilinear rank, also called the Tucker rank. This definition is based
on the ranks of mode-\(n\) unfoldings and gives a more stable and
computable measure of complexity.

\subsubsection{Mode-n Rank}\label{mode-n-rank}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\):

\begin{itemize}
\tightlist
\item
  The mode-n rank is the matrix rank of its mode-\(n\) unfolding
  \(T^{(n)}\).
\item
  Denote it by \(\text{rank}_n(T)\).
\end{itemize}

\subsubsection{Multilinear (Tucker) Rank}\label{multilinear-tucker-rank}

The multilinear rank of \(T\) is the tuple

\[
\big( \text{rank}_1(T), \text{rank}_2(T), \dots, \text{rank}_N(T) \big).
\]

This captures how much independent variation the tensor has along each
mode.

\subsubsection{Example (3rd-Order
Tensor)}\label{example-3rd-order-tensor-1}

Suppose \(T \in \mathbb{R}^{3 \times 4 \times 5}\).

\begin{itemize}
\tightlist
\item
  Mode-1 unfolding rank = 2.
\item
  Mode-2 unfolding rank = 3.
\item
  Mode-3 unfolding rank = 4.
\end{itemize}

Then the multilinear rank of \(T\) is \((2,3,4)\).

\subsubsection{Comparison with CP Rank}\label{comparison-with-cp-rank}

\begin{itemize}
\tightlist
\item
  CP rank: minimal number of rank-1 outer products.
\item
  Tucker rank: ranks of unfoldings (tuple of integers).
\item
  CP rank is a single number, often hard to compute.
\item
  Tucker rank is a multi-dimensional profile, easier to compute via SVD
  of unfoldings.
\end{itemize}

\subsubsection{Tucker Decomposition}\label{tucker-decomposition}

The multilinear rank is closely tied to the Tucker decomposition:

\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]

where

\begin{itemize}
\tightlist
\item
  \(G\) = core tensor,
\item
  \(U^{(n)}\) = basis matrices capturing each mode's subspace.
\end{itemize}

The dimensions of \(G\) are exactly the multilinear rank.

\subsubsection{Why This Matters}\label{why-this-matters-34}

\begin{itemize}
\tightlist
\item
  Multilinear rank provides a practical, computable measure of tensor
  complexity.
\item
  It is robust under noise and approximations, unlike CP rank.
\item
  Widely used in tensor compression (Tucker, HOSVD) and machine
  learning.
\end{itemize}

\subsubsection{Exercises}\label{exercises-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Case: Show that for a matrix (2nd-order tensor), the Tucker
  rank reduces to the usual matrix rank.
\item
  Rank Profile: Suppose \(T \in \mathbb{R}^{2 \times 3 \times 4}\) has
  mode ranks (2,2,1). What does this tell you about its structure?
\item
  Unfolding Rank: For a tensor \(T_{ijk} = u_i v_j\) (independent of
  \(k\)), compute its Tucker rank.
\item
  Comparison: Give an example of a tensor with low Tucker rank but high
  CP rank.
\item
  Thought Experiment: Why might Tucker rank be preferred over CP rank in
  applications like compression or noise-robust signal processing?
  \#\#\# 13.3 Identifiability and Uniqueness
\end{enumerate}

When we decompose a tensor into simpler components, a natural question
arises: is the decomposition unique? For matrices, the SVD gives a
stable, essentially unique factorization. For tensors, things are more
subtle. This section explores identifiability - the conditions under
which tensor decompositions are unique.

\subsubsection{Identifiability in CP
Rank}\label{identifiability-in-cp-rank}

For a CP (CANDECOMP/PARAFAC) decomposition

\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]

the decomposition is said to be identifiable if no other decomposition
with the same rank \(r\) exists (up to trivial scaling and permutation).

\begin{itemize}
\item
  Trivial indeterminacies:

  \begin{itemize}
  \tightlist
  \item
    Scaling: multiplying one factor by \(\alpha\) and another by
    \(1/\alpha\).
  \item
    Permutation: reordering the components.
  \end{itemize}
\end{itemize}

\subsubsection{Kruskal's Condition (for
Uniqueness)}\label{kruskals-condition-for-uniqueness}

A famous result: If each factor matrix \(U^{(n)}\) has Kruskal rank
\(k_n\) (maximum number of columns that are linearly independent), and

\[
k_1 + k_2 + \cdots + k_N \geq 2r + (N-1),
\]

then the CP decomposition is unique (up to scaling and permutation).

\subsubsection{Identifiability in Tucker
Rank}\label{identifiability-in-tucker-rank}

For Tucker decomposition

\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]

uniqueness is generally weaker:

\begin{itemize}
\tightlist
\item
  The core tensor \(G\) is not unique.
\item
  However, the subspaces spanned by the factor matrices \(U^{(n)}\) are
  unique (this is the essence of HOSVD).
\end{itemize}

\subsubsection{Why Uniqueness Matters}\label{why-uniqueness-matters}

\begin{itemize}
\tightlist
\item
  In applications like chemometrics, signal separation, and latent
  factor models, unique decomposition means interpretable components.
\item
  Without identifiability, decompositions may still compress data but
  lose meaning.
\item
  CP rank uniqueness is one reason tensors can capture latent structure
  more faithfully than matrices.
\end{itemize}

\subsubsection{Challenges}\label{challenges}

\begin{itemize}
\tightlist
\item
  Checking uniqueness conditions is not always easy.
\item
  In practice, algorithms may converge to non-unique solutions.
\item
  Regularization and domain knowledge are often used to guide towards
  interpretable decompositions.
\end{itemize}

\subsubsection{Exercises}\label{exercises-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix vs.~Tensor: Why is the SVD of a matrix unique (up to signs),
  but CP decomposition of a tensor not always unique?
\item
  Scaling Ambiguity: Show explicitly how scaling one factor vector and
  compensating in another keeps the CP decomposition unchanged.
\item
  Permutation Ambiguity: Given two equivalent CP decompositions,
  illustrate how permuting rank-1 components yields the same tensor.
\item
  Kruskal's Condition: Suppose a 3rd-order tensor has factor matrices
  with Kruskal ranks (3,3,3). For rank \(r=3\), does Kruskal's condition
  guarantee uniqueness?
\item
  Thought Experiment: Why might uniqueness of tensor decompositions be
  more valuable in data analysis than uniqueness of matrix
  decompositions?
\end{enumerate}

\section{Chapter 14. Cannonical
Decompositions}\label{chapter-14.-cannonical-decompositions}

\subsection{14.1 CP (CANDECOMP/PARAFAC)}\label{cp-candecompparafac}

The Canonical Polyadic (CP) decomposition - also known as
CANDECOMP/PARAFAC - is one of the most fundamental ways to factorize a
tensor. It generalizes the idea of expressing a matrix as a sum of
rank-one outer products to higher-order tensors.

\subsubsection{Definition}\label{definition-2}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\), a CP
decomposition of rank \(r\) is:

\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]

where each \(u^{(n)}_i \in \mathbb{R}^{I_n}\).

\begin{itemize}
\tightlist
\item
  Each term is a rank-one tensor.
\item
  The smallest such \(r\) is the CP rank of \(T\).
\end{itemize}

\subsubsection{Example (Matrix Case)}\label{example-matrix-case}

For a matrix \(A\), the CP decomposition reduces to the familiar rank
factorization:

\[
A = \sum_{i=1}^r u_i v_i^\top.
\]

\subsubsection{Example (3rd-Order
Tensor)}\label{example-3rd-order-tensor-2}

For \(T \in \mathbb{R}^{I \times J \times K}\), the CP decomposition is:

\[
T_{ijk} = \sum_{r=1}^R a_{ir} b_{jr} c_{kr},
\]

where \(A = [a_{ir}], B = [b_{jr}], C = [c_{kr}]\) are called factor
matrices.

\subsubsection{Properties}\label{properties-3}

\begin{itemize}
\tightlist
\item
  CP decomposition is unique under mild conditions (contrast with matrix
  factorization).
\item
  Provides a compact representation: instead of storing \(IJK\) entries,
  we store only factor matrices of size \((I+J+K)R\).
\item
  Often interpretable in applications (each component corresponds to a
  latent factor).
\end{itemize}

\subsubsection{Applications}\label{applications-2}

\begin{itemize}
\tightlist
\item
  Psychometrics: original use of PARAFAC for analyzing survey data.
\item
  Chemometrics: spectral analysis of chemical mixtures.
\item
  Signal processing: blind source separation.
\item
  Machine learning: latent factor models, recommender systems, neural
  net compression.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-35}

\begin{itemize}
\tightlist
\item
  CP is the most direct generalization of matrix rank factorization.
\item
  Unlike SVD, CP works without orthogonality constraints, making it more
  flexible.
\item
  Its uniqueness is a key reason for its wide use in data analysis.
\end{itemize}

\subsubsection{Exercises}\label{exercises-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Analogy: Show that the CP decomposition for a \(2 \times 2\)
  matrix is the same as its rank decomposition.
\item
  3rd-Order Example: Suppose \(T_{ijk} = u_i v_j w_k\). Show that \(T\)
  has CP rank 1.
\item
  Factor Matrices: Write explicitly the factor matrices \(A,B,C\) for

  \[
  T_{ijk} = \delta_{ij}\delta_{jk}.
  \]
\item
  Compression: Estimate storage cost for a tensor of size
  \(50 \times 40 \times 30\) if represented directly vs.~CP
  decomposition with rank \(r=10\).
\item
  Thought Experiment: Why might CP decomposition be more interpretable
  than Tucker decomposition in some applications?
\end{enumerate}

\subsection{14.2 Tucker and HOSVD}\label{tucker-and-hosvd}

The Tucker decomposition is another foundational way to factorize
tensors. It generalizes the matrix SVD to higher dimensions and is
closely tied to the concept of multilinear rank. When computed with
orthogonal factors, it is often called the Higher-Order SVD (HOSVD).

\subsubsection{Tucker Decomposition}\label{tucker-decomposition-1}

For a tensor
\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\), the
Tucker decomposition is:

\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]

where:

\begin{itemize}
\tightlist
\item
  \(G \in \mathbb{R}^{R_1 \times R_2 \times \cdots \times R_N}\) is the
  core tensor,
\item
  \(U^{(n)} \in \mathbb{R}^{I_n \times R_n}\) are the factor matrices,
\item
  \((R_1, R_2, \dots, R_N)\) is the multilinear rank of \(T\).
\end{itemize}

\subsubsection{Comparison with CP}\label{comparison-with-cp}

\begin{itemize}
\tightlist
\item
  CP: sum of rank-one components, no core tensor.
\item
  Tucker: includes a core tensor that mixes components across modes.
\item
  CP rank: single number, often hard to compute.
\item
  Tucker rank: tuple \((R_1,\dots,R_N)\), easier to compute (via SVD of
  unfoldings).
\end{itemize}

\subsubsection{Higher-Order SVD (HOSVD)}\label{higher-order-svd-hosvd}

The HOSVD is a special Tucker decomposition where:

\begin{itemize}
\tightlist
\item
  Each factor matrix \(U^{(n)}\) has orthonormal columns (from the SVD
  of the mode-\(n\) unfolding).
\item
  The core tensor \(G\) has certain orthogonality properties.
\end{itemize}

HOSVD is not unique but provides a stable, interpretable decomposition.

\subsubsection{Example (3rd-Order
Tensor)}\label{example-3rd-order-tensor-3}

For \(T \in \mathbb{R}^{I \times J \times K}\):

\[
T = \sum_{p=1}^{R_1}\sum_{q=1}^{R_2}\sum_{r=1}^{R_3} g_{pqr}\, u^{(1)}_p \otimes u^{(2)}_q \otimes u^{(3)}_r.
\]

Here, the core entries \(g_{pqr}\) show how basis vectors across
different modes interact.

\subsubsection{Applications}\label{applications-3}

\begin{itemize}
\tightlist
\item
  Compression: reduce each dimension by truncating factor matrices.
\item
  Signal processing: spatiotemporal data analysis.
\item
  Machine learning: feature extraction and dimensionality reduction.
\item
  Neuroscience: multi-subject brain activity analysis.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-36}

\begin{itemize}
\tightlist
\item
  Tucker/HOSVD generalize the SVD to tensors, making them intuitive for
  those familiar with matrices.
\item
  Tucker rank is easier to compute than CP rank, and truncation gives
  practical low-rank approximations.
\item
  Provides a balance between interpretability (factor matrices) and
  flexibility (core tensor).
\end{itemize}

\subsubsection{Exercises}\label{exercises-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Analogy: Show that Tucker decomposition reduces to the usual
  SVD when \(N=2\).
\item
  Mode Ranks: For \(T \in \mathbb{R}^{3 \times 4 \times 5}\), suppose
  its multilinear rank is \((2,3,2)\). What is the size of the core
  tensor?
\item
  Factor Matrix Construction: Explain how to construct the mode-1 factor
  matrix \(U^{(1)}\) from the SVD of the mode-1 unfolding.
\item
  Compression: Estimate storage cost for a tensor of size
  \(30 \times 40 \times 50\) with Tucker rank \((5,5,5)\). Compare to
  full storage.
\item
  Thought Experiment: Why might Tucker/HOSVD be better than CP for
  approximation, even if CP is more interpretable?
\end{enumerate}

\subsection{14.3 Tensor Trains (TT) and Hierarchical
Formats}\label{tensor-trains-tt-and-hierarchical-formats}

When tensors become very large (high dimensions or many modes), storing
and computing with them directly becomes impossible. Tensor Train (TT)
decomposition and related hierarchical formats provide scalable ways to
represent such tensors with drastically reduced storage.

\subsubsection{Tensor Train (TT)
Decomposition}\label{tensor-train-tt-decomposition}

A tensor \(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)
is represented as a product of smaller 3-way tensors (called cores):

\[
T_{i_1 i_2 \cdots i_N} = G^{(1)}_{i_1} G^{(2)}_{i_2} \cdots G^{(N)}_{i_N},
\]

where:

\begin{itemize}
\tightlist
\item
  Each \(G^{(k)}_{i_k}\) is an \(r_{k-1} \times r_k\) matrix,
\item
  \((r_0, r_1, \dots, r_N)\) are the TT ranks, with \(r_0 = r_N = 1\).
\end{itemize}

Thus, the tensor entry is computed by multiplying a chain of matrices.

\subsubsection{Example (Order-3 Tensor)}\label{example-order-3-tensor}

For \(T \in \mathbb{R}^{I_1 \times I_2 \times I_3}\):

\[
T_{i_1 i_2 i_3} = \sum_{a=1}^{r_1}\sum_{b=1}^{r_2} G^{(1)}_{i_1, a} \, G^{(2)}_{a, i_2, b} \, G^{(3)}_{b, i_3}.
\]

\subsubsection{Storage Cost}\label{storage-cost}

\begin{itemize}
\tightlist
\item
  Full tensor: \(I_1 I_2 \cdots I_N\) entries.
\item
  TT decomposition: about \(\sum_{k=1}^N I_k r_{k-1} r_k\).
\item
  For moderate TT ranks, this is exponentially smaller.
\end{itemize}

\subsubsection{Hierarchical Formats (HT,
H-Tucker)}\label{hierarchical-formats-ht-h-tucker}

\begin{itemize}
\tightlist
\item
  Hierarchical Tucker (HT) generalizes TT with a tree structure.
\item
  Both TT and HT exploit low-rank structure in different unfolding
  schemes.
\item
  Widely used in scientific computing, quantum physics, and deep
  learning.
\end{itemize}

\subsubsection{Applications}\label{applications-4}

\begin{itemize}
\tightlist
\item
  Scientific computing: solving PDEs with high-dimensional
  discretizations.
\item
  Quantum physics: matrix product states (MPS) in quantum many-body
  systems.
\item
  Machine learning: compressing large models, representing structured
  kernels.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-37}

\begin{itemize}
\tightlist
\item
  TT and HT make computations feasible in dimensions where naive methods
  fail (``curse of dimensionality'').
\item
  They connect tensor methods with physics (MPS) and numerical
  mathematics (low-rank solvers).
\item
  Provide scalable building blocks for high-dimensional learning and
  simulation.
\end{itemize}

\subsubsection{Exercises}\label{exercises-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Storage Comparison: Compute storage size for a tensor of size
  \(10 \times 10 \times 10 \times 10\) vs.~TT decomposition with TT
  ranks all equal to 5.
\item
  Chain Structure: For \(T \in \mathbb{R}^{4 \times 4 \times 4}\),
  sketch the TT structure (cores and ranks).
\item
  Rank-1 Case: Show that if all TT ranks are 1, the TT decomposition
  reduces to a rank-one tensor.
\item
  Quantum Link: Explain how TT decomposition corresponds to Matrix
  Product States (MPS) in physics.
\item
  Thought Experiment: Why does the chain-like structure of TT
  decomposition scale better than CP or Tucker decompositions in very
  high dimensions?
\end{enumerate}

\subsection{14.4 Connections to SVD and
PCA}\label{connections-to-svd-and-pca}

Tensor decompositions are natural generalizations of matrix
factorizations such as Singular Value Decomposition (SVD) and Principal
Component Analysis (PCA). Understanding these connections helps link
classical linear algebra intuition with modern multilinear methods.

\subsubsection{SVD Recap (Matrix Case)}\label{svd-recap-matrix-case}

For a matrix \(A \in \mathbb{R}^{m \times n}\):

\[
A = U \Sigma V^\top,
\]

where

\begin{itemize}
\tightlist
\item
  \(U\) and \(V\) are orthogonal,
\item
  \(\Sigma\) is diagonal with singular values.
\end{itemize}

This factorization is unique (up to signs) and provides:

\begin{itemize}
\tightlist
\item
  Rank,
\item
  Best low-rank approximation,
\item
  Geometric interpretation (rotations and scalings).
\end{itemize}

\subsubsection{PCA Recap}\label{pca-recap}

PCA applies SVD to a data matrix:

\begin{itemize}
\tightlist
\item
  Rows = observations, columns = features.
\item
  Extracts orthogonal directions (principal components) that maximize
  variance.
\end{itemize}

\subsubsection{CP and SVD}\label{cp-and-svd}

\begin{itemize}
\tightlist
\item
  CP decomposition generalizes matrix rank factorization to higher
  orders.
\item
  A matrix is just a 2-way tensor: CP = rank decomposition = SVD without
  orthogonality.
\item
  For \(N \geq 3\), CP decomposition does not correspond to orthogonal
  factors, but it still reveals latent components.
\end{itemize}

\subsubsection{Tucker/HOSVD and SVD}\label{tuckerhosvd-and-svd}

\begin{itemize}
\item
  Tucker decomposition generalizes SVD to higher-order tensors.
\item
  HOSVD is literally the higher-order SVD:

  \begin{itemize}
  \tightlist
  \item
    Compute SVD of each mode unfolding.
  \item
    Factor matrices = left singular vectors.
  \item
    Core tensor = interaction of components across modes.
  \end{itemize}
\item
  Truncating singular vectors gives best low-rank approximations in a
  multilinear sense.
\end{itemize}

\subsubsection{PCA vs.~Multilinear PCA}\label{pca-vs.-multilinear-pca}

\begin{itemize}
\tightlist
\item
  PCA = best low-rank approximation of a data matrix.
\item
  Tucker decomposition = multilinear PCA, reducing dimensionality along
  each mode simultaneously.
\item
  Applications: face recognition, video compression, multi-way data
  analysis.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-38}

\begin{itemize}
\tightlist
\item
  CP ↔ matrix rank factorization.
\item
  Tucker/HOSVD ↔ SVD/PCA.
\item
  These links provide intuition: tensor decompositions are not
  arbitrary; they extend familiar tools to multi-way data.
\end{itemize}

\subsubsection{Exercises}\label{exercises-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  SVD Analogy: Show how SVD of a \(3 \times 3\) matrix can be viewed as
  a Tucker decomposition with a diagonal core.
\item
  CP vs.~SVD: Explain why CP decomposition of a matrix reduces to its
  rank factorization, but not to SVD.
\item
  HOSVD Steps: Outline the steps of HOSVD for a 3rd-order tensor
  \(T \in \mathbb{R}^{4 \times 5 \times 6}\).
\item
  PCA Analogy: Suppose we have a video dataset stored as a tensor
  \((\text{frames} \times \text{height} \times \text{width})\). Explain
  how Tucker decomposition acts as a multilinear PCA.
\item
  Thought Experiment: Why is orthogonality central in SVD/PCA, but not
  in CP decomposition? What are the trade-offs?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part VIII. Computation and Numerical
Practice}\label{part-viii.-computation-and-numerical-practice}

\section{Chapter 15. Working with Tensors in
Code}\label{chapter-15.-working-with-tensors-in-code}

\subsection{15.1 Efficient Indexing and Memory
Layout}\label{efficient-indexing-and-memory-layout}

Working with tensors in practice requires careful attention to how they
are stored and accessed in memory. Poor indexing can make even simple
operations slow. This section explains how indexing works under the hood
and how to optimize memory layout for performance.

\subsubsection{Linearization of
Multi-Indices}\label{linearization-of-multi-indices}

A tensor \(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)
is stored in linear memory.

\begin{itemize}
\item
  Each entry \(T_{i_1,i_2,\dots,i_N}\) is mapped to a single offset in
  memory.
\item
  Formula (row-major / C-style layout):

  \[
  \text{offset}(i_1,\dots,i_N) = i_1 (I_2 I_3 \cdots I_N) + i_2 (I_3 \cdots I_N) + \cdots + i_N.
  \]
\item
  Formula (column-major / Fortran, MATLAB):

  \[
  \text{offset}(i_1,\dots,i_N) = i_1 + I_1(i_2 + I_2(i_3 + \cdots + I_{N-1} i_N)).
  \]
\end{itemize}

\subsubsection{Strides}\label{strides}

\begin{itemize}
\tightlist
\item
  A stride tells how far (in memory) you move when an index increases by
  1.
\item
  Example: in row-major order, stride along the last index = 1.
\item
  Efficient access requires stepping through memory with small,
  contiguous strides.
\end{itemize}

\subsubsection{Cache Efficiency}\label{cache-efficiency}

\begin{itemize}
\tightlist
\item
  CPUs fetch memory in blocks (cache lines).
\item
  Accessing elements in a contiguous block is fast.
\item
  Jumping across large strides leads to cache misses and slow
  performance.
\end{itemize}

\subsubsection{Practical Implications}\label{practical-implications}

\begin{itemize}
\tightlist
\item
  Loop ordering matters: iterate over the innermost (stride-1) dimension
  in the inner loop.
\item
  Slicing tensors may produce views with non-contiguous strides (NumPy,
  PyTorch). Copying may be needed for efficiency.
\item
  Tensor libraries often allow explicit control of memory layout
  (row-major vs.~column-major).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-39}

\begin{itemize}
\tightlist
\item
  Many tensor operations are memory-bound, not compute-bound.
\item
  Efficient indexing and layout can make the difference between minutes
  and milliseconds.
\item
  A good understanding of strides helps when debugging tensor code in
  NumPy, PyTorch, JAX, etc.
\end{itemize}

\subsection{Exercises}\label{exercises-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Offset Calculation (Row-Major): For
  \(T \in \mathbb{R}^{2 \times 3 \times 4}\), what is the memory offset
  of \(T_{1,2,3}\) (0-based indexing)?
\item
  Offset Calculation (Column-Major): For the same tensor, compute the
  offset of \(T_{1,2,3}\) in column-major order.
\item
  Stride Check: In row-major order for
  \(T \in \mathbb{R}^{3 \times 4 \times 5}\), what are the strides for
  each dimension?
\item
  Cache-Efficient Loop: Write pseudocode for iterating over all entries
  of a 3D tensor in row-major order.
\item
  Thought Experiment: Why might a deep learning library internally
  reorder tensor layouts depending on the hardware (CPU vs.~GPU)?
\end{enumerate}

\subsection{15.2 BLAS, Einsum, Performance
Patterns}\label{blas-einsum-performance-patterns}

After understanding memory layout, the next step in efficient tensor
computation is using optimized libraries and abstractions. This section
covers BLAS, the einsum notation, and common performance patterns that
make tensor operations practical at scale.

\subsubsection{BLAS: Basic Linear Algebra
Subprograms}\label{blas-basic-linear-algebra-subprograms}

\begin{itemize}
\item
  BLAS is the standard library for high-performance vector and matrix
  operations.
\item
  Many tensor operations reduce to BLAS calls:

  \begin{itemize}
  \tightlist
  \item
    Matrix multiplication (GEMM) is the backbone of most tensor
    contractions.
  \item
    Level-1 BLAS: vector ops (\(y \leftarrow ax + y\)).
  \item
    Level-2 BLAS: matrix-vector ops.
  \item
    Level-3 BLAS: matrix-matrix ops (highest efficiency).
  \end{itemize}
\end{itemize}

Why this matters: Efficient tensor libraries (NumPy, PyTorch,
TensorFlow, JAX) rely heavily on BLAS under the hood.

\subsubsection{Einsum Notation}\label{einsum-notation}

The Einstein summation convention (``einsum'') expresses tensor
contractions concisely.

Example:

\begin{itemize}
\item
  Matrix multiplication:

  \[
  C_{ij} = \sum_k A_{ik} B_{kj}
  \]

  In einsum:
  \texttt{einsum(\textquotesingle{}ik,kj-\textgreater{}ij\textquotesingle{},\ A,\ B)}.
\item
  Inner product:

  \[
  \langle u,v \rangle = \sum_i u_i v_i
  \]

  In einsum:
  \texttt{einsum(\textquotesingle{}i,i-\textgreater{}\textquotesingle{},\ u,\ v)}.
\item
  Outer product:

  \[
  T_{ij} = u_i v_j
  \]

  In einsum:
  \texttt{einsum(\textquotesingle{}i,j-\textgreater{}ij\textquotesingle{},\ u,\ v)}.
\end{itemize}

Advantages:

\begin{itemize}
\tightlist
\item
  Expresses contractions clearly without reshaping.
\item
  Often compiles to highly efficient BLAS/GPU kernels.
\end{itemize}

\subsubsection{Performance Patterns}\label{performance-patterns}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Batching: Group operations across multiple tensors (e.g., batched GEMM
  in GPUs).
\item
  Blocking / Tiling: Break large tensors into cache-sized blocks to
  improve locality.
\item
  Fusing Operations: Combine multiple small operations into one kernel
  (important in GPU computing).
\item
  Avoiding Copies: Use views/strides instead of reshaping whenever
  possible.
\item
  Automatic Differentiation: Frameworks like PyTorch and JAX integrate
  einsum with backpropagation efficiently.
\end{enumerate}

\subsubsection{Why This Matters}\label{why-this-matters-40}

\begin{itemize}
\tightlist
\item
  BLAS-level performance is critical for large-scale tensor
  applications.
\item
  Einsum notation unifies different tensor operations under one compact
  framework.
\item
  Recognizing performance patterns makes code scale across CPUs, GPUs,
  and accelerators.
\end{itemize}

\subsubsection{Exercises}\label{exercises-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Einsum Practice: Write the einsum expression for computing

  \[
  C_{ij} = \sum_{k,l} A_{ik} B_{kl} D_{lj}.
  \]
\item
  Outer Product: Using einsum, compute the 3rd-order tensor
  \(T_{ijk} = u_i v_j w_k\).
\item
  BLAS Levels: Classify the following into BLAS level-1, 2, or 3:

  \begin{itemize}
  \tightlist
  \item
    Dot product,
  \item
    Matrix-vector product,
  \item
    Matrix-matrix product.
  \end{itemize}
\item
  Batching Example: If you have 100 matrices of size \(50 \times 50\),
  why is a batched GEMM faster than 100 separate GEMM calls?
\item
  Thought Experiment: Why might einsum be more maintainable than
  manually reshaping and transposing arrays for contractions?
\end{enumerate}

\subsection{15.3 Stability, Conditioning, Scaling
Tricks}\label{stability-conditioning-scaling-tricks}

Efficient computation is not enough - tensor operations must also be
numerically stable. Large-scale problems often involve ill-conditioned
matrices or tensors, and small floating-point errors can accumulate
dramatically. This section introduces stability concerns and practical
tricks for keeping computations reliable.

\subsubsection{Conditioning and
Stability}\label{conditioning-and-stability}

\begin{itemize}
\item
  Condition number: For a matrix \(A\),

  \[
  \kappa(A) = \|A\| \cdot \|A^{-1}\|
  \]

  measures sensitivity of solutions to perturbations.
\item
  In tensors, contractions can amplify errors, especially when factors
  are nearly linearly dependent.
\item
  High tensor ranks often worsen conditioning.
\end{itemize}

Rule of thumb: Poorly conditioned problems cannot be solved accurately,
no matter the algorithm.

\subsubsection{Common Stability Issues in
Tensors}\label{common-stability-issues-in-tensors}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Overflows/underflows: multiplying many large/small entries.
\item
  Loss of orthogonality: iterative algorithms drift from true subspaces.
\item
  Cancellation errors: subtracting nearly equal numbers.
\item
  Exploding/vanishing gradients: in automatic differentiation with deep
  tensor networks.
\end{enumerate}

\subsubsection{Scaling Tricks}\label{scaling-tricks}

\begin{itemize}
\tightlist
\item
  Normalization: Rescale vectors and factor matrices to keep entries in
  a safe range.
\item
  Orthogonalization: Regularly re-orthogonalize factor matrices in
  decompositions (QR or SVD steps).
\item
  Log-domain computations: Replace products with sums of logarithms to
  prevent overflow (e.g., in probabilistic models).
\item
  Balanced scaling: In CP decompositions, distribute scale evenly across
  modes to avoid extreme values.
\end{itemize}

\subsubsection{Regularization}\label{regularization}

\begin{itemize}
\tightlist
\item
  Add small perturbations (like \(\lambda I\)) to stabilize inversions
  (``Tikhonov regularization'').
\item
  In optimization, add penalties to discourage ill-conditioned
  solutions.
\item
  Helps avoid overfitting in statistical tensor models.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-41}

\begin{itemize}
\tightlist
\item
  Numerical stability is essential for trustworthy tensor computations.
\item
  Scaling and orthogonalization tricks are used in nearly every
  practical algorithm.
\item
  Without them, decompositions may diverge, optimizations may fail, and
  results may become meaningless.
\end{itemize}

\subsubsection{Exercises}\label{exercises-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Condition Number: Compute the condition number of

  \[
  A = \begin{bmatrix}1 & 0 \\ 0 & 10^{-6}\end{bmatrix}.
  \]

  What does it tell you about stability?
\item
  Overflow Example: Suppose we compute the product of 100 numbers all
  equal to 1.01. Estimate the result. Why might floating-point overflow
  occur?
\item
  Scaling in CP: Explain why rescaling one factor by \(10^6\) and
  another by \(10^{-6}\) in a CP decomposition gives the same tensor but
  may cause instability.
\item
  Orthogonalization: Describe how QR factorization can help maintain
  numerical stability in HOSVD computations.
\item
  Thought Experiment: Why might log-domain computation be essential in
  probabilistic models with tensors (e.g., hidden Markov models,
  Bayesian networks)?
\end{enumerate}

\section{Chapter 16. Automatic Differentiation and
Gradients}\label{chapter-16.-automatic-differentiation-and-gradients}

\subsection{16.1 Jacobians/Hessians as
Tensors}\label{jacobianshessians-as-tensors}

In calculus, derivatives of multivariable functions are naturally
represented as tensors. Recognizing this viewpoint helps connect
analysis with multilinear algebra, and explains why tensors appear in
optimization, machine learning, and physics.

\subsubsection{Jacobian as a Matrix (2nd-Order
Tensor)}\label{jacobian-as-a-matrix-2nd-order-tensor}

For a function \(f: \mathbb{R}^n \to \mathbb{R}^m\), the Jacobian matrix
is

\[
J_{ij} = \frac{\partial f_i}{\partial x_j}.
\]

\begin{itemize}
\tightlist
\item
  It describes how small changes in input \(x\) produce changes in
  output \(f(x)\).
\item
  In tensor terms: \(J \in \mathbb{R}^{m \times n}\).
\end{itemize}

Example:

\[
f(x,y) = (x^2, xy), \quad J = \begin{bmatrix} 2x & 0 \\ y & x \end{bmatrix}.
\]

\subsubsection{Hessian as a 2nd-Order Derivative
Tensor}\label{hessian-as-a-2nd-order-derivative-tensor}

For a scalar function \(g: \mathbb{R}^n \to \mathbb{R}\), the Hessian is

\[
H_{ij} = \frac{\partial^2 g}{\partial x_i \partial x_j}.
\]

\begin{itemize}
\tightlist
\item
  It is a symmetric matrix (\(H_{ij} = H_{ji}\)).
\item
  Encodes curvature: quadratic approximation of \(g(x)\).
\item
  In optimization, eigenvalues of \(H\) indicate convexity.
\end{itemize}

\subsubsection{Higher-Order Derivatives as
Tensors}\label{higher-order-derivatives-as-tensors}

\begin{itemize}
\item
  Third derivatives form a 3rd-order tensor:

  \[
  T_{ijk} = \frac{\partial^3 g}{\partial x_i \partial x_j \partial x_k}.
  \]
\item
  In general, the \(k\)-th derivative of \(g\) is a symmetric
  \(k\)-tensor.
\item
  These appear in Taylor expansions, perturbation analysis, and physics
  (nonlinear elasticity, quantum chemistry).
\end{itemize}

\subsubsection{Automatic Differentiation (AD)
Perspective}\label{automatic-differentiation-ad-perspective}

\begin{itemize}
\tightlist
\item
  AD frameworks (PyTorch, JAX, TensorFlow) compute Jacobians, Hessians,
  and higher derivatives automatically.
\item
  Under the hood, they construct tensors of partial derivatives and
  contract them efficiently.
\item
  Tensor viewpoint clarifies why gradients, Jacobians, and Hessians are
  different ``levels'' of the same structure.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-42}

\begin{itemize}
\tightlist
\item
  Viewing derivatives as tensors unifies multivariable calculus and
  multilinear algebra.
\item
  Explains the role of Jacobians in transformations, Hessians in
  optimization, and higher derivatives in scientific modeling.
\item
  Essential foundation for backpropagation and deep learning.
\end{itemize}

\subsection{Exercises}\label{exercises-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Jacobian Practice: Compute the Jacobian of

  \[
  f(x,y,z) = (xy, yz, xz).
  \]
\item
  Hessian Example: For \(g(x,y) = x^2y + y^3\), compute the Hessian
  matrix.
\item
  Symmetry Check: Show explicitly that mixed partials of \(g(x,y)\) are
  equal:
  \(\frac{\partial^2 g}{\partial x \partial y} = \frac{\partial^2 g}{\partial y \partial x}\).
\item
  Third Derivative Tensor: Write down all nonzero entries of the 3rd
  derivative tensor of \(g(x) = x^4\) (1D case).
\item
  Thought Experiment: Why is it natural that higher derivatives are
  symmetric tensors? What would break if they weren't?
\end{enumerate}

\subsection{16.2 Backprop as Structured
Contractions}\label{backprop-as-structured-contractions}

Backpropagation, the core algorithm behind training neural networks, is
fundamentally a sequence of tensor contractions guided by the chain
rule. Multilinear algebra provides a clean way to see why backprop works
and why it is efficient.

\subsubsection{Chain Rule in Tensor
Form}\label{chain-rule-in-tensor-form}

For functions \(f: \mathbb{R}^n \to \mathbb{R}^m\) and
\(g: \mathbb{R}^m \to \mathbb{R}^p\):

\[
J_{g \circ f}(x) = J_g(f(x)) \, J_f(x),
\]

where \(J\) denotes the Jacobian.

\begin{itemize}
\tightlist
\item
  Composition of functions = multiplication (contraction) of Jacobians.
\item
  Backpropagation efficiently evaluates this chain without materializing
  huge Jacobians.
\end{itemize}

\subsubsection{Forward vs.~Reverse Mode}\label{forward-vs.-reverse-mode}

\begin{itemize}
\tightlist
\item
  Forward mode AD: propagate derivatives forward (good when inputs are
  few).
\item
  Reverse mode AD (backprop): propagate sensitivities backward (good
  when outputs are few, e.g.~scalar loss).
\end{itemize}

In reverse mode:

\[
\frac{\partial L}{\partial x} = \left( \frac{\partial y}{\partial x} \right)^\top \frac{\partial L}{\partial y}.
\]

This is a tensor contraction: contract gradient vector with a Jacobian.

\subsubsection{Layer-by-Layer in Neural
Networks}\label{layer-by-layer-in-neural-networks}

Each layer is a function:

\[
h^{(l+1)} = \sigma(W^{(l)} h^{(l)} + b^{(l)}).
\]

Backprop proceeds by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute forward activations.
\item
  For loss \(L\), compute gradient wrt output:
  \(\frac{\partial L}{\partial h^{(L)}}\).
\item
  Contract backwards through each layer:

  \begin{itemize}
  \tightlist
  \item
    Jacobian of linear part: \(W^{(l)}\).
  \item
    Jacobian of nonlinearity: diagonal tensor of \(\sigma'(z)\).
  \end{itemize}
\end{enumerate}

\subsubsection{Example (Two Layers)}\label{example-two-layers}

For \(L(f(x))\) with \(f(x) = \sigma(Wx)\):

\[
\frac{\partial L}{\partial x} = W^\top \big( \sigma'(Wx) \odot \frac{\partial L}{\partial f} \big).
\]

Here:

\begin{itemize}
\tightlist
\item
  \(\odot\) = elementwise product,
\item
  Contraction with \(W^\top\) propagates gradient backward.
\end{itemize}

\subsubsection{Tensor Viewpoint}\label{tensor-viewpoint}

\begin{itemize}
\tightlist
\item
  Jacobians are tensors.
\item
  Backprop avoids forming full Jacobians (which would be huge) by
  contracting only along needed directions.
\item
  Each step is a structured contraction of the gradient with the local
  Jacobian.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-43}

\begin{itemize}
\tightlist
\item
  Explains efficiency: backprop runs in time proportional to the forward
  pass.
\item
  Shows the unity of AD, calculus, and multilinear algebra.
\item
  Clarifies why backprop generalizes beyond neural nets (any
  differentiable computational graph).
\end{itemize}

\subsubsection{Exercises}\label{exercises-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Chain Rule Contraction: Let \(f(x,y) = (x+y, xy)\),
  \(g(u,v) = u^2+v\). Write the backprop step explicitly using
  contractions.
\item
  Linear Layer: For \(h = Wx\), show that
  \(\frac{\partial L}{\partial x} = W^\top \frac{\partial L}{\partial h}\).
\item
  Nonlinear Layer: For \(h = \tanh(z)\), derive the contraction rule for
  backpropagation.
\item
  Efficiency Check: Estimate the cost of explicitly forming the Jacobian
  of a fully connected layer with \(m\) outputs and \(n\) inputs, versus
  the cost of backprop.
\item
  Thought Experiment: Why is reverse-mode AD (backprop) much more
  efficient than forward-mode AD for training neural networks?
\end{enumerate}

\subsection{16.3 Practical Tips for
PyTorch/JAX/NumPy}\label{practical-tips-for-pytorchjaxnumpy}

Automatic differentiation (AD) frameworks like PyTorch, JAX, and NumPy
(with autograd extensions) make tensor calculus practical. But
efficiency and clarity depend on how you structure code. This section
gives concrete tips to avoid pitfalls and exploit the strengths of these
libraries.

\subsubsection{Tip 1. Use Vectorization, Not
Loops}\label{tip-1.-use-vectorization-not-loops}

\begin{itemize}
\item
  Replace Python loops with tensorized operations.
\item
  Example (inefficient):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.zeros(n)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{    y[i] }\OperatorTok{=}\NormalTok{ a[i] }\OperatorTok{{-}}\NormalTok{ b[i]}
\end{Highlighting}
\end{Shaded}
\item
  Example (efficient):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ a }\OperatorTok{{-}}\NormalTok{ b}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsubsection{Tip 2. Exploit
Broadcasting}\label{tip-2.-exploit-broadcasting}

\begin{itemize}
\item
  Broadcasting avoids unnecessary reshaping and repetition.
\item
  Example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add bias vector to each row}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ X }\OperatorTok{+}\NormalTok{ b   }\CommentTok{\# automatic broadcasting}
\end{Highlighting}
\end{Shaded}
\item
  Broadcasting keeps memory use low and code clean.
\end{itemize}

\subsubsection{\texorpdfstring{Tip 3. Prefer \texttt{einsum} for Complex
Contractions}{Tip 3. Prefer einsum for Complex Contractions}}\label{tip-3.-prefer-einsum-for-complex-contractions}

\begin{itemize}
\item
  \texttt{einsum} is expressive and optimized.
\item
  Example: matrix multiplication:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C }\OperatorTok{=}\NormalTok{ torch.einsum(}\StringTok{\textquotesingle{}ik,kj{-}\textgreater{}ij\textquotesingle{}}\NormalTok{, A, B)}
\end{Highlighting}
\end{Shaded}
\item
  Works the same in NumPy and JAX.
\end{itemize}

\subsubsection{Tip 4. Control Gradient
Flow}\label{tip-4.-control-gradient-flow}

\begin{itemize}
\tightlist
\item
  In PyTorch: \texttt{x.detach()} to stop gradients.
\item
  In JAX: use \texttt{jax.lax.stop\_gradient(x)}.
\item
  Important for stabilizing training and avoiding accidental memory
  blowups.
\end{itemize}

\subsubsection{Tip 5. Check Shapes with
Assertions}\label{tip-5.-check-shapes-with-assertions}

\begin{itemize}
\item
  Many AD errors come from shape mismatches.
\item
  Insert sanity checks:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{assert}\NormalTok{ X.shape }\OperatorTok{==}\NormalTok{ (batch, features)}
\end{Highlighting}
\end{Shaded}
\item
  Shape discipline avoids subtle bugs in backprop.
\end{itemize}

\subsubsection{Tip 6. Monitor Numerical
Stability}\label{tip-6.-monitor-numerical-stability}

\begin{itemize}
\tightlist
\item
  Use functions like \texttt{torch.nn.functional.log\_softmax} instead
  of naive \texttt{softmax} to avoid overflow.
\item
  Add small epsilons in denominators: \texttt{x\ /\ (y\ +\ 1e-8)}.
\item
  Use mixed precision cautiously (FP16 vs.~FP32).
\end{itemize}

\subsubsection{Tip 7. Benchmark with
Profilers}\label{tip-7.-benchmark-with-profilers}

\begin{itemize}
\tightlist
\item
  PyTorch: \texttt{torch.profiler}.
\item
  JAX: \texttt{jax.profiler.trace}.
\item
  NumPy: \texttt{\%timeit} (Jupyter).
\item
  Helps identify bottlenecks in contractions and data movement.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-44}

\begin{itemize}
\tightlist
\item
  Writing tensor code is easy; writing fast, stable, and scalable tensor
  code requires discipline.
\item
  Following these practices prevents performance cliffs and silent
  gradient bugs.
\item
  Bridges theory (multilinear algebra) with implementation (real
  training pipelines).
\end{itemize}

\subsubsection{Exercises}\label{exercises-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vectorization: Rewrite a loop-based dot product in PyTorch using
  vectorized syntax.
\item
  Broadcasting: Given \(X \in \mathbb{R}^{100 \times 50}\) and
  \(b \in \mathbb{R}^{50}\), add \(b\) to each row using broadcasting.
\item
  Einsum Practice: Write the einsum expression for batched matrix
  multiplication \(Y_b = A_b B_b\), with batch dimension \(b\).
\item
  Gradient Stop: In PyTorch, why might we use \texttt{x.detach()} inside
  a training loop? Give an example.
\item
  Thought Experiment: Why is \texttt{log\_softmax} numerically safer
  than \texttt{exp(x)/sum(exp(x))}?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part IX. Applications you can
touch}\label{part-ix.-applications-you-can-touch}

\section{Chapter 17. Data Science and Signal
Processing}\label{chapter-17.-data-science-and-signal-processing}

\subsection{17.1 Multilinear Regression}\label{multilinear-regression}

Regression is one of the most basic tools in data science: fitting a
model that predicts an output from input data. When the data is
naturally multi-way (tensor-structured) instead of flat vectors or
matrices, multilinear regression becomes a natural extension.

\subsubsection{Ordinary Regression
(Review)}\label{ordinary-regression-review}

For data pairs \((x_i, y_i)\):

\[
y \approx Wx + b.
\]

Here, \(x \in \mathbb{R}^n\), \(y \in \mathbb{R}^m\), and
\(W \in \mathbb{R}^{m \times n}\).

This assumes vector inputs.

\subsubsection{Multilinear Regression
Model}\label{multilinear-regression-model}

Suppose input data is a tensor
\(X \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\). Instead
of flattening \(X\) into a vector, we preserve its structure by using a
multilinear map:

\[
\hat{y} = X \times_1 W^{(1)} \times_2 W^{(2)} \cdots \times_N W^{(N)} + b,
\]

where each \(W^{(n)}\) acts along mode-\(n\).

\begin{itemize}
\tightlist
\item
  This reduces parameter count dramatically compared to a full
  vectorized regression.
\item
  Preserves interpretability along each mode (e.g., time, space,
  frequency).
\end{itemize}

\subsubsection{Example}\label{example-1}

\begin{itemize}
\tightlist
\item
  Input: video clip
  \(X \in \mathbb{R}^{\text{frames} \times \text{height} \times \text{width}}\).
\item
  Flattened regression would need millions of parameters.
\item
  Multilinear regression uses three factor matrices
  \(W^{(\text{frames})}, W^{(\text{height})}, W^{(\text{width})}\),
  drastically reducing parameters.
\end{itemize}

\subsubsection{Training}\label{training}

\begin{itemize}
\tightlist
\item
  Solve by least squares or regularized optimization.
\item
  Often implemented via alternating minimization (update one \(W^{(n)}\)
  at a time).
\item
  Regularization (e.g., low-rank constraints) prevents overfitting.
\end{itemize}

\subsubsection{Applications}\label{applications-5}

\begin{itemize}
\tightlist
\item
  Neuroscience: predict brain activity from multi-way stimulus data.
\item
  Chemometrics: regression on spectral cubes.
\item
  Time-series analysis: structured prediction across modes (time,
  channels, features).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-45}

\begin{itemize}
\tightlist
\item
  Exploits multi-way structure instead of destroying it by flattening.
\item
  Reduces model complexity while keeping interpretability.
\item
  Lays the groundwork for tensor methods in supervised learning.
\end{itemize}

\subsubsection{Exercises}\label{exercises-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Flattened vs.~Multilinear: For input
  \(X \in \mathbb{R}^{10 \times 20}\) and output scalar, how many
  parameters does flattened regression need? How many parameters if we
  use multilinear regression with
  \(W^{(1)} \in \mathbb{R}^{10 \times 3}, W^{(2)} \in \mathbb{R}^{20 \times 3}\)?
\item
  Mode Multiplication: Write explicitly how
  \(X \times_1 W^{(1)} \times_2 W^{(2)}\) works for a 2D input (matrix).
\item
  Interpretability: Explain why multilinear regression can separate
  effects of time and space in spatiotemporal data.
\item
  Optimization: Why is alternating minimization a natural algorithm for
  training multilinear regression models?
\item
  Thought Experiment: In what situations would flattening be acceptable,
  and when is multilinear regression clearly superior?
\end{enumerate}

\subsection{17.2 Spatiotemporal Data and Video
Tensors}\label{spatiotemporal-data-and-video-tensors}

Many real-world datasets are not flat vectors or simple matrices but
inherently multi-way arrays. A prime example is spatiotemporal data -
measurements varying across both space and time. Video is a natural
case: each frame is a 2D image, and the sequence of frames adds a
temporal dimension, giving a 3rd-order tensor.

\subsubsection{Video as a Tensor}\label{video-as-a-tensor}

A grayscale video with \(F\) frames, height \(H\), and width \(W\) is
naturally represented as:

\[
X \in \mathbb{R}^{F \times H \times W}.
\]

For color video, an additional channel dimension is added:

\[
X \in \mathbb{R}^{F \times H \times W \times 3}.
\]

Flattening this into a matrix or vector loses structure and explodes
parameter count.

\subsubsection{Tensor Decomposition for Spatiotemporal
Data}\label{tensor-decomposition-for-spatiotemporal-data}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Tucker decomposition:

  \begin{itemize}
  \tightlist
  \item
    Separates time, spatial rows, and spatial columns into low-rank
    factors.
  \item
    Compresses video efficiently while preserving essential dynamics.
  \end{itemize}
\item
  CP decomposition:

  \begin{itemize}
  \tightlist
  \item
    Represents data as a sum of rank-one spatiotemporal components.
  \item
    Each component factors into (time profile) × (spatial pattern).
  \end{itemize}
\item
  Tensor Train (TT):

  \begin{itemize}
  \tightlist
  \item
    Handles very long video sequences by chaining local factors.
  \end{itemize}
\end{enumerate}

\subsubsection{Applications}\label{applications-6}

\begin{itemize}
\tightlist
\item
  Compression: reduce storage while keeping perceptual quality.
\item
  Background modeling: separate foreground objects from static
  background (via low-rank + sparse decomposition).
\item
  Forecasting: use multilinear regression on decomposed factors to
  predict future frames.
\item
  Pattern discovery: extract temporal modes (e.g., daily cycles) and
  spatial modes (e.g., recurring structures).
\end{itemize}

\subsubsection{Beyond Video: General Spatiotemporal
Data}\label{beyond-video-general-spatiotemporal-data}

\begin{itemize}
\tightlist
\item
  Climate data: temperature, humidity, pressure (time × latitude ×
  longitude × altitude).
\item
  Neuroscience: brain activity measured over time across sensors.
\item
  Traffic flows: time × location × type of vehicle.
\end{itemize}

All benefit from multilinear analysis.

\subsubsection{Why This Matters}\label{why-this-matters-46}

\begin{itemize}
\tightlist
\item
  Treating spatiotemporal data as tensors respects its inherent
  multi-way structure.
\item
  Leads to compact models, better interpretability, and efficient
  computation.
\item
  Bridges data science, machine learning, and physics-based modeling.
\end{itemize}

\subsubsection{Exercises}\label{exercises-60}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Video Dimensions: A color video with 100 frames of size
  \(64 \times 64\). What is its tensor shape?
\item
  Compression: Estimate the storage size (in entries) of this video
  vs.~a Tucker decomposition with ranks \((10, 10, 10, 3)\).
\item
  Foreground/Background: Explain how a low-rank + sparse model might
  separate background (low-rank) from moving objects (sparse).
\item
  Temporal Modes: If CP decomposition yields components of the form
  (time × space), how would you interpret a component with strong daily
  periodicity in the time factor?
\item
  Thought Experiment: Why might tensor decompositions uncover hidden
  structure in spatiotemporal data that PCA on flattened vectors would
  miss?
\end{enumerate}

\subsection{17.3 Blind Source Separation}\label{blind-source-separation}

Blind Source Separation (BSS) is the problem of extracting hidden
signals (sources) from observed mixtures, without detailed knowledge of
how they were mixed. Tensors provide powerful tools for solving BSS,
often outperforming classical matrix-based methods.

\subsubsection{The Mixing Problem}\label{the-mixing-problem}

Suppose we observe signals \(x(t) \in \mathbb{R}^m\) that are mixtures
of \(n\) hidden sources \(s(t) \in \mathbb{R}^n\):

\[
x(t) = A s(t),
\]

where \(A\) is an unknown mixing matrix.

Goal: Recover \(s(t)\) and \(A\) from only the observations \(x(t)\).

\subsubsection{Classical Approach: ICA (Independent Component
Analysis)}\label{classical-approach-ica-independent-component-analysis}

\begin{itemize}
\tightlist
\item
  Assumes sources are statistically independent.
\item
  Uses second- and higher-order statistics to separate signals.
\item
  Works well for simple mixtures, but struggles with multi-way
  structure.
\end{itemize}

\subsubsection{Tensor Approach to BSS}\label{tensor-approach-to-bss}

Moments and cumulants of observed signals are naturally represented as
symmetric tensors:

\begin{itemize}
\tightlist
\item
  2nd-order cumulant (covariance): matrix.
\item
  4th-order cumulant: 4th-order tensor.
\end{itemize}

By analyzing these higher-order tensors:

\begin{itemize}
\tightlist
\item
  Sources can be separated even when covariance is insufficient.
\item
  CP decomposition of the cumulant tensor reveals source directions.
\end{itemize}

\subsubsection{Example: Cocktail Party
Problem}\label{example-cocktail-party-problem}

\begin{itemize}
\tightlist
\item
  Microphones record overlapping voices in a room.
\item
  Covariance matrix cannot separate voices if they overlap in energy.
\item
  4th-order cumulant tensor factorization recovers independent voices.
\end{itemize}

\subsubsection{Applications}\label{applications-7}

\begin{itemize}
\tightlist
\item
  Audio processing: separating voices, music, or environmental sounds.
\item
  Medical imaging: separating independent brain activity sources from
  EEG/fMRI data.
\item
  Telecommunications: extracting signals from mixed channels.
\item
  Finance: identifying independent factors driving market time series.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-47}

\begin{itemize}
\tightlist
\item
  Tensor methods exploit multi-way statistical structure, not just
  pairwise correlations.
\item
  CP decomposition guarantees identifiability in cases where matrix
  factorizations fail.
\item
  This makes BSS one of the most successful real-world applications of
  multilinear algebra.
\end{itemize}

\subsubsection{Exercises}\label{exercises-61}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Covariance Limitation: Why might two voices with similar pitch have
  indistinguishable covariance, but separable 4th-order statistics?
\item
  Tensor Rank: Explain why the CP rank of the 4th-order cumulant tensor
  corresponds to the number of independent sources.
\item
  Practical Example: Given three observed mixtures of two signals,
  sketch how CP decomposition could be used to separate them.
\item
  ICA vs.~Tensor: Compare ICA (matrix-based) and tensor-based approaches
  for BSS. Which one uses more information about the data?
\item
  Thought Experiment: Why might tensors be especially effective for BSS
  when the number of sensors is close to the number of sources?
\end{enumerate}

\section{Chapter 18. Machine Learning and Deep
Models}\label{chapter-18.-machine-learning-and-deep-models}

\subsection{18.1 Convolutions as Multilinear
Maps}\label{convolutions-as-multilinear-maps}

Convolutions, a cornerstone of modern deep learning, are fundamentally
multilinear operations. While often introduced algorithmically (sliding
filters over data), they can be expressed neatly within the tensor
algebra framework.

\subsubsection{Convolution as a Tensor
Contraction}\label{convolution-as-a-tensor-contraction}

Consider a 1D convolution of an input signal \(x \in \mathbb{R}^n\) with
a kernel \(h \in \mathbb{R}^k\):

\[
y_i = \sum_{j=1}^k h_j \, x_{i-j}.
\]

This is a bilinear map: linear in both the input \(x\) and kernel \(h\).

\begin{itemize}
\tightlist
\item
  In higher dimensions (2D, 3D), the same structure holds: convolution =
  tensor contraction between input data and kernel.
\end{itemize}

\subsubsection{Convolution as a Multilinear
Operator}\label{convolution-as-a-multilinear-operator}

For 2D convolution (images):

\begin{itemize}
\item
  Input: \(X \in \mathbb{R}^{H \times W \times C}\) (height × width ×
  channels).
\item
  Kernel: \(K \in \mathbb{R}^{r \times s \times C \times M}\) (filter
  height × filter width × channels × output channels).
\item
  Output:

  \[
  Y_{i,j,m} = \sum_{p,q,c} K_{p,q,c,m} \, X_{i+p, j+q, c}.
  \]
\end{itemize}

This is a 4-way contraction across indices \(p,q,c\).

\subsubsection{Tensor Perspective
Benefits}\label{tensor-perspective-benefits}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unification: Convolution is just a structured multilinear map.
\item
  Efficiency: Frameworks optimize convolution via tensor contractions
  and reshaping into matrix multiplications (im2col trick).
\item
  Generalization: Other operations (cross-correlation, attention) are
  tensor contractions of similar form.
\end{enumerate}

\subsubsection{Connection to Low-Rank
Tensors}\label{connection-to-low-rank-tensors}

\begin{itemize}
\tightlist
\item
  Convolution kernels can be approximated by low-rank tensor
  decompositions (e.g., CP, Tucker).
\item
  This reduces parameters and speeds up training in deep neural
  networks.
\item
  Example: a 3D convolution kernel decomposed into separable 1D kernels.
\end{itemize}

\subsubsection{Applications Beyond Deep
Nets}\label{applications-beyond-deep-nets}

\begin{itemize}
\tightlist
\item
  Signal processing: filtering, denoising, feature extraction.
\item
  Physics: differential operators (Laplacian, wave equation) as
  convolutions.
\item
  Graphics: image blurring, sharpening, edge detection.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-48}

\begin{itemize}
\tightlist
\item
  Shows that convolutions are not ``magic,'' but structured tensor
  contractions.
\item
  Provides a natural bridge between deep learning and multilinear
  algebra.
\item
  Explains why tensor decompositions are effective for convolutional
  networks.
\end{itemize}

\subsubsection{Exercises}\label{exercises-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  1D Convolution: Write the convolution of \(x = (1,2,3,4)\) with
  \(h = (1,-1)\) explicitly.
\item
  Tensor Formulation: For a grayscale image
  \(X \in \mathbb{R}^{H \times W}\) and filter
  \(K \in \mathbb{R}^{r \times s}\), express convolution as a tensor
  contraction.
\item
  Kernel Decomposition: Show how a separable 2D kernel (rank-1 matrix)
  can be written as outer product of two 1D kernels.
\item
  Low-Rank Compression: Estimate parameter savings if a
  \(7 \times 7 \times 64 \times 128\) kernel is approximated by
  separable \(7 \times 1\) and \(1 \times 7\) filters.
\item
  Thought Experiment: Why might expressing convolutions as multilinear
  maps help in designing more efficient deep learning architectures?
\end{enumerate}

\subsection{18.2 Low-Rank Tensor Compression of
Nets}\label{low-rank-tensor-compression-of-nets}

Modern neural networks, especially convolutional and transformer-based
models, contain millions (or even billions) of parameters. Many of these
parameters are highly redundant. Low-rank tensor decompositions provide
a principled way to compress networks without losing much accuracy.

\subsubsection{Redundancy in Neural
Nets}\label{redundancy-in-neural-nets}

\begin{itemize}
\tightlist
\item
  Convolution kernels:
  \(K \in \mathbb{R}^{r \times s \times C_{\text{in}} \times C_{\text{out}}}\).
\item
  Fully connected layers: weight matrices
  \(W \in \mathbb{R}^{m \times n}\).
\item
  These often have effective rank much smaller than full dimensions.
\end{itemize}

\subsubsection{CP Decomposition for
Compression}\label{cp-decomposition-for-compression}

A convolutional kernel \(K\) can be approximated as:

\[
K \approx \sum_{i=1}^R a^{(1)}_i \otimes a^{(2)}_i \otimes a^{(3)}_i \otimes a^{(4)}_i,
\]

where \(R\) is small.

\begin{itemize}
\tightlist
\item
  Reduces parameters from
  \(r \cdot s \cdot C_{\text{in}} \cdot C_{\text{out}}\) to about
  \(R(r+s+C_{\text{in}}+C_{\text{out}})\).
\end{itemize}

\subsubsection{Tucker Decomposition for
Compression}\label{tucker-decomposition-for-compression}

Factorize kernel as:

\[
K \approx G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)} \times_4 U^{(4)},
\]

where \(G\) is a small core tensor.

\begin{itemize}
\tightlist
\item
  Allows flexible rank choices along each mode.
\item
  Often used for compressing fully connected layers.
\end{itemize}

\subsubsection{Tensor Train (TT) for
Compression}\label{tensor-train-tt-for-compression}

Large fully connected layers \(W \in \mathbb{R}^{m \times n}\) can be
reshaped into a high-order tensor and approximated in TT format.

\begin{itemize}
\tightlist
\item
  Parameters scale as \(\mathcal{O}(d r^2 n)\) instead of
  \(\mathcal{O}(mn)\).
\item
  Enables deployment of large models on resource-limited devices.
\end{itemize}

\subsubsection{Applications in Deep
Learning}\label{applications-in-deep-learning}

\begin{itemize}
\tightlist
\item
  CNNs: low-rank approximations of convolution filters.
\item
  Transformers: compress attention matrices with tensor decomposition.
\item
  Mobile AI: deploy compressed models on smartphones or edge devices.
\end{itemize}

\subsubsection{Trade-offs}\label{trade-offs}

\begin{itemize}
\tightlist
\item
  Pros: fewer parameters, lower memory, faster inference.
\item
  Cons: extra decomposition step, potential accuracy loss if ranks are
  too low, need for retraining/fine-tuning.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-49}

\begin{itemize}
\tightlist
\item
  Low-rank tensor methods make deep learning more efficient and
  accessible.
\item
  They link classical multilinear algebra directly with modern AI
  engineering.
\item
  Provide theoretical tools to understand redundancy and
  overparameterization.
\end{itemize}

\subsubsection{Exercises}\label{exercises-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parameter Counting: Compare parameter count of a
  \(7 \times 7 \times 64 \times 128\) convolution kernel vs.~CP
  decomposition with rank \(R=20\).
\item
  Tucker Compression: Suppose a kernel has shape
  \(10 \times 10 \times 32 \times 64\). If Tucker ranks are
  \((5,5,10,10)\), how many parameters are needed (core + factors)?
\item
  TT Format: Explain how reshaping a \(1024 \times 1024\) weight matrix
  into a 4th-order tensor enables TT compression.
\item
  Accuracy vs.~Efficiency: Why might too aggressive a low-rank
  approximation harm model accuracy?
\item
  Thought Experiment: Could a neural net be trained directly in
  compressed tensor form, instead of compressing after training? What
  might be the advantages? \#\#\# 18.3 Attention as Tensor Contractions
\end{enumerate}

The attention mechanism, central to transformer models, can be seen as a
sequence of structured tensor contractions. Expressing attention in
multilinear algebra terms clarifies both its efficiency and its
flexibility.

\subsubsection{Standard Attention
Formula}\label{standard-attention-formula}

Given queries \(Q \in \mathbb{R}^{n \times d}\), keys
\(K \in \mathbb{R}^{m \times d}\), and values
\(V \in \mathbb{R}^{m \times d_v}\):

\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right) V.
\]

\begin{itemize}
\tightlist
\item
  \(QK^\top\): similarity scores between queries and keys.
\item
  Softmax: normalizes across keys.
\item
  Multiplication with \(V\): aggregates values.
\end{itemize}

\subsubsection{Tensor Contraction View}\label{tensor-contraction-view}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Similarity computation:

  \[
  S_{ij} = \sum_{k} Q_{ik} K_{jk},
  \]

  a contraction over feature index \(k\).
\item
  Weighted aggregation:

  \[
  O_{i\ell} = \sum_{j} \text{softmax}(S_{ij}) \, V_{j\ell}.
  \]
\end{enumerate}

Thus, attention = two contractions:

\begin{itemize}
\tightlist
\item
  Contract \(Q\) with \(K\) (dot product).
\item
  Contract softmax weights with \(V\).
\end{itemize}

\subsubsection{Multi-Head Attention as Blocked
Contractions}\label{multi-head-attention-as-blocked-contractions}

\begin{itemize}
\tightlist
\item
  Split \(Q, K, V\) into \(h\) heads (smaller feature dimensions).
\item
  Perform attention contraction in parallel for each head.
\item
  Concatenate results.
\end{itemize}

This is equivalent to block-structured tensor contractions.

\subsubsection{Low-Rank and Tensorized
Variants}\label{low-rank-and-tensorized-variants}

\begin{itemize}
\tightlist
\item
  Low-rank attention: approximate \(QK^\top\) with a low-rank
  factorization.
\item
  Tensorized attention: represent weights in CP/Tucker/TT form for
  efficiency.
\item
  Linear attention: replace full contraction with kernelized
  approximations.
\end{itemize}

\subsubsection{Applications and
Insights}\label{applications-and-insights}

\begin{itemize}
\tightlist
\item
  Shows attention is not ``black magic'' but structured multilinear
  algebra.
\item
  Explains why tensor decompositions reduce attention cost.
\item
  Connects attention with classical bilinear forms and projections.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-50}

\begin{itemize}
\tightlist
\item
  Brings transformers into the same framework as convolutions and
  regression.
\item
  Provides a language for designing efficient attention mechanisms.
\item
  Helps bridge deep learning architectures with tensor theory.
\end{itemize}

\subsubsection{Exercises}\label{exercises-64}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dot-Product Attention: Express \(S = QK^\top\) as an einsum
  contraction.
\item
  Aggregation Step: Show how multiplying softmax-normalized scores with
  \(V\) is another einsum contraction.
\item
  Multi-Head Splitting: If \(d=64\) and \(h=8\), what is the per-head
  dimension?
\item
  Low-Rank Trick: If \(QK^\top\) is approximated by \(Q(UV^\top)K^\top\)
  with rank \(r\), how does this reduce complexity?
\item
  Thought Experiment: Why might thinking of attention as a tensor
  contraction help design new transformer variants?
\end{enumerate}

\section{Chapter 19. Physics, Graphics, and
Beyond}\label{chapter-19.-physics-graphics-and-beyond}

\subsection{19.1 Stress/Strain Tensors}\label{stressstrain-tensors}

In physics and engineering, stress and strain are key concepts for
understanding how materials deform under forces. Both are naturally
expressed as second-order tensors, making them a classic application of
multilinear algebra.

\subsubsection{Strain Tensor
(Deformation)}\label{strain-tensor-deformation}

When a material is deformed, each point moves by a displacement vector
\(u(x)\).

\begin{itemize}
\tightlist
\item
  The strain tensor measures local stretching, compression, and shear.
\end{itemize}

\[
\varepsilon_{ij} = \tfrac{1}{2} \left( \frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i} \right).
\]

\begin{itemize}
\tightlist
\item
  Symmetric: \(\varepsilon_{ij} = \varepsilon_{ji}\).
\item
  Diagonal entries: stretching along axes.
\item
  Off-diagonal entries: shear distortions.
\end{itemize}

\subsubsection{Stress Tensor (Internal
Forces)}\label{stress-tensor-internal-forces}

The stress tensor \(\sigma_{ij}\) describes internal forces per unit
area inside a material.

\begin{itemize}
\tightlist
\item
  Defined so that force on a surface with normal \(n_j\) is
\end{itemize}

\[
f_i = \sigma_{ij} n_j.
\]

\begin{itemize}
\tightlist
\item
  Diagonal entries: normal stresses (compression/tension).
\item
  Off-diagonal entries: shear stresses.
\end{itemize}

\subsubsection{Hooke's Law (Linear
Elasticity)}\label{hookes-law-linear-elasticity}

Stress and strain are related by a 4th-order elasticity tensor \(C\):

\[
\sigma_{ij} = \sum_{k,l} C_{ijkl} \, \varepsilon_{kl}.
\]

\begin{itemize}
\tightlist
\item
  In isotropic materials, \(C\) depends only on two constants (Young's
  modulus and Poisson's ratio).
\item
  This is a bilinear relation between strain and stress tensors.
\end{itemize}

\subsubsection{Eigenvalues and Principal
Axes}\label{eigenvalues-and-principal-axes}

\begin{itemize}
\item
  Stress tensor \(\sigma\) can be diagonalized:

  \begin{itemize}
  \tightlist
  \item
    Eigenvalues = principal stresses.
  \item
    Eigenvectors = principal directions.
  \end{itemize}
\item
  Interpretation: directions along which stress is purely compressive or
  tensile.
\end{itemize}

\subsubsection{Applications}\label{applications-8}

\begin{itemize}
\tightlist
\item
  Civil engineering: bridge and building safety.
\item
  Mechanical engineering: design of engines, aircraft, machines.
\item
  Geophysics: stress in Earth's crust, earthquakes.
\item
  Medical imaging: elastography for tissue stiffness.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-51}

\begin{itemize}
\tightlist
\item
  Stress and strain are everyday tensor applications in engineering.
\item
  They demonstrate how multilinear algebra naturally describes geometry
  and physics.
\item
  Provide a tangible connection between abstract tensors and real-world
  forces.
\end{itemize}

\subsubsection{Exercises}\label{exercises-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Strain Calculation: For displacement field \(u(x,y) = (x+y, y)\),
  compute the strain tensor \(\varepsilon\).
\item
  Stress on a Plane: If

  \[
  \sigma = \begin{bmatrix} 10 & 2 \\ 2 & 5 \end{bmatrix}, \quad n = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
  \]

  compute the force vector \(f\).
\item
  Symmetry: Show that both stress and strain tensors are symmetric.
\item
  Principal Stresses: Find the eigenvalues of
  \(\sigma = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}\). Interpret
  them.
\item
  Thought Experiment: Why is it natural that stress and strain are
  tensors instead of just vectors?
\end{enumerate}

\subsection{19.2 Inertia Tensors and Principal
Axes}\label{inertia-tensors-and-principal-axes}

In mechanics, the moment of inertia describes how mass distribution
resists rotational motion. While for simple objects it is a scalar, in
general it is a second-order tensor - the inertia tensor.

\subsubsection{Definition of Inertia
Tensor}\label{definition-of-inertia-tensor}

For a rigid body with mass density \(\rho(\mathbf{r})\), the inertia
tensor is:

\[
I_{ij} = \int \left( \|\mathbf{r}\|^2 \delta_{ij} - r_i r_j \right) \rho(\mathbf{r}) \, dV,
\]

where \(\mathbf{r} = (x,y,z)\) is the position vector relative to the
chosen origin.

\begin{itemize}
\tightlist
\item
  \(I_{ij}\) encodes how difficult it is to rotate the body around axis
  \(i\).
\item
  Symmetric: \(I_{ij} = I_{ji}\).
\end{itemize}

\subsubsection{Angular Momentum and Kinetic
Energy}\label{angular-momentum-and-kinetic-energy}

For angular velocity \(\omega\):

\begin{itemize}
\item
  Angular momentum:

  \[
  \mathbf{L} = I \, \boldsymbol{\omega}.
  \]
\item
  Rotational kinetic energy:

  \[
  T = \tfrac{1}{2} \boldsymbol{\omega}^\top I \boldsymbol{\omega}.
  \]
\end{itemize}

Thus, \(I\) acts as the matrix linking angular velocity to angular
momentum.

\subsubsection{Principal Axes}\label{principal-axes}

\begin{itemize}
\item
  Inertia tensor can be diagonalized:

  \[
  I = P \Lambda P^\top,
  \]

  where \(\Lambda\) contains principal moments of inertia, and \(P\)
  gives principal axes.
\item
  Rotations about principal axes are ``decoupled'' and simpler to
  analyze.
\end{itemize}

\subsubsection{Examples}\label{examples-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solid sphere (mass \(M\), radius \(R\)):

  \[
  I = \tfrac{2}{5} M R^2 I_3.
  \]

  (isotropic: same inertia around all axes).
\item
  Thin rod (length \(L\), axis through center):

  \[
  I = \tfrac{1}{12} M L^2.
  \]
\item
  Rectangular box: inertia tensor has different diagonal entries
  depending on edge lengths.
\end{enumerate}

\subsubsection{Applications}\label{applications-9}

\begin{itemize}
\tightlist
\item
  Mechanical engineering: robotics, aerospace, vehicle dynamics.
\item
  Astronomy: rotation of planets, stability of satellites.
\item
  Computer graphics: simulating rigid-body dynamics in physics engines.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-52}

\begin{itemize}
\tightlist
\item
  Inertia tensors are a clear example of a physical system governed by
  symmetric tensors.
\item
  Principal axes give both mathematical elegance and practical insight
  (e.g., why objects tumble).
\item
  Connects linear algebra (eigenvalues) directly with physical motion.
\end{itemize}

\subsubsection{Exercises}\label{exercises-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rod Example: Compute the inertia tensor of a thin rod of length \(L\)
  and mass \(M\) lying along the \(x\)-axis.
\item
  Sphere Symmetry: Show that a solid sphere's inertia tensor is
  isotropic (same in all directions).
\item
  Principal Axes: Diagonalize

  \[
  I = \begin{bmatrix} 5 & 1 & 0 \\ 1 & 4 & 0 \\ 0 & 0 & 3 \end{bmatrix}.
  \]

  Interpret the eigenvalues.
\item
  Angular Momentum: For \(\omega = (1,0,0)\) and
  \(I = \mathrm{diag}(2,3,4)\), compute \(\mathbf{L}\).
\item
  Thought Experiment: Why are principal axes of inertia so useful in
  spacecraft design?
\end{enumerate}

\subsection{19.3 3D Graphics: Transforms and
Shading}\label{d-graphics-transforms-and-shading}

Computer graphics relies heavily on linear and multilinear algebra.
Behind every rendered image are tensor operations that handle
transformations, lighting, and shading.

\subsubsection{Homogeneous Coordinates and
Transforms}\label{homogeneous-coordinates-and-transforms}

\begin{itemize}
\item
  A 3D point \((x,y,z)\) is represented as a 4D vector \((x,y,z,1)\).
\item
  Transformations are represented as \(4 \times 4\) matrices:

  \begin{itemize}
  \tightlist
  \item
    Translation, rotation, scaling, perspective projection.
  \end{itemize}
\item
  Composition of transformations = matrix multiplication (tensor
  contraction).
\end{itemize}

Example:

\[
\begin{bmatrix}
x' \\ y' \\ z' \\ 1
\end{bmatrix}
= T R S
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}.
\]

\subsubsection{Lighting as a Tensor
Operation}\label{lighting-as-a-tensor-operation}

The Phong reflection model:

\[
I = k_a I_a + k_d (\mathbf{L} \cdot \mathbf{N}) I_d + k_s (\mathbf{R} \cdot \mathbf{V})^n I_s,
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{L}\) = light direction,
\item
  \(\mathbf{N}\) = surface normal,
\item
  \(\mathbf{R}\) = reflection direction,
\item
  \(\mathbf{V}\) = viewer direction.
\end{itemize}

Each dot product is a tensor contraction between vectors.

\subsubsection{Normals and
Transformations}\label{normals-and-transformations}

\begin{itemize}
\tightlist
\item
  Surface normals transform differently than points (using inverse
  transpose of transformation matrix).
\item
  Preserves correct shading under scaling/shearing.
\item
  Another case where raising/lowering indices (via metrics) appears in
  practice.
\end{itemize}

\subsubsection{Shading as Multilinear
Maps}\label{shading-as-multilinear-maps}

\begin{itemize}
\item
  Shading combines:

  \begin{itemize}
  \tightlist
  \item
    Light properties (color, intensity).
  \item
    Surface properties (material, texture).
  \item
    Geometry (normals, tangents).
  \end{itemize}
\item
  The mapping from these multi-way inputs to final pixel intensity is a
  multilinear function.
\end{itemize}

\subsubsection{Applications in Graphics
Pipelines}\label{applications-in-graphics-pipelines}

\begin{itemize}
\tightlist
\item
  Vertex shaders: apply transformations (matrix multiplications).
\item
  Fragment shaders: compute color via multilinear lighting models.
\item
  Physics-based rendering: more advanced tensor models (BRDFs, radiance
  fields).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-53}

\begin{itemize}
\tightlist
\item
  Brings tensor ideas into a domain familiar to many learners: 3D
  graphics.
\item
  Shows that rendering engines are, at heart, optimized tensor
  pipelines.
\item
  Builds intuition that tensor algebra is not abstract - it powers
  everyday technology (games, movies, VR).
\end{itemize}

\subsubsection{Exercises}\label{exercises-67}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Homogeneous Transform: Write the homogeneous transformation matrix for
  rotating 90° about the \(z\)-axis and then translating by (2,3,0).
\item
  Dot Product Lighting: Given \(\mathbf{L} = (0,0,1)\),
  \(\mathbf{N} = (0,0,1)\), compute the diffuse term in the Phong model.
\item
  Normal Transformation: Explain why a non-uniform scaling requires
  using the inverse transpose matrix to transform normals.
\item
  Matrix Composition: If \(M_1\) is a scaling matrix and \(M_2\) is a
  rotation, what is the composite transformation?
\item
  Thought Experiment: How might tensor decompositions (CP, Tucker) be
  used to compress lighting models or neural radiance fields (NeRFs)?
\end{enumerate}

\subsection{19.4 Quantum States and
Operators}\label{quantum-states-and-operators}

Quantum mechanics is one of the most natural playgrounds for multilinear
algebra: states, observables, and dynamics are all encoded as tensors.

\subsubsection{Quantum States as
Vectors}\label{quantum-states-as-vectors}

\begin{itemize}
\item
  A pure quantum state is a vector in a complex Hilbert space:

  \[
  |\psi\rangle \in \mathbb{C}^n.
  \]
\item
  Example: a single qubit is a vector in \(\mathbb{C}^2\):

  \[
  |\psi\rangle = \alpha |0\rangle + \beta |1\rangle, \quad |\alpha|^2 + |\beta|^2 = 1.
  \]
\end{itemize}

\subsubsection{Operators as Matrices (2nd-Order
Tensors)}\label{operators-as-matrices-2nd-order-tensors}

\begin{itemize}
\item
  Observables and dynamics are represented as linear operators
  (Hermitian or unitary matrices).
\item
  Measurement probabilities come from contractions:

  \[
  p = \langle \psi | A | \psi \rangle.
  \]
\end{itemize}

\subsubsection{Composite Systems and Tensor
Products}\label{composite-systems-and-tensor-products}

\begin{itemize}
\item
  Multi-particle systems live in the tensor product of state spaces.
\item
  Two qubits:

  \[
  \mathbb{C}^2 \otimes \mathbb{C}^2 = \mathbb{C}^4.
  \]
\item
  Example entangled state (Bell state):

  \[
  |\Phi^+\rangle = \tfrac{1}{\sqrt{2}} (|00\rangle + |11\rangle).
  \]
\end{itemize}

Entanglement is simply non-separability in tensor terms.

\subsubsection{Density Matrices (Mixed
States)}\label{density-matrices-mixed-states}

\begin{itemize}
\item
  General states described by density operator \(\rho\), a positive
  semidefinite Hermitian matrix with trace 1.
\item
  Expectation values:

  \[
  \langle A \rangle = \mathrm{Tr}(\rho A).
  \]
\end{itemize}

\subsubsection{Quantum Gates as Tensor
Maps}\label{quantum-gates-as-tensor-maps}

\begin{itemize}
\tightlist
\item
  Single-qubit gates: \(2 \times 2\) unitary matrices (e.g., Pauli
  matrices).
\item
  Multi-qubit gates: act via Kronecker (tensor) products.
\item
  Example: CNOT = a \(4 \times 4\) matrix acting on two-qubit states.
\end{itemize}

\subsubsection{Applications}\label{applications-10}

\begin{itemize}
\tightlist
\item
  Quantum computing: algorithms rely on tensor contractions for
  simulating circuits.
\item
  Quantum many-body physics: tensor networks (MPS, PEPS) compress
  exponential state spaces.
\item
  Chemistry: molecular states represented as high-order tensors.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-54}

\begin{itemize}
\tightlist
\item
  Shows how tensors form the mathematical backbone of quantum mechanics.
\item
  Explains entanglement as a tensor phenomenon.
\item
  Connects multilinear algebra directly with one of the most exciting
  modern sciences.
\end{itemize}

\subsubsection{Exercises}\label{exercises-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Qubit State: Write the state vector for a qubit in superposition
  \(|\psi\rangle = \tfrac{1}{\sqrt{2}}(|0\rangle + |1\rangle)\).
\item
  Operator Expectation: For Pauli-\(Z\) operator
  \(\sigma_z = \begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}\), compute
  \(\langle \psi | \sigma_z | \psi \rangle\) for \(|\psi\rangle\) above.
\item
  Tensor Product: Compute \(|0\rangle \otimes |1\rangle\) explicitly as
  a vector in \(\mathbb{C}^4\).
\item
  CNOT Gate: Write the \(4 \times 4\) matrix representation of the CNOT
  gate.
\item
  Thought Experiment: Why are tensor decompositions (MPS, PEPS, TT)
  essential for simulating quantum many-body systems efficiently?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Part IX. Glimpses Beyond This
Book}\label{part-ix.-glimpses-beyond-this-book}

\section{Chapter 20. Manifolds and Tensor Fields
(Preview)}\label{chapter-20.-manifolds-and-tensor-fields-preview}

\subsection{20.1 Tangent and Cotangent
Bundles}\label{tangent-and-cotangent-bundles}

So far, we treated tensors on vector spaces with fixed bases. In
geometry and physics, tensors often live on manifolds, where each point
has its own tangent space. The tangent and cotangent bundles provide the
foundation for tensor fields.

\subsubsection{Tangent Space at a Point}\label{tangent-space-at-a-point}

For a smooth manifold \(M\) and point \(p \in M\):

\begin{itemize}
\tightlist
\item
  The tangent space \(T_p M\) is the vector space of all possible
  velocity vectors of curves through \(p\).
\item
  Dimension of \(T_p M\) = dimension of \(M\).
\item
  Example: On a sphere \(S^2\), \(T_p S^2\) is the plane tangent to the
  sphere at \(p\).
\end{itemize}

\subsubsection{Tangent Bundle}\label{tangent-bundle}

The tangent bundle is the union of all tangent spaces:

\[
TM = \bigsqcup_{p \in M} T_p M.
\]

\begin{itemize}
\tightlist
\item
  A smooth manifold itself (of dimension \(2n\) if \(\dim M = n\)).
\item
  A point in \(TM\) is a pair \((p, v)\) with \(v \in T_p M\).
\end{itemize}

\subsubsection{Cotangent Space and
Bundle}\label{cotangent-space-and-bundle}

\begin{itemize}
\tightlist
\item
  The cotangent space \(T^*_p M\) is the dual space of \(T_p M\): linear
  functionals on tangent vectors.
\item
  Elements are called covectors (or differential 1-forms).
\item
  The cotangent bundle \(T^*M = \bigsqcup_{p \in M} T^*_p M\).
\end{itemize}

\subsubsection{Local Coordinates}\label{local-coordinates}

If \(M\) has coordinates \((x^1, \dots, x^n)\):

\begin{itemize}
\item
  Basis of tangent space: \(\{\partial/\partial x^i\}\).
\item
  Basis of cotangent space: \(\{dx^i\}\).
\item
  Any tangent vector \(v \in T_p M\):

  \[
  v = \sum_i v^i \frac{\partial}{\partial x^i}.
  \]
\item
  Any covector \(\omega \in T^*_p M\):

  \[
  \omega = \sum_i \omega_i dx^i.
  \]
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-55}

\begin{itemize}
\tightlist
\item
  Tangent and cotangent bundles generalize the vector/covector
  distinction from linear algebra to curved spaces.
\item
  They form the stage on which tensor fields (next sections) live.
\item
  Central in physics: tangent = velocities, cotangent = momenta.
\end{itemize}

\subsubsection{Exercises}\label{exercises-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Tangent Space: On the circle \(S^1\), describe the tangent space at
  the point \((1,0)\).
\item
  Cotangent Basis: In \(\mathbb{R}^2\) with coordinates \((x,y)\), what
  are the basis vectors of the tangent and cotangent spaces?
\item
  Pairing: For \(v = v^1 \partial/\partial x + v^2 \partial/\partial y\)
  and \(\omega = \omega_1 dx + \omega_2 dy\), compute \(\omega(v)\).
\item
  Bundle Structure: Explain why the tangent bundle of a 2D manifold has
  dimension 4.
\item
  Thought Experiment: Why is momentum naturally a covector (in
  \(T^*_p M\)) instead of a vector?
\end{enumerate}

\subsection{20.2 Tensor Fields and Coordinate
Changes}\label{tensor-fields-and-coordinate-changes}

So far we have defined tangent and cotangent spaces at a single point.
To do geometry and physics, we need tensors that vary smoothly across a
manifold. These are called tensor fields.

\subsubsection{Tensor Fields}\label{tensor-fields}

\begin{itemize}
\item
  A tensor field of type (r,s) assigns to each point \(p \in M\) a
  tensor

  \[
  T(p) \in (T_pM)^{\otimes r} \otimes (T^*_pM)^{\otimes s}.
  \]
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Vector field = (1,0) tensor field (assigns a tangent vector at each
    point).
  \item
    Covector field (1-form): (0,1) tensor field.
  \item
    Metric: (0,2) symmetric tensor field.
  \end{itemize}
\end{itemize}

\subsubsection{Coordinate Expressions}\label{coordinate-expressions}

If \((x^1,\dots,x^n)\) are local coordinates, then:

\begin{itemize}
\item
  A vector field:

  \[
  X = \sum_i X^i(x) \frac{\partial}{\partial x^i}.
  \]
\item
  A covector field:

  \[
  \omega = \sum_i \omega_i(x) dx^i.
  \]
\item
  A general (r,s) tensor field:

  \[
  T = \sum T^{i_1 \cdots i_r}{}_{j_1 \cdots j_s}(x)
  \frac{\partial}{\partial x^{i_1}} \otimes \cdots \otimes \frac{\partial}{\partial x^{i_r}}
  \otimes dx^{j_1} \otimes \cdots \otimes dx^{j_s}.
  \]
\end{itemize}

\subsubsection{Coordinate
Transformations}\label{coordinate-transformations}

If we change coordinates from \(x^i\) to \(\tilde{x}^j\):

\begin{itemize}
\item
  Basis vectors transform as

  \[
  \frac{\partial}{\partial \tilde{x}^j} = \sum_i \frac{\partial x^i}{\partial \tilde{x}^j} \frac{\partial}{\partial x^i}.
  \]
\item
  Dual basis transforms oppositely:

  \[
  d\tilde{x}^j = \sum_i \frac{\partial \tilde{x}^j}{\partial x^i} dx^i.
  \]
\item
  Tensor components transform with a mix of both rules (contravariant
  and covariant).
\end{itemize}

Example: For a (1,1) tensor field \(A^i{}_j\):

\[
\tilde{A}^i{}_j =
\frac{\partial \tilde{x}^i}{\partial x^p}
\frac{\partial x^q}{\partial \tilde{x}^j}
A^p{}_q.
\]

\subsubsection{Why This Matters}\label{why-this-matters-56}

\begin{itemize}
\tightlist
\item
  Tensor fields generalize the coordinate-free viewpoint: the object is
  intrinsic, components adapt to coordinates.
\item
  Physics laws are tensorial: their form is preserved under coordinate
  transformations.
\item
  Explains why tensors are the ``language of nature'' in relativity and
  continuum mechanics.
\end{itemize}

\subsubsection{Exercises}\label{exercises-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vector Field: Write the vector field
  \(X = x \frac{\partial}{\partial x} + y \frac{\partial}{\partial y}\)
  on \(\mathbb{R}^2\).
\item
  1-Form Field: Write the 1-form \(\omega = x \, dx + y \, dy\).
  Evaluate \(\omega(X)\) for the vector field above.
\item
  Transformation Rule: Show how a vector field \(X^i\) transforms under
  coordinate change \(x^i \mapsto \tilde{x}^j\).
\item
  Mixed Tensor: Verify the transformation law for a (1,1) tensor
  \(A^i{}_j\).
\item
  Thought Experiment: Why is it essential in physics that tensorial
  equations look the same in any coordinate system?
\end{enumerate}

\subsection{20.3 Covariant Derivatives and
Curvature}\label{covariant-derivatives-and-curvature}

On flat spaces like \(\mathbb{R}^n\), derivatives of vector fields are
straightforward. On curved manifolds, however, we cannot subtract
vectors at different points directly because they belong to different
tangent spaces. The covariant derivative solves this problem and leads
naturally to curvature.

\subsubsection{Covariant Derivative}\label{covariant-derivative}

\begin{itemize}
\tightlist
\item
  For a vector field \(X\) and another vector field \(Y\), the covariant
  derivative \(\nabla_X Y\) measures how \(Y\) changes along \(X\).
\item
  Unlike the usual derivative, \(\nabla_X Y \in T_p M\), so it lives in
  the tangent space at the same point.
\end{itemize}

In coordinates \((x^i)\):

\[
\nabla_i Y^j = \frac{\partial Y^j}{\partial x^i} + \Gamma^j_{ik} Y^k,
\]

where \(\Gamma^j_{ik}\) are the Christoffel symbols of the connection.

\subsubsection{Parallel Transport}\label{parallel-transport}

\begin{itemize}
\tightlist
\item
  Parallel transport moves a vector along a curve while keeping it ``as
  constant as possible.''
\item
  Depends on the connection.
\item
  In Euclidean space, this agrees with ordinary translation; on curved
  manifolds (like spheres), the result depends on the path.
\end{itemize}

\subsubsection{Curvature Tensor}\label{curvature-tensor}

The failure of parallel transport to be path-independent is measured by
the Riemann curvature tensor:

\[
R^i{}_{jkl} = \partial_k \Gamma^i_{jl} - \partial_l \Gamma^i_{jk}
+ \Gamma^i_{km} \Gamma^m_{jl} - \Gamma^i_{lm} \Gamma^m_{jk}.
\]

\begin{itemize}
\tightlist
\item
  If \(R=0\), the manifold is flat (locally Euclidean).
\item
  Nonzero \(R\) encodes intrinsic curvature.
\end{itemize}

\subsubsection{Ricci Tensor and Scalar
Curvature}\label{ricci-tensor-and-scalar-curvature}

\begin{itemize}
\item
  Contracting indices of the Riemann tensor gives the Ricci tensor
  \(R_{ij}\).
\item
  Further contraction gives the scalar curvature \(R\).
\item
  Central in Einstein's field equations of general relativity:

  \[
  G_{ij} = R_{ij} - \tfrac{1}{2} g_{ij} R.
  \]
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-57}

\begin{itemize}
\tightlist
\item
  Covariant derivative generalizes differentiation to curved spaces.
\item
  Curvature is intrinsic: no embedding is needed to detect it.
\item
  These ideas connect multilinear algebra to geometry, relativity, and
  modern physics.
\end{itemize}

\subsubsection{Exercises}\label{exercises-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Flat Space: Show that in Euclidean coordinates, Christoffel symbols
  vanish and the covariant derivative reduces to the usual derivative.
\item
  Sphere Example: Explain why parallel transport around a closed loop on
  a sphere rotates a vector.
\item
  Riemann Tensor Symmetries: Verify that \(R^i{}_{jkl} = -R^i{}_{jlk}\).
\item
  Ricci Contraction: Show how to obtain \(R_{ij}\) from \(R^k{}_{ikj}\).
\item
  Thought Experiment: Why does general relativity require curvature,
  while Newtonian gravity does not?
\end{enumerate}

\section{Chapter 21. Representation Theory and Invariants
(Preview)}\label{chapter-21.-representation-theory-and-invariants-preview}

\subsection{21.1 Group Actions on Tensor
Spaces}\label{group-actions-on-tensor-spaces}

Symmetry plays a central role in mathematics and physics. Groups capture
symmetry, and their actions on tensor spaces reveal invariants and
structure. This section introduces how groups act on tensors and why
this matters.

\subsubsection{Group Actions}\label{group-actions}

\begin{itemize}
\item
  A group action of \(G\) on a vector space \(V\) is a map

  \[
  G \times V \to V, \quad (g,v) \mapsto g \cdot v,
  \]

  such that \(e \cdot v = v\) (identity acts trivially) and
  \((gh)\cdot v = g\cdot(h\cdot v)\).
\item
  Example: The rotation group \(SO(3)\) acts on \(\mathbb{R}^3\) by
  matrix multiplication.
\end{itemize}

\subsubsection{Group Actions on Tensor
Products}\label{group-actions-on-tensor-products}

If a group \(G\) acts on \(V\), then it acts naturally on tensor powers:

\[
g \cdot (v_1 \otimes v_2 \otimes \cdots \otimes v_k)
= (g \cdot v_1) \otimes (g \cdot v_2) \otimes \cdots \otimes (g \cdot v_k).
\]

Thus, tensors inherit group actions from their underlying vector spaces.

\subsubsection{Examples}\label{examples-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rotations on Vectors: In physics, tensors transform under coordinate
  rotations. Stress and strain tensors are invariant laws expressed
  under such actions.
\item
  Permutation Group: Acts on tensor indices by permuting them. Leads to
  symmetric and antisymmetric tensors.
\item
  General Linear Group \(GL(n)\): Natural action on \(\mathbb{R}^n\).
  Extends to higher-order tensors, explaining transformation rules under
  basis changes.
\end{enumerate}

\subsubsection{Why Group Actions Matter}\label{why-group-actions-matter}

\begin{itemize}
\tightlist
\item
  Provide the language for defining invariants: tensorial equations
  remain true under group actions.
\item
  In physics: laws of nature are symmetric under rotations, Lorentz
  transformations, gauge groups.
\item
  In data science: invariance to permutations, rotations, or scalings
  improves model generalization.
\end{itemize}

\subsubsection{Tensor Invariants}\label{tensor-invariants}

\begin{itemize}
\tightlist
\item
  A tensor is invariant under a group if \(g \cdot T = T\) for all
  \(g \in G\).
\item
  Example: the Euclidean inner product \(\langle x,y\rangle\) is
  invariant under \(SO(n)\).
\item
  Determinants, traces, and volume forms are other classical invariants.
\end{itemize}

\subsubsection{Exercises}\label{exercises-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vector Rotation: Show that the Euclidean norm
  \(\|x\|^2 = x_1^2 + x_2^2 + x_3^2\) is invariant under \(SO(3)\).
\item
  Permutation Action: Describe how the permutation \((12)\) acts on a
  tensor \(T_{ijk}\).
\item
  GL(n) Action: For a (1,1) tensor \(A^i{}_j\), write its transformation
  rule under \(GL(n)\).
\item
  Invariant Tensor: Prove that the Kronecker delta \(\delta_{ij}\) is
  invariant under orthogonal transformations.
\item
  Thought Experiment: Why is invariance under certain group actions a
  guiding principle in formulating physical laws?
\end{enumerate}

\subsection{21.2 Invariant Tensors and
Symmetry}\label{invariant-tensors-and-symmetry}

Invariant tensors capture quantities that remain unchanged under group
actions. They are the backbone of conservation laws in physics,
canonical forms in mathematics, and inductive biases in machine
learning.

\subsubsection{Definition}\label{definition-3}

A tensor \(T\) is invariant under a group \(G\) if

\[
g \cdot T = T \quad \text{for all } g \in G.
\]

\begin{itemize}
\tightlist
\item
  Example: Under the rotation group \(SO(n)\), the Kronecker delta
  \(\delta_{ij}\) is invariant.
\item
  Example: The Levi-Civita symbol \(\varepsilon_{ijk}\) is invariant
  under \(SO(3)\) but changes sign under reflections.
\end{itemize}

\subsubsection{Classical Invariant
Tensors}\label{classical-invariant-tensors}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inner Product: \(\langle x, y \rangle = \delta_{ij} x^i y^j\) is
  invariant under orthogonal transformations.
\item
  Determinant: Expressed with \(\varepsilon_{i_1 i_2 \dots i_n}\),
  invariant under \(SL(n)\) (special linear group).
\item
  Volume Form: Orientation-preserving transformations preserve volume.
\item
  Metric Tensor \(g_{ij}\): Fundamental invariant under coordinate
  changes, defining distances.
\end{enumerate}

\subsubsection{Invariant Theory}\label{invariant-theory}

\begin{itemize}
\tightlist
\item
  Studies polynomial functions or tensors that remain unchanged under
  group actions.
\item
  Example: Symmetric polynomials invariant under the permutation group
  \(S_n\).
\item
  Example: Trace and determinant are invariants under conjugation by
  \(GL(n)\).
\end{itemize}

\subsubsection{Physics Examples}\label{physics-examples}

\begin{itemize}
\tightlist
\item
  Conservation of energy and momentum ↔ invariance under time and space
  translations (Noether's theorem).
\item
  Angular momentum ↔ invariance under rotations.
\item
  Gauge invariants define observable quantities in quantum field theory.
\end{itemize}

\subsubsection{Applications in Data Science \&
ML}\label{applications-in-data-science-ml}

\begin{itemize}
\tightlist
\item
  Designing models invariant to certain transformations (e.g.,
  convolutional nets = translation invariance).
\item
  Graph neural networks = invariance under node permutations.
\item
  Tensor methods enforce symmetries directly in architecture.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-58}

\begin{itemize}
\tightlist
\item
  Invariants identify essential quantities independent of coordinates or
  representation.
\item
  Symmetry reduces complexity: from many possible features to a few
  invariant ones.
\item
  This unifies physics, pure math, and modern AI design.
\end{itemize}

\subsubsection{Exercises}\label{exercises-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rotation Invariance: Show explicitly that \(\delta_{ij}\) is unchanged
  under rotation matrices \(R \in SO(3)\).
\item
  Levi-Civita: Verify that \(\varepsilon_{ijk}\) changes sign under
  reflection (determinant = --1).
\item
  Determinant: Why is the determinant invariant under \(SL(n)\) but not
  under all of \(GL(n)\)?
\item
  Graph Example: Explain why node permutation invariance is crucial in
  graph neural networks.
\item
  Thought Experiment: Why do invariants often correspond to conserved
  physical quantities?
\end{enumerate}

\subsection{21.3 Why Invariants Matter in
Algorithms}\label{why-invariants-matter-in-algorithms}

Invariants are not just elegant mathematical objects - they are
practical tools that make algorithms more robust, efficient, and
interpretable. When an algorithm respects the right invariants, it
exploits symmetry instead of fighting it.

\subsubsection{Invariants Reduce
Redundancy}\label{invariants-reduce-redundancy}

\begin{itemize}
\tightlist
\item
  Without invariants, algorithms may waste time learning the same thing
  in multiple coordinate systems.
\item
  Example: PCA uses covariance, which is rotation-invariant, so it
  doesn't matter how data is oriented.
\item
  Example: Graph algorithms often rely on permutation-invariant
  structures (degree, adjacency spectrum).
\end{itemize}

\subsubsection{Invariants Improve
Robustness}\label{invariants-improve-robustness}

\begin{itemize}
\tightlist
\item
  If an algorithm outputs the same result under symmetry
  transformations, results are stable and reproducible.
\item
  Example: CNNs are translation-invariant, making them robust to shifts
  in input images.
\item
  Example: Physics simulations rely on energy invariants for numerical
  stability.
\end{itemize}

\subsubsection{Invariants Simplify
Computation}\label{invariants-simplify-computation}

\begin{itemize}
\tightlist
\item
  Many invariants collapse high-dimensional data into simpler summaries.
\item
  Example: Determinant summarizes a matrix's volume-scaling property.
\item
  Example: Tensor contractions with invariant tensors (like
  \(\delta_{ij}\) or \(\varepsilon_{ijk}\)) simplify calculations.
\end{itemize}

\subsubsection{Algorithm Design via
Invariants}\label{algorithm-design-via-invariants}

\begin{itemize}
\tightlist
\item
  Signal processing: invariant features (moments, cumulants) used for
  blind source separation.
\item
  Machine learning: group-equivariant networks encode invariances
  (rotations, permutations).
\item
  Optimization: invariance-aware preconditioning improves convergence.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-59}

\begin{itemize}
\tightlist
\item
  Invariants let us design algorithms that generalize better, by
  focusing only on structure that truly matters.
\item
  They connect deep theory (group actions, tensor algebra) with
  practical implementations (CNNs, GNNs, physics engines).
\item
  Understanding invariants is key to the next generation of geometry-
  and symmetry-aware AI systems.
\end{itemize}

\subsubsection{Exercises}\label{exercises-74}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Rotation-Invariant Feature: For a set of 2D points, explain why
  pairwise distances are invariant under rotations and translations.
\item
  Permutation-Invariance: Show that the sum of node features in a graph
  is invariant under permutations of node labels.
\item
  Algorithm Stability: Why does enforcing conservation of energy in a
  simulation improve long-term stability?
\item
  Invariant Contraction: Use \(\delta_{ij}\) to show that contracting
  \(A_{ij}\delta_{ij}\) yields the trace of \(A\), an invariant.
\item
  Thought Experiment: If a machine learning model ignores known
  invariances in the data, what are the risks?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{End Matter}\label{end-matter}

\section{A. Symbols and Notation
Cheatsheet}\label{a.-symbols-and-notation-cheatsheet}

This appendix gathers the most common symbols used throughout the book.
Use it as a quick reference when working through chapters and exercises.

\subsection{Vector Spaces and Duals}\label{vector-spaces-and-duals}

\begin{itemize}
\tightlist
\item
  \(V, W, U\) - vector spaces
\item
  \(\dim V\) - dimension of \(V\)
\item
  \(V^*\) - dual space (space of linear functionals on \(V\))
\item
  \(v \in V\) - vector
\item
  \(\alpha \in V^*\) - covector (linear form)
\end{itemize}

\subsection{Tensors}\label{tensors}

\begin{itemize}
\tightlist
\item
  \(T^r_s(V)\) - space of tensors of type (r,s) over \(V\)
\item
  \(u \otimes v\) - tensor (outer) product of \(u\) and \(v\)
\item
  \(T_{i_1 \dots i_s}^{j_1 \dots j_r}\) - components of a (r,s) tensor
\item
  Contraction - summing over one upper and one lower index
\item
  Symmetric / antisymmetric tensors - invariant or alternating under
  index permutations
\end{itemize}

\subsection{Indices and Summation}\label{indices-and-summation}

\begin{itemize}
\tightlist
\item
  \(i,j,k,l,m,n\) - generic indices
\item
  Einstein summation convention - repeated upper/lower indices are
  summed over
\item
  \(\delta_{ij}\) - Kronecker delta, identity for index contraction
\item
  \(\varepsilon_{ijk}\) - Levi-Civita symbol (totally antisymmetric,
  used for cross products, determinants)
\end{itemize}

\subsection{Linear Maps and Operators}\label{linear-maps-and-operators}

\begin{itemize}
\tightlist
\item
  \(A: V \to W\) - linear map from \(V\) to \(W\)
\item
  \(A^i{}_j\) - components of a (1,1) tensor, i.e., a linear operator
\item
  \(\mathrm{tr}(A)\) - trace of operator \(A\)
\item
  \(\det(A)\) - determinant of \(A\)
\end{itemize}

\subsection{Tensor Operations}\label{tensor-operations}

\begin{itemize}
\tightlist
\item
  \(\otimes\) - tensor product
\item
  \(\wedge\) - wedge product (alternating tensor product)
\item
  \(\times_n\) - mode-\(n\) product of a tensor with a matrix
\item
  \(\mathrm{vec}(\cdot)\) - vectorization operator (flatten tensor into
  a vector)
\item
  \(\mathrm{Tr}(\cdot)\) - matrix trace
\item
  \(\nabla\) - gradient / covariant derivative
\end{itemize}

\subsection{Special Objects}\label{special-objects}

\begin{itemize}
\tightlist
\item
  \(g_{ij}\) - metric tensor
\item
  \(R^i{}_{jkl}\) - Riemann curvature tensor
\item
  \(R_{ij}\) - Ricci tensor
\item
  \(R\) - scalar curvature
\item
  \(|v|\) or \(\|v\|\) - norm of a vector
\item
  \(\langle u, v \rangle\) - inner product of \(u\) and \(v\)
\end{itemize}

\section{Appendix B. Proof Sketches of Core
Theorems}\label{appendix-b.-proof-sketches-of-core-theorems}

\subsection{Appendix B.1 Universal Property of the Tensor Product (Proof
Sketch)}\label{appendix-b.1-universal-property-of-the-tensor-product-proof-sketch}

The tensor product \(V \otimes W\) is not just a convenient
construction: it is characterized uniquely by its universal property.
This section gives an intuitive sketch of the proof.

\subsubsection{Statement of the Universal
Property}\label{statement-of-the-universal-property}

Given vector spaces \(V, W\) over a field \(\mathbb{F}\):

\begin{itemize}
\item
  There exists a vector space \(V \otimes W\) together with a bilinear
  map

  \[
  \otimes : V \times W \to V \otimes W
  \]

  such that:
\end{itemize}

For any bilinear map \(f : V \times W \to U\) into another vector space
\(U\), there exists a unique linear map

\[
\tilde{f}: V \otimes W \to U
\]

satisfying

\[
f(v,w) = \tilde{f}(v \otimes w).
\]

\subsubsection{Idea of the Proof}\label{idea-of-the-proof}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Build a candidate space:

  \begin{itemize}
  \item
    Start with the free vector space generated by pairs \((v,w)\).
  \item
    Impose relations to enforce bilinearity:

    \begin{itemize}
    \tightlist
    \item
      \((v_1+v_2, w) \sim (v_1, w) + (v_2, w)\)
    \item
      \((v, w_1+w_2) \sim (v, w_1) + (v, w_2)\)
    \item
      \((\alpha v, w) \sim \alpha (v,w)\),
      \((v, \alpha w) \sim \alpha (v,w)\).
    \end{itemize}
  \end{itemize}

  The quotient space is defined as \(V \otimes W\).
\item
  Define the canonical bilinear map:

  \[
  (v,w) \mapsto v \otimes w.
  \]
\item
  Factorization property:

  \begin{itemize}
  \item
    For any bilinear map \(f: V \times W \to U\), define \(\tilde{f}\)
    by

    \[
    \tilde{f}(v \otimes w) = f(v,w).
    \]
  \item
    This is well-defined because the relations in step 1 match the
    bilinear properties of \(f\).
  \end{itemize}
\item
  Uniqueness:

  \begin{itemize}
  \tightlist
  \item
    Any linear map \(\tilde{f}\) satisfying the condition must agree on
    generators \(v \otimes w\).
  \item
    Since these generate the whole space, \(\tilde{f}\) is unique.
  \end{itemize}
\end{enumerate}

\subsubsection{Why This Matters}\label{why-this-matters-60}

\begin{itemize}
\tightlist
\item
  The tensor product is defined *not- by coordinates but by this
  universal property.
\item
  It guarantees that tensors are the natural home for bilinear (and
  multilinear) maps.
\item
  In applications, this explains why changing bases, reshaping, or
  contracting tensors always behaves consistently.
\end{itemize}

\subsubsection{Exercises}\label{exercises-75}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Multiplication as a Bilinear Map: Show that the bilinear map
  \(f: \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}\),
  \(f(x,y) = x^\top A y\), factors through a linear map on
  \(\mathbb{R}^n \otimes \mathbb{R}^m\).
\item
  Dimension Formula: Using a basis \(\{e_i\}\) for \(V\) and \(\{f_j\}\)
  for \(W\), prove that \(\{e_i \otimes f_j\}\) is a basis of
  \(V \otimes W\).
\item
  Uniqueness Check: Why can't two different linear maps
  \(\tilde{f}_1, \tilde{f}_2 : V \otimes W \to U\) both satisfy
  \(f(v,w) = \tilde{f}(v \otimes w)\)?
\item
  Free Vector Space Analogy: Compare the construction of \(V \otimes W\)
  to the way free groups or free vector spaces are defined.
\item
  Thought Experiment: If the tensor product is defined via a universal
  property, why does this make it ``canonical'' (independent of
  choices)?
\end{enumerate}

\subsection{Appendix B.2 Dimension Formula for Tensor Products (Proof
Sketch)}\label{appendix-b.2-dimension-formula-for-tensor-products-proof-sketch}

The tensor product has a clean relationship between dimensions:

\[
\dim(V \otimes W) = \dim(V) \cdot \dim(W).
\]

Here's a sketch of why this is true.

\subsubsection{Step 1. Choose Bases}\label{step-1.-choose-bases}

\begin{itemize}
\tightlist
\item
  Let \(\{e_i\}_{i=1}^m\) be a basis for \(V\).
\item
  Let \(\{f_j\}_{j=1}^n\) be a basis for \(W\).
\end{itemize}

\subsubsection{Step 2. Construct Tensor
Basis}\label{step-2.-construct-tensor-basis}

\begin{itemize}
\tightlist
\item
  Consider all simple tensors \(e_i \otimes f_j\), with
  \(1 \leq i \leq m\), \(1 \leq j \leq n\).
\item
  There are exactly \(mn\) such tensors.
\end{itemize}

\subsubsection{Step 3. Spanning
Argument}\label{step-3.-spanning-argument}

\begin{itemize}
\item
  Any element of \(V \otimes W\) is a linear combination of simple
  tensors \(v \otimes w\).
\item
  Expanding \(v = \sum a_i e_i\), \(w = \sum b_j f_j\), we get

  \[
  v \otimes w = \sum_{i,j} a_i b_j \,(e_i \otimes f_j).
  \]
\item
  Therefore, all tensors are linear combinations of
  \(\{e_i \otimes f_j\}\).
\end{itemize}

\subsubsection{Step 4. Linear
Independence}\label{step-4.-linear-independence}

\begin{itemize}
\tightlist
\item
  Suppose \(\sum_{i,j} c_{ij} \, (e_i \otimes f_j) = 0\).
\item
  Apply the universal property with bilinear maps defined by coordinate
  projections.
\item
  One shows all coefficients \(c_{ij}\) must vanish.
\item
  Hence, \(\{e_i \otimes f_j\}\) is linearly independent.
\end{itemize}

\subsubsection{Conclusion}\label{conclusion}

\begin{itemize}
\tightlist
\item
  \(\{e_i \otimes f_j\}\) is a basis.
\item
  Thus, \(\dim(V \otimes W) = mn = \dim(V) \cdot \dim(W)\).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-61}

\begin{itemize}
\tightlist
\item
  Dimension formula ensures that tensor products don't ``create new
  degrees of freedom'' beyond combinations of existing bases.
\item
  Makes tensor products predictable: if \(V = \mathbb{R}^m\) and
  \(W = \mathbb{R}^n\), then \(V \otimes W \cong \mathbb{R}^{mn}\).
\item
  Explains why tensor reshaping between matrices, higher arrays, and
  Kronecker products is consistent.
\end{itemize}

\subsubsection{Exercises}\label{exercises-76}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basis Construction: For \(V = \mathbb{R}^2\) with basis
  \(\{e_1,e_2\}\) and \(W = \mathbb{R}^3\) with basis
  \(\{f_1,f_2,f_3\}\), explicitly list the basis of \(V \otimes W\).
\item
  Counting Dimensions: If \(\dim(V)=4\) and \(\dim(W)=5\), what is
  \(\dim(V \otimes W)\)?
\item
  Matrix Analogy: Show that \(V \otimes W\) with chosen bases is
  isomorphic to the space of \(m \times n\) matrices.
\item
  Independence Check: Prove linear independence of
  \(\{e_i \otimes f_j\}\) by applying a bilinear map
  \(f(e_i,f_j) = \delta_{ii_0}\delta_{jj_0}\).
\item
  Thought Experiment: How does this dimension formula generalize to
  \(V_1 \otimes V_2 \otimes \cdots \otimes V_k\)?
\end{enumerate}

\subsection{Appendix B.3 Decomposition of Tensors into Symmetric and
Antisymmetric Parts (Proof
Sketch)}\label{appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch}

Any tensor with two indices can be decomposed uniquely into a symmetric
and an antisymmetric component. This is a fundamental structural result
and often the first place where symmetry in tensors becomes useful.

\subsubsection{Statement}\label{statement}

Let \(T \in V \otimes V\) (a bilinear form, or a \((0,2)\)-tensor).
Then:

\[
T = \tfrac{1}{2}(T + T^\top) \;+\; \tfrac{1}{2}(T - T^\top),
\]

where

\begin{itemize}
\tightlist
\item
  \(S = \tfrac{1}{2}(T + T^\top)\) is symmetric,
\item
  \(A = \tfrac{1}{2}(T - T^\top)\) is antisymmetric, and the
  decomposition is unique.
\end{itemize}

\subsubsection{Proof Sketch}\label{proof-sketch}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Symmetrization and Antisymmetrization Operators

  \begin{itemize}
  \item
    Define the symmetrization operator:

    \[
    \text{Sym}(T)(u,v) = \tfrac{1}{2}[T(u,v) + T(v,u)].
    \]
  \item
    Define the antisymmetrization operator:

    \[
    \text{Alt}(T)(u,v) = \tfrac{1}{2}[T(u,v) - T(v,u)].
    \]
  \end{itemize}
\item
  Linearity and Decomposition

  \begin{itemize}
  \item
    Both operators are linear.
  \item
    For any \(u,v\):

    \[
    T(u,v) = \text{Sym}(T)(u,v) + \text{Alt}(T)(u,v).
    \]
  \end{itemize}
\item
  Uniqueness

  \begin{itemize}
  \tightlist
  \item
    Suppose \(T = S + A\) with \(S\) symmetric and \(A\) antisymmetric.
  \item
    Then \(S = \text{Sym}(T)\) and \(A = \text{Alt}(T)\).
  \item
    No other decomposition is possible, proving uniqueness.
  \end{itemize}
\end{enumerate}

\subsubsection{Example}\label{example-2}

Matrix form:

\[
T = \begin{bmatrix} 1 & 3 \\ 4 & 2 \end{bmatrix}.
\]

\begin{itemize}
\item
  Symmetric part:

  \[
  S = \tfrac{1}{2}\left(\begin{bmatrix} 1 & 3 \\ 4 & 2 \end{bmatrix} + \begin{bmatrix} 1 & 4 \\ 3 & 2 \end{bmatrix}\right)
  = \begin{bmatrix} 1 & 3.5 \\ 3.5 & 2 \end{bmatrix}.
  \]
\item
  Antisymmetric part:

  \[
  A = \tfrac{1}{2}\left(\begin{bmatrix} 1 & 3 \\ 4 & 2 \end{bmatrix} - \begin{bmatrix} 1 & 4 \\ 3 & 2 \end{bmatrix}\right)
  = \begin{bmatrix} 0 & -0.5 \\ 0.5 & 0 \end{bmatrix}.
  \]
\end{itemize}

So \(T = S + A\).

\subsubsection{Why This Matters}\label{why-this-matters-62}

\begin{itemize}
\tightlist
\item
  Symmetric tensors capture ``energy-like'' or metric quantities.
\item
  Antisymmetric tensors capture orientation, area, and rotational
  effects.
\item
  In physics: stress tensor = symmetric, electromagnetic field tensor =
  antisymmetric.
\end{itemize}

\subsubsection{Exercises}\label{exercises-77}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the Decomposition: Decompose

  \[
  T = \begin{bmatrix} 0 & 2 \\ -1 & 3 \end{bmatrix}
  \]

  into symmetric and antisymmetric parts.
\item
  Uniqueness Check: Why can't a nonzero symmetric matrix also be
  antisymmetric?
\item
  Dimension Argument: Show that in \(\mathbb{R}^n\):

  \begin{itemize}
  \tightlist
  \item
    Dimension of symmetric 2-tensors = \(\tfrac{n(n+1)}{2}\).
  \item
    Dimension of antisymmetric 2-tensors = \(\tfrac{n(n-1)}{2}\).
  \item
    Total adds up to \(n^2\).
  \end{itemize}
\item
  Application in Physics: Which parts of the strain tensor (from
  mechanics) and electromagnetic field tensor (from relativity)
  correspond to symmetric and antisymmetric parts?
\item
  Thought Experiment: Can you imagine a scenario where the antisymmetric
  part of a tensor carries more physical information than the symmetric
  part? \#\#\# Appendix B.4 Rank Decomposition for Matrices and
  Extension to CP Rank (Proof Sketch)
\end{enumerate}

The concept of rank begins with matrices and extends to higher-order
tensors. This appendix sketches the reasoning behind matrix rank
decomposition and how it generalizes to the Canonical Polyadic (CP)
decomposition for tensors.

\subsubsection{Matrix Rank
Decomposition}\label{matrix-rank-decomposition}

Statement: Any matrix \(A \in \mathbb{R}^{m \times n}\) of rank \(r\)
can be written as a sum of \(r\) rank-one matrices.

Formally:

\[
A = \sum_{k=1}^r u^{(k)} (v^{(k)})^\top,
\]

with \(u^{(k)} \in \mathbb{R}^m\), \(v^{(k)} \in \mathbb{R}^n\).

Sketch of Proof:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Column Space Basis:

  \begin{itemize}
  \tightlist
  \item
    Since \(\text{rank}(A) = r\), there exist \(r\) independent columns.
  \item
    Let \(u^{(1)}, \dots, u^{(r)}\) be these basis vectors.
  \end{itemize}
\item
  Expansion of Columns:

  \begin{itemize}
  \tightlist
  \item
    Each column of \(A\) is a linear combination of \(\{u^{(k)}\}\).
  \item
    Thus, \(A\) can be written as a sum of outer products between
    \(u^{(k)}\) and suitable coefficient vectors \(v^{(k)}\).
  \end{itemize}
\item
  Minimality:

  \begin{itemize}
  \tightlist
  \item
    No fewer than \(r\) terms suffice: otherwise, the column space
    dimension would drop below \(r\).
  \end{itemize}
\end{enumerate}

This proves the decomposition exists and is minimal.

\subsubsection{Extension to Tensors: CP
Decomposition}\label{extension-to-tensors-cp-decomposition}

For a 3rd-order tensor \(T \in \mathbb{R}^{I \times J \times K}\):

\[
T_{ijk} = \sum_{r=1}^R a^{(r)}_i \, b^{(r)}_j \, c^{(r)}_k,
\]

or equivalently,

\[
T = \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)}.
\]

\begin{itemize}
\tightlist
\item
  \(R\) = tensor rank (minimal number of rank-one tensors).
\item
  Unlike matrices, computing \(R\) is much harder (NP-hard in general).
\end{itemize}

\subsubsection{Key Differences Between Matrix Rank and Tensor
Rank}\label{key-differences-between-matrix-rank-and-tensor-rank}

\begin{itemize}
\tightlist
\item
  Matrices: rank \(\leq \min(m,n)\).
\item
  Tensors: rank can exceed dimensions, and uniqueness properties are
  more subtle.
\item
  Matrix SVD: always gives orthogonal decomposition.
\item
  Tensor CP: uniqueness requires conditions (e.g., Kruskal's condition).
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-63}

\begin{itemize}
\tightlist
\item
  Rank decomposition gives the conceptual backbone of low-rank
  approximation.
\item
  CP decomposition underlies applications in data compression,
  chemometrics, neuroscience, and machine learning.
\item
  Shows how a simple linear algebra property grows into a rich
  multilinear theory.
\end{itemize}

\subsubsection{Exercises}\label{exercises-78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix Rank-One Decomposition: Decompose

  \[
  A = \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}
  \]

  into rank-one matrices.
\item
  Minimality Check: Why can't the above \(A\) be written as a single
  rank-one matrix?
\item
  Tensor Example: Express the tensor
  \(T_{ijk} = \delta_{ij}\delta_{jk}\) (the identity cube) as a CP
  decomposition.
\item
  Rank Bound: Show that for \(T \in \mathbb{R}^{I \times J \times K}\),
  the rank is at most \(\min(IJ, JK, IK)\).
\item
  Thought Experiment: Why might tensor rank decomposition be
  \emph{harder- but also }more useful- than matrix rank decomposition in
  data science? \#\#\# Appendix B.5 Identifiability Conditions for CP
  Decomposition (Proof Sketch)
\end{enumerate}

One of the most important questions in tensor decomposition is
uniqueness: when is a CP (Canonical Polyadic) decomposition determined
uniquely (up to permutation and scaling)? This property is called
identifiability.

\subsubsection{Statement (Informal)}\label{statement-informal}

A CP decomposition

\[
T = \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)}
\]

is essentially unique (unique up to permutation and scaling of terms) if
the factor matrices \([a^{(1)} \ \dots \ a^{(R)}]\),
\([b^{(1)} \ \dots \ b^{(R)}]\), \([c^{(1)} \ \dots \ c^{(R)}]\) satisfy
certain rank conditions.

\subsubsection{Kruskal's Theorem (Key
Result)}\label{kruskals-theorem-key-result}

Let
\(A \in \mathbb{R}^{I \times R}, B \in \mathbb{R}^{J \times R}, C \in \mathbb{R}^{K \times R}\).
Define the Kruskal rank of a matrix \(M\), written \(k_M\), as the
maximum number such that every subset of \(k_M\) columns is linearly
independent.

Kruskal's condition: If

\[
k_A + k_B + k_C \geq 2R + 2,
\]

then the CP decomposition is unique (up to trivial indeterminacies).

\subsubsection{Sketch of Why This Works}\label{sketch-of-why-this-works}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Overcompleteness and Mixing:

  \begin{itemize}
  \tightlist
  \item
    Without conditions, many different sets of factors can represent the
    same tensor.
  \end{itemize}
\item
  Column Independence:

  \begin{itemize}
  \tightlist
  \item
    Kruskal rank ensures that subsets of columns are independent, ruling
    out hidden degeneracies.
  \end{itemize}
\item
  Balancing the Inequality:

  \begin{itemize}
  \tightlist
  \item
    The inequality ensures enough ``spread'' across modes so that
    factors can't be swapped or recombined into different
    decompositions.
  \end{itemize}
\item
  Result:

  \begin{itemize}
  \tightlist
  \item
    Any alternative decomposition must match the original one (up to
    permutation and scaling).
  \end{itemize}
\end{enumerate}

\subsubsection{Intuition}\label{intuition}

\begin{itemize}
\tightlist
\item
  For matrices, SVD gives uniqueness (up to rotations) because of
  orthogonality.
\item
  For tensors, uniqueness is subtler: CP decomposition is often more
  unique than matrix decompositions.
\item
  This uniqueness makes tensors powerful in applications like blind
  source separation and latent-variable modeling.
\end{itemize}

\subsubsection{Why This Matters}\label{why-this-matters-64}

\begin{itemize}
\tightlist
\item
  Guarantees that discovered latent factors have meaning.
\item
  Critical in chemometrics, neuroscience, psychometrics, and machine
  learning.
\item
  Explains why tensor methods succeed where matrix methods fail:
  uniqueness despite underdetermined settings.
\end{itemize}

\subsubsection{Exercises}\label{exercises-79}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Kruskal Rank: Compute the Kruskal rank of

  \[
  A = \begin{bmatrix}
  1 & 0 & 1 \\
  0 & 1 & 1 \\
  0 & 0 & 1
  \end{bmatrix}.
  \]
\item
  Uniqueness Check: Why is CP decomposition with \(R=1\) always unique?
\item
  Matrix vs Tensor: Compare uniqueness of matrix rank decomposition with
  tensor CP decomposition. Which one is stricter?
\item
  Practical Implication: In blind source separation, why is uniqueness
  of CP decomposition essential?
\item
  Thought Experiment: Why might identifiability be a \emph{blessing- in
  data analysis but a }curse- for numerical algorithms?
\end{enumerate}

\section{Appendix D. Identities \&
``Cookbook''}\label{appendix-d.-identities-cookbook}

This appendix collects the most frequently used identities in
multilinear algebra, matrix calculus, and tensor manipulation. They are
written in a quick-lookup style - proofs and derivations appear in the
main chapters.

\subsection{1. Kronecker Product \& Vec
Identities}\label{kronecker-product-vec-identities}

\begin{itemize}
\item
  Vectorization of matrix products:

  \[
  \mathrm{vec}(AXB) = (B^\top \otimes A)\, \mathrm{vec}(X).
  \]
\item
  Vec of outer product:

  \[
  \mathrm{vec}(uv^\top) = v \otimes u.
  \]
\item
  Mixed product property:

  \[
  (A \otimes B)(C \otimes D) = (AC) \otimes (BD),
  \]

  if dimensions match.
\end{itemize}

\subsection{2. Trace Tricks}\label{trace-tricks}

\begin{itemize}
\item
  Cyclic property:

  \[
  \mathrm{tr}(AB) = \mathrm{tr}(BA).
  \]
\item
  Frobenius inner product:

  \[
  \langle A,B \rangle = \mathrm{tr}(A^\top B).
  \]
\item
  Trace with Kronecker:

  \[
  \mathrm{tr}(A \otimes B) = \mathrm{tr}(A)\, \mathrm{tr}(B).
  \]
\end{itemize}

\subsection{3. Tensor Contractions}\label{tensor-contractions}

\begin{itemize}
\item
  Inner product of tensors:

  \[
  \langle T, U \rangle = \sum_{i_1,\dots,i_k} T_{i_1\dots i_k} U_{i_1\dots i_k}.
  \]
\item
  Contraction with \(\delta_{ij}\):

  \[
  A_{ij}\,\delta_{ij} = \mathrm{tr}(A).
  \]
\item
  Contraction with Levi-Civita (\(\varepsilon_{ijk}\)):

  \[
  \varepsilon_{ijk}\, u_j v_k = (u \times v)_i.
  \]
\end{itemize}

\subsection{4. Determinant \& Volumes}\label{determinant-volumes}

\begin{itemize}
\item
  Determinant via Levi-Civita:

  \[
  \det(A) = \sum_{i_1,\dots,i_n} \varepsilon_{i_1 \dots i_n}
  A_{1,i_1} A_{2,i_2} \cdots A_{n,i_n}.
  \]
\item
  Volume of parallelepiped (vectors \(v_1,\dots,v_n\)):

  \[
  \text{Vol} = |\det([v_1 \ \dots \ v_n])|.
  \]
\end{itemize}

\subsection{5. Differential \& Gradient
Identities}\label{differential-gradient-identities}

\begin{itemize}
\item
  Gradient of quadratic form:

  \[
  \nabla_x (x^\top A x) = (A + A^\top)x.
  \]
\item
  Matrix calculus rule:

  \[
  \nabla_X \,\mathrm{tr}(A^\top X) = A.
  \]
\item
  Log-det derivative:

  \[
  \nabla_X \log \det(X) = (X^{-1})^\top.
  \]
\end{itemize}

\subsection{6. Useful Einsum Patterns}\label{useful-einsum-patterns}

\begin{itemize}
\item
  Matrix multiplication:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C }\OperatorTok{=}\NormalTok{ np.einsum(}\StringTok{\textquotesingle{}ik,kj{-}\textgreater{}ij\textquotesingle{}}\NormalTok{, A, B)}
\end{Highlighting}
\end{Shaded}
\item
  Tensor contraction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.einsum(}\StringTok{\textquotesingle{}ijk,k{-}\textgreater{}ij\textquotesingle{}}\NormalTok{, T, x)}
\end{Highlighting}
\end{Shaded}
\item
  Inner product:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OperatorTok{=}\NormalTok{ np.einsum(}\StringTok{\textquotesingle{}i,i{-}\textgreater{}\textquotesingle{}}\NormalTok{, u, v)}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsection{7. Symmetrization /
Antisymmetrization}\label{symmetrization-antisymmetrization}

\begin{itemize}
\item
  Symmetrization of 2-tensor:

  \[
  S_{ij} = \tfrac{1}{2}(T_{ij} + T_{ji}).
  \]
\item
  Antisymmetrization:

  \[
  A_{ij} = \tfrac{1}{2}(T_{ij} - T_{ji}).
  \]
\item
  General \(k\):

  \[
  \text{Sym}(T) = \frac{1}{k!} \sum_{\pi \in S_k} T_{i_{\pi(1)} \dots i_{\pi(k)}}.
  \]
\end{itemize}

\section{Appendix E.1 Mini-Project: Implement CP Decomposition for Small
3-Way
Tensors}\label{appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors}

Goal: Learn how to compute a Canonical Polyadic (CP) decomposition of a
small 3-way tensor by implementing an algorithm step-by-step. This
project combines theory (tensor rank, factorization) with practice
(numerical methods).

\subsection{1. Background}\label{background}

\begin{itemize}
\item
  Recall: a 3rd-order tensor \(T \in \mathbb{R}^{I \times J \times K}\)
  has CP form

  \[
  T \approx \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)},
  \]

  where \(R\) is the target rank and
  \(a^{(r)} \in \mathbb{R}^I, b^{(r)} \in \mathbb{R}^J, c^{(r)} \in \mathbb{R}^K\).
\item
  CP decomposition generalizes SVD to higher-order tensors.
\item
  Unlike SVD, exact decomposition is not guaranteed for arbitrary
  tensors, so we usually solve an optimization problem.
\end{itemize}

\subsection{2. Implementation Plan}\label{implementation-plan}

Step 1: Generate a Synthetic Tensor

\begin{itemize}
\item
  Choose dimensions, e.g., \(I=J=K=4\).
\item
  Pick random factor matrices
  \(A \in \mathbb{R}^{I \times R}, B \in \mathbb{R}^{J \times R}, C \in \mathbb{R}^{K \times R}\).
\item
  Form a tensor:

  \[
  T_{ijk} = \sum_{r=1}^R A_{ir} B_{jr} C_{kr}.
  \]
\end{itemize}

Step 2: Alternating Least Squares (ALS) Algorithm

\begin{itemize}
\item
  Initialize random factor matrices \(\hat{A}, \hat{B}, \hat{C}\).
\item
  Repeat until convergence:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Fix \(\hat{B}, \hat{C}\), update \(\hat{A}\) by solving least
    squares.
  \item
    Fix \(\hat{A}, \hat{C}\), update \(\hat{B}\).
  \item
    Fix \(\hat{A}, \hat{B}\), update \(\hat{C}\).
  \end{enumerate}
\end{itemize}

Step 3: Evaluate Error

\begin{itemize}
\item
  Reconstruction error:

  \[
  \text{Error} = \frac{\|T - \hat{T}\|_F}{\|T\|_F}.
  \]
\item
  Stop when error falls below threshold (e.g., \(10^{-6}\)) or max
  iterations reached.
\end{itemize}

\subsection{3. Coding Hints}\label{coding-hints}

\begin{itemize}
\tightlist
\item
  Use NumPy (or PyTorch/JAX) for efficient tensor operations.
\item
  Reshape and unfold tensors along modes for least squares updates.
\item
  Use \texttt{numpy.linalg.lstsq} to solve linear systems.
\item
  Normalize columns of factors periodically to avoid numerical
  instability.
\end{itemize}

\subsection{4. Extensions (Optional)}\label{extensions-optional}

\begin{itemize}
\tightlist
\item
  Compare convergence for different ranks \(R\).
\item
  Add Gaussian noise to the tensor and see how well CP decomposition
  recovers the factors.
\item
  Test uniqueness: permute/scaled versions of factors should still
  reconstruct the tensor.
\item
  Visualize factor matrices as heatmaps.
\end{itemize}

\subsection{5. Deliverables}\label{deliverables}

\begin{itemize}
\item
  Working implementation of CP-ALS for 3-way tensors.
\item
  Plots of error vs.~iterations.
\item
  Short report (1--2 pages) discussing:

  \begin{itemize}
  \tightlist
  \item
    Accuracy of decomposition.
  \item
    Effect of rank choice.
  \item
    Challenges in convergence.
  \end{itemize}
\end{itemize}

By finishing this project, you'll understand CP decomposition at both
the conceptual (tensor rank, multilinearity) and practical (numerical
algorithms, implementation) levels.

\section{Appendix E.2 Mini-Project: Tucker Decomposition for Video
Compression}\label{appendix-e.2-mini-project-tucker-decomposition-for-video-compression}

Goal: Explore how Tucker decomposition can compress multi-way data
efficiently, using a small grayscale video (or synthetic 3D tensor).

\subsection{1. Background}\label{background-1}

\begin{itemize}
\item
  A video clip with \(F\) frames, each of size \(H \times W\), is
  naturally represented as a 3-way tensor:

  \[
  X \in \mathbb{R}^{F \times H \times W}.
  \]
\item
  Tucker decomposition:

  \[
  X \approx G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)},
  \]

  where

  \begin{itemize}
  \tightlist
  \item
    \(G\) is a smaller core tensor,
  \item
    \(U^{(1)}, U^{(2)}, U^{(3)}\) are factor matrices (orthogonal bases
    along each mode).
  \end{itemize}
\item
  This is a higher-order analogue of SVD (HOSVD).
\end{itemize}

\subsection{2. Implementation Plan}\label{implementation-plan-1}

Step 1: Data Preparation

\begin{itemize}
\tightlist
\item
  Option A: Load a small grayscale video (e.g.,
  \(30 \times 64 \times 64\): 30 frames, 64×64 pixels).
\item
  Option B: Generate synthetic video data (moving shapes, noise).
\end{itemize}

Step 2: Compute Tucker Decomposition (HOSVD)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Unfold \(X\) along each mode (frame, height, width).
\item
  Compute SVD of each unfolding.
\item
  Select leading \(r_1, r_2, r_3\) singular vectors for factor matrices
  \(U^{(1)}, U^{(2)}, U^{(3)}\).
\item
  Form core tensor:

  \[
  G = X \times_1 (U^{(1)})^\top \times_2 (U^{(2)})^\top \times_3 (U^{(3)})^\top.
  \]
\end{enumerate}

Step 3: Reconstruct Compressed Video

\begin{itemize}
\item
  Approximation:

  \[
  \hat{X} = G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}.
  \]
\item
  Compare reconstruction with original.
\end{itemize}

Step 4: Evaluate Compression

\begin{itemize}
\item
  Compression ratio:

  \[
  \text{CR} = \frac{\text{entries in original tensor}}{\text{entries in core + factors}}.
  \]
\item
  Reconstruction error:

  \[
  \text{Error} = \frac{\|X - \hat{X}\|_F}{\|X\|_F}.
  \]
\end{itemize}

\subsection{3. Coding Hints}\label{coding-hints-1}

\begin{itemize}
\tightlist
\item
  Use NumPy or Tensorly (\texttt{tensorly.decomposition.tucker}) to
  avoid low-level SVD coding.
\item
  Visualize reconstruction error by displaying frames before and after
  compression.
\item
  Try varying rank choices \((r_1,r_2,r_3)\).
\end{itemize}

\subsection{4. Extensions (Optional)}\label{extensions-optional-1}

\begin{itemize}
\tightlist
\item
  Compare Tucker compression with simple PCA on flattened frames.
\item
  Add noise to video and check whether Tucker decomposition captures
  underlying structure better than PCA.
\item
  Explore color video: treat as a 4-way tensor
  \(F \times H \times W \times 3\).
\item
  Implement randomized SVD for scalability.
\end{itemize}

\subsection{5. Deliverables}\label{deliverables-1}

\begin{itemize}
\item
  Code that performs Tucker decomposition on a small video dataset.
\item
  Plots:

  \begin{itemize}
  \tightlist
  \item
    Error vs.~compression ratio.
  \item
    Example frame (original vs.~compressed reconstruction).
  \end{itemize}
\item
  Short writeup explaining:

  \begin{itemize}
  \tightlist
  \item
    How compression works.
  \item
    Trade-off between rank choice and accuracy.
  \end{itemize}
\end{itemize}

By completing this project, you'll see how tensor decompositions reduce
storage and preserve structure in real data, and why multilinear methods
are superior to naive flattening.

\section{Appendix E.3 Mini-Project: Strain Tensor for a Rotating
Plate}\label{appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate}

Goal: Understand how the strain tensor captures deformation by computing
it for a simple rotating square plate. This bridges mechanics
(stress/strain) with tensor calculus.

\subsection{1. Background}\label{background-2}

\begin{itemize}
\item
  In continuum mechanics, the strain tensor measures local deformation
  of a material:

  \[
  \varepsilon_{ij} = \tfrac{1}{2}\left(\frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i}\right),
  \]

  where \(u(x)\) is the displacement field.
\item
  Pure rotation should produce no strain (only rigid-body motion).
\item
  This project demonstrates that principle.
\end{itemize}

\subsection{2. Setup: Rotating Plate}\label{setup-rotating-plate}

\begin{itemize}
\item
  Consider a 2D square plate centered at the origin.
\item
  Apply a small rotation by angle \(\theta\).
\item
  Displacement of a point \((x,y)\):

  \[
  u(x,y) = R(\theta)\begin{bmatrix} x \\ y \end{bmatrix} - \begin{bmatrix} x \\ y \end{bmatrix},
  \]

  where
  \(R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}\).
\end{itemize}

\subsection{3. Compute Strain Tensor}\label{compute-strain-tensor}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Expand \(u(x,y)\):

  \[
  u_1(x,y) = (\cos\theta - 1)x - (\sin\theta)y,
  \]

  \[
  u_2(x,y) = (\sin\theta)x + (\cos\theta - 1)y.
  \]
\item
  Compute partial derivatives:

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\partial u_1}{\partial x} = \cos\theta - 1,\)
  \item
    \(\frac{\partial u_1}{\partial y} = -\sin\theta,\)
  \item
    \(\frac{\partial u_2}{\partial x} = \sin\theta,\)
  \item
    \(\frac{\partial u_2}{\partial y} = \cos\theta - 1.\)
  \end{itemize}
\item
  Build strain tensor:

  \[
  \varepsilon = \tfrac{1}{2}\begin{bmatrix}
  2(\cos\theta - 1) & (-\sin\theta + \sin\theta) \\
  (\sin\theta - \sin\theta) & 2(\cos\theta - 1)
  \end{bmatrix}.
  \]
\item
  Simplify:

  \[
  \varepsilon = (\cos\theta - 1) I,
  \]

  which vanishes when \(\theta\) is small (pure rotation ≈ no strain).
\end{enumerate}

\subsection{4. Interpretation}\label{interpretation}

\begin{itemize}
\tightlist
\item
  For infinitesimal \(\theta\), expand
  \(\cos\theta \approx 1 - \tfrac{1}{2}\theta^2\).
\item
  So strain \(\varepsilon \sim -\tfrac{1}{2}\theta^2 I\), i.e.,
  negligible for small angles.
\item
  Confirms physical intuition: rigid rotations produce no first-order
  strain.
\end{itemize}

\subsection{5. Coding Hints}\label{coding-hints-2}

\begin{itemize}
\tightlist
\item
  Implement in Python/NumPy: define displacement field, compute
  gradients symbolically (SymPy) or numerically (finite differences).
\item
  Visualize deformation arrows for a square grid of points.
\item
  Plot strain tensor components as heatmaps.
\end{itemize}

\subsection{6. Extensions (Optional)}\label{extensions-optional-2}

\begin{itemize}
\tightlist
\item
  Apply non-uniform deformation (e.g., shear or stretching) and compute
  strain.
\item
  Compare symmetric (strain) vs.~antisymmetric (rotation) parts of
  displacement gradient.
\item
  Extend to 3D cube rotation.
\end{itemize}

\subsection{7. Deliverables}\label{deliverables-2}

\begin{itemize}
\tightlist
\item
  Code computing strain tensor for rotating plate.
\item
  Visualization of deformation vs.~strain (showing nearly zero strain
  for pure rotation).
\item
  Short explanation connecting math to physical interpretation.
\end{itemize}

This project illustrates how the strain tensor isolates real
deformation, distinguishing it from rigid-body motion.

\section{Appendix E.4 Mini-Project: Parallel Transport of a Vector on a
Sphere}\label{appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere}

Goal: Visualize and compute parallel transport of a vector along a
closed path on a sphere, showing how curvature affects vector
orientation.

\subsection{1. Background}\label{background-3}

\begin{itemize}
\tightlist
\item
  On curved manifolds, moving a vector along a path while keeping it
  ``as constant as possible'' is called parallel transport.
\item
  On a flat plane: parallel transport around any loop returns the vector
  unchanged.
\item
  On a sphere: transporting a vector around a loop can change its
  orientation, revealing curvature.
\end{itemize}

\subsection{\texorpdfstring{2. Setup: The Sphere
\(S^2\)}{2. Setup: The Sphere S\^{}2}}\label{setup-the-sphere-s2}

\begin{itemize}
\tightlist
\item
  Sphere of radius \(R=1\), embedded in \(\mathbb{R}^3\).
\item
  Tangent space at a point \(p \in S^2\): plane orthogonal to \(p\).
\item
  Path: for simplicity, use a triangle on the sphere (e.g., along the
  equator and a meridian).
\end{itemize}

\subsection{3. Parallel Transport along
Geodesics}\label{parallel-transport-along-geodesics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose Starting Point:

  \begin{itemize}
  \tightlist
  \item
    \(p_0 = (0,0,1)\) (north pole).
  \item
    Tangent vector: \(v_0 = (1,0,0)\).
  \end{itemize}
\item
  Transport Path:

  \begin{itemize}
  \tightlist
  \item
    Move vector along a geodesic (great circle).
  \item
    Keep it tangent at each point.
  \item
    Algorithmically: project derivative back into tangent space.
  \end{itemize}
\item
  Closed Loop Example:

  \begin{itemize}
  \tightlist
  \item
    Move from north pole down to equator (longitude 0).
  \item
    Travel along equator by 90°.
  \item
    Return to north pole.
  \end{itemize}
\item
  Result:

  \begin{itemize}
  \tightlist
  \item
    Vector rotates relative to original orientation.
  \item
    Rotation angle equals the spherical excess of the triangle (area on
    sphere).
  \end{itemize}
\end{enumerate}

\subsection{4. Coding Hints}\label{coding-hints-3}

\begin{itemize}
\item
  Use Python + NumPy/Matplotlib.
\item
  Represent path as discrete points on sphere.
\item
  At each step:

  \begin{itemize}
  \item
    Move point along geodesic.
  \item
    Update vector by projecting back into tangent plane:

    \[
    v \gets v - (v \cdot p)p, \quad \text{then normalize}.
    \]
  \end{itemize}
\item
  Visualize trajectory of vector arrows along sphere using 3D plotting
  (\texttt{matplotlib.pyplot.quiver}).
\end{itemize}

\subsection{5. Extensions (Optional)}\label{extensions-optional-3}

\begin{itemize}
\tightlist
\item
  Experiment with different spherical triangles.
\item
  Compute holonomy angle = enclosed area × curvature (for unit sphere,
  curvature = 1).
\item
  Compare parallel transport on sphere vs.~flat plane (no change).
\item
  Extend to numerical geodesics on other manifolds (torus, hyperbolic
  surface).
\end{itemize}

\subsection{6. Deliverables}\label{deliverables-3}

\begin{itemize}
\tightlist
\item
  Code that simulates parallel transport on a sphere.
\item
  3D visualization of vector transport around loop.
\item
  Measurement of net rotation angle (compare with theory).
\item
  Short reflection: ``What does this reveal about curvature?''
\end{itemize}

This project gives tangible insight into how curvature manifests in
parallel transport, making an abstract tensorial concept geometrically
vivid.

\section{Appendix E.5 Mini-Project: PCA vs.~Tucker Decomposition on Real
Data}\label{appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data}

Goal: Compare Principal Component Analysis (PCA), which is matrix-based,
with Tucker decomposition, which is tensor-based, on a real dataset.
This project shows why multilinear approaches capture more structure
than flattening data.

\subsection{1. Background}\label{background-4}

\begin{itemize}
\tightlist
\item
  PCA: finds low-rank approximations of matrices (e.g., flatten images
  or videos into 2D).
\item
  Tucker decomposition: generalizes PCA to tensors, preserving multi-way
  structure.
\item
  Key question: Does Tucker give a better representation than PCA when
  applied to data with natural multi-dimensional structure (e.g.,
  images, videos, EEG signals)?
\end{itemize}

\subsection{2. Dataset Options}\label{dataset-options}

\begin{itemize}
\tightlist
\item
  Images: MNIST digits (28×28 grayscale images).
\item
  Video: small grayscale clip (frames × height × width).
\item
  Multichannel signals: EEG (time × channel × trial).
\end{itemize}

\subsection{3. Methodology}\label{methodology}

Step 1: Prepare Data

\begin{itemize}
\tightlist
\item
  For PCA: flatten each sample into a long vector.
\item
  For Tucker: keep data as tensor.
\end{itemize}

Step 2: Apply PCA

\begin{itemize}
\tightlist
\item
  Use \texttt{scikit-learn} PCA.
\item
  Choose top \(k\) components.
\item
  Reconstruct approximations of data.
\end{itemize}

Step 3: Apply Tucker Decomposition

\begin{itemize}
\tightlist
\item
  Use \texttt{tensorly.decomposition.tucker}.
\item
  Choose rank tuple \((r_1, r_2, r_3)\).
\item
  Reconstruct approximations.
\end{itemize}

Step 4: Compare Results

\begin{itemize}
\item
  Compute reconstruction error:

  \[
  \text{Error} = \frac{\|X - \hat{X}\|_F}{\|X\|_F}.
  \]
\item
  Compare compression ratio (\# of parameters stored).
\item
  Visualize original vs reconstructed samples.
\end{itemize}

\subsection{4. Coding Hints}\label{coding-hints-4}

\begin{itemize}
\tightlist
\item
  Libraries: \texttt{numpy}, \texttt{scikit-learn}, \texttt{tensorly}.
\item
  To compare fairly: match number of parameters used by PCA and Tucker.
\item
  For MNIST: display digits reconstructed with 5, 10, 20 components.
\item
  For Tucker: vary ranks (e.g., (10,10) vs (5,10,5)).
\end{itemize}

\subsection{5. Extensions (Optional)}\label{extensions-optional-4}

\begin{itemize}
\tightlist
\item
  Add Gaussian noise to data and test denoising power of PCA vs Tucker.
\item
  Compare runtime and scalability.
\item
  Try CP decomposition instead of Tucker.
\item
  Explore color images as 3-way tensors (height × width × channels).
\end{itemize}

\subsection{6. Deliverables}\label{deliverables-4}

\begin{itemize}
\item
  Code implementing both PCA and Tucker on chosen dataset.
\item
  Plots:

  \begin{itemize}
  \tightlist
  \item
    Error vs.~compression ratio.
  \item
    Side-by-side reconstruction images (original, PCA, Tucker).
  \end{itemize}
\item
  Short writeup:

  \begin{itemize}
  \tightlist
  \item
    Which method captures structure better?
  \item
    Does Tucker handle correlations across modes more effectively?
  \end{itemize}
\end{itemize}

This project highlights the advantage of tensor methods over flattening
approaches, making the case for multilinear models in data science.




\end{document}
