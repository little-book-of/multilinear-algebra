<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Duc-Tam Nguyen">
<meta name="dcterms.date" content="2025-09-05">

<title>The Little Book of Multilinear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-27c261d06b905028a18691de25d09dde.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#part-i.-orientation-motivation" id="toc-part-i.-orientation-motivation" class="nav-link active" data-scroll-target="#part-i.-orientation-motivation">Part I. Orientation &amp; Motivation</a>
  <ul class="collapse">
  <li><a href="#chapter-1.-what-is-multilinear" id="toc-chapter-1.-what-is-multilinear" class="nav-link" data-scroll-target="#chapter-1.-what-is-multilinear">Chapter 1. What is Multilinear?</a>
  <ul class="collapse">
  <li><a href="#linear-vs.-multilinear-from-lines-to-volumes" id="toc-linear-vs.-multilinear-from-lines-to-volumes" class="nav-link" data-scroll-target="#linear-vs.-multilinear-from-lines-to-volumes">1.1 Linear vs.&nbsp;Multilinear: From Lines to Volumes</a></li>
  <li><a href="#three-faces-of-a-tensor-array-map-and-element-of-a-product-space" id="toc-three-faces-of-a-tensor-array-map-and-element-of-a-product-space" class="nav-link" data-scroll-target="#three-faces-of-a-tensor-array-map-and-element-of-a-product-space">1.2 Three Faces of a Tensor: Array, Map, and Element of a Product Space</a></li>
  <li><a href="#why-it-matters-graphics-physics-ml-data-compression" id="toc-why-it-matters-graphics-physics-ml-data-compression" class="nav-link" data-scroll-target="#why-it-matters-graphics-physics-ml-data-compression">1.3 Why It Matters: Graphics, Physics, ML, Data Compression</a></li>
  </ul></li>
  <li><a href="#chapter-2.-minimal-prerequisites" id="toc-chapter-2.-minimal-prerequisites" class="nav-link" data-scroll-target="#chapter-2.-minimal-prerequisites">Chapter 2. Minimal Prerequisites</a>
  <ul class="collapse">
  <li><a href="#vector-spaces-bases-dimension" id="toc-vector-spaces-bases-dimension" class="nav-link" data-scroll-target="#vector-spaces-bases-dimension">2.1 Vector Spaces, Bases, Dimension</a></li>
  <li><a href="#linear-maps-matrices-change-of-basis" id="toc-linear-maps-matrices-change-of-basis" class="nav-link" data-scroll-target="#linear-maps-matrices-change-of-basis">2.2 Linear Maps, Matrices, Change of Basis</a></li>
  <li><a href="#exercises-5" id="toc-exercises-5" class="nav-link" data-scroll-target="#exercises-5">Exercises</a></li>
  <li><a href="#inner-products-dual-spaces-adjoint" id="toc-inner-products-dual-spaces-adjoint" class="nav-link" data-scroll-target="#inner-products-dual-spaces-adjoint">2.3 Inner Products, Dual Spaces, Adjoint</a></li>
  <li><a href="#bilinear-forms-and-quadratic-forms" id="toc-bilinear-forms-and-quadratic-forms" class="nav-link" data-scroll-target="#bilinear-forms-and-quadratic-forms">2.4 Bilinear Forms and Quadratic Forms</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-ii.-tensors" id="toc-part-ii.-tensors" class="nav-link" data-scroll-target="#part-ii.-tensors">Part II. Tensors</a>
  <ul class="collapse">
  <li><a href="#chapter-3.-tensors-as-indexed-arrays" id="toc-chapter-3.-tensors-as-indexed-arrays" class="nav-link" data-scroll-target="#chapter-3.-tensors-as-indexed-arrays">Chapter 3. Tensors as Indexed Arrays</a>
  <ul class="collapse">
  <li><a href="#order-arity-shape-and-indices" id="toc-order-arity-shape-and-indices" class="nav-link" data-scroll-target="#order-arity-shape-and-indices">3.1 Order (Arity), Shape, and Indices</a></li>
  <li><a href="#covariant-vs.-contravariant-indices" id="toc-covariant-vs.-contravariant-indices" class="nav-link" data-scroll-target="#covariant-vs.-contravariant-indices">3.2 Covariant vs.&nbsp;Contravariant Indices</a></li>
  <li><a href="#change-of-basis-rules-in-coordinates" id="toc-change-of-basis-rules-in-coordinates" class="nav-link" data-scroll-target="#change-of-basis-rules-in-coordinates">3.3 Change of Basis Rules in Coordinates</a></li>
  <li><a href="#einstein-summation-and-index-hygiene" id="toc-einstein-summation-and-index-hygiene" class="nav-link" data-scroll-target="#einstein-summation-and-index-hygiene">3.4 Einstein Summation and Index Hygiene</a></li>
  </ul></li>
  <li><a href="#chapter-4.-tensors-as-multilinear-maps" id="toc-chapter-4.-tensors-as-multilinear-maps" class="nav-link" data-scroll-target="#chapter-4.-tensors-as-multilinear-maps">Chapter 4. Tensors as Multilinear Maps</a>
  <ul class="collapse">
  <li><a href="#multilinearity-and-currying" id="toc-multilinearity-and-currying" class="nav-link" data-scroll-target="#multilinearity-and-currying">4.1 Multilinearity and Currying</a></li>
  <li><a href="#evaluation-with-vectors-and-covectors" id="toc-evaluation-with-vectors-and-covectors" class="nav-link" data-scroll-target="#evaluation-with-vectors-and-covectors">4.2 Evaluation with Vectors and Covectors</a></li>
  <li><a href="#from-maps-to-arrays-and-back-via-a-basis" id="toc-from-maps-to-arrays-and-back-via-a-basis" class="nav-link" data-scroll-target="#from-maps-to-arrays-and-back-via-a-basis">4.3 From Maps to Arrays (and Back) via a Basis</a></li>
  <li><a href="#universal-examples-bilinear-forms-trilinear-mixing" id="toc-universal-examples-bilinear-forms-trilinear-mixing" class="nav-link" data-scroll-target="#universal-examples-bilinear-forms-trilinear-mixing">4.4 Universal Examples: Bilinear Forms, Trilinear Mixing</a></li>
  </ul></li>
  <li><a href="#chapter-5.-tensors-as-elements-of-tensor-products" id="toc-chapter-5.-tensors-as-elements-of-tensor-products" class="nav-link" data-scroll-target="#chapter-5.-tensors-as-elements-of-tensor-products">Chapter 5. Tensors as Elements of Tensor Products</a>
  <ul class="collapse">
  <li><a href="#constructing-v-otimes-w-intuition-and-goals" id="toc-constructing-v-otimes-w-intuition-and-goals" class="nav-link" data-scroll-target="#constructing-v-otimes-w-intuition-and-goals">5.1 Constructing <span class="math inline">\(V \otimes W\)</span>: Intuition and Goals</a></li>
  <li><a href="#simple-vs.-general-tensors" id="toc-simple-vs.-general-tensors" class="nav-link" data-scroll-target="#simple-vs.-general-tensors">5.2 Simple vs.&nbsp;General Tensors</a></li>
  <li><a href="#dimension-and-bases-of-tensor-products" id="toc-dimension-and-bases-of-tensor-products" class="nav-link" data-scroll-target="#dimension-and-bases-of-tensor-products">5.3 Dimension and Bases of Tensor Products</a></li>
  <li><a href="#three-viewpoints-reconciled" id="toc-three-viewpoints-reconciled" class="nav-link" data-scroll-target="#three-viewpoints-reconciled">5.4 Three Viewpoints Reconciled</a></li>
  <li><a href="#exercises-19" id="toc-exercises-19" class="nav-link" data-scroll-target="#exercises-19">Exercises</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-iii.-core-operations-on-tensors" id="toc-part-iii.-core-operations-on-tensors" class="nav-link" data-scroll-target="#part-iii.-core-operations-on-tensors">Part III. Core Operations on Tensors</a>
  <ul class="collapse">
  <li><a href="#chapter-6.-building-blocks" id="toc-chapter-6.-building-blocks" class="nav-link" data-scroll-target="#chapter-6.-building-blocks">Chapter 6. Building Blocks</a>
  <ul class="collapse">
  <li><a href="#tensor-outer-product" id="toc-tensor-outer-product" class="nav-link" data-scroll-target="#tensor-outer-product">6.1 Tensor (Outer) Product</a></li>
  <li><a href="#contraction-summing-paired-indices" id="toc-contraction-summing-paired-indices" class="nav-link" data-scroll-target="#contraction-summing-paired-indices">6.2 Contraction: Summing Paired Indices</a></li>
  <li><a href="#permutations-of-modes-transpose-unfold-matricize" id="toc-permutations-of-modes-transpose-unfold-matricize" class="nav-link" data-scroll-target="#permutations-of-modes-transpose-unfold-matricize">6.3 Permutations of Modes: Transpose, Unfold, Matricize</a></li>
  <li><a href="#kronecker-product-vs.-tensor-product" id="toc-kronecker-product-vs.-tensor-product" class="nav-link" data-scroll-target="#kronecker-product-vs.-tensor-product">6.4 Kronecker Product vs.&nbsp;Tensor Product</a></li>
  </ul></li>
  <li><a href="#chapter-7.-symmetry-and-antisymmetry" id="toc-chapter-7.-symmetry-and-antisymmetry" class="nav-link" data-scroll-target="#chapter-7.-symmetry-and-antisymmetry">Chapter 7. Symmetry and (Anti)Symmetry</a>
  <ul class="collapse">
  <li><a href="#symmetrization-operators" id="toc-symmetrization-operators" class="nav-link" data-scroll-target="#symmetrization-operators">7.1 Symmetrization Operators</a></li>
  <li><a href="#alternation-antisymmetrization" id="toc-alternation-antisymmetrization" class="nav-link" data-scroll-target="#alternation-antisymmetrization">7.2 Alternation (Antisymmetrization)</a></li>
  <li><a href="#decompositions-by-symmetry-type" id="toc-decompositions-by-symmetry-type" class="nav-link" data-scroll-target="#decompositions-by-symmetry-type">7.3 Decompositions by Symmetry Type</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-iv.-exterior-algebra" id="toc-part-iv.-exterior-algebra" class="nav-link" data-scroll-target="#part-iv.-exterior-algebra">Part IV. Exterior Algebra</a>
  <ul class="collapse">
  <li><a href="#chapter-8.-wedge-products-and-k-vectors" id="toc-chapter-8.-wedge-products-and-k-vectors" class="nav-link" data-scroll-target="#chapter-8.-wedge-products-and-k-vectors">Chapter 8. Wedge Products and k-Vectors</a>
  <ul class="collapse">
  <li><a href="#the-wedge-product-and-geometric-meaning" id="toc-the-wedge-product-and-geometric-meaning" class="nav-link" data-scroll-target="#the-wedge-product-and-geometric-meaning">8.1 The Wedge Product and Geometric Meaning</a></li>
  <li><a href="#areas-volumes-orientation-determinant" id="toc-areas-volumes-orientation-determinant" class="nav-link" data-scroll-target="#areas-volumes-orientation-determinant">8.2 Areas, Volumes, Orientation, Determinant</a></li>
  <li><a href="#basis-dimension-grassmann-algebra" id="toc-basis-dimension-grassmann-algebra" class="nav-link" data-scroll-target="#basis-dimension-grassmann-algebra">8.3 Basis, Dimension, Grassmann Algebra</a></li>
  <li><a href="#differential-forms-gentle-preview" id="toc-differential-forms-gentle-preview" class="nav-link" data-scroll-target="#differential-forms-gentle-preview">8.4 Differential Forms (Gentle Preview)</a></li>
  </ul></li>
  <li><a href="#chapter-9.-hodge-dual-and-metrics-optional-but-useful" id="toc-chapter-9.-hodge-dual-and-metrics-optional-but-useful" class="nav-link" data-scroll-target="#chapter-9.-hodge-dual-and-metrics-optional-but-useful">Chapter 9. Hodge Dual and Metrics (Optional but Useful)</a>
  <ul class="collapse">
  <li><a href="#inner-products-on-exterior-powers" id="toc-inner-products-on-exterior-powers" class="nav-link" data-scroll-target="#inner-products-on-exterior-powers">9.1 Inner Products on Exterior Powers</a></li>
  <li><a href="#hodge-star-pseudo-vectors-cross-products" id="toc-hodge-star-pseudo-vectors-cross-products" class="nav-link" data-scroll-target="#hodge-star-pseudo-vectors-cross-products">9.2 Hodge Star, Pseudo-Vectors, Cross Products</a></li>
  <li><a href="#volume-forms-and-integration-glimpses" id="toc-volume-forms-and-integration-glimpses" class="nav-link" data-scroll-target="#volume-forms-and-integration-glimpses">9.3 Volume Forms and Integration Glimpses</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-v.-symmetric-tensors-and-polynomial-view" id="toc-part-v.-symmetric-tensors-and-polynomial-view" class="nav-link" data-scroll-target="#part-v.-symmetric-tensors-and-polynomial-view">Part V. Symmetric Tensors and Polynomial View</a>
  <ul class="collapse">
  <li><a href="#chapter-10.-symmetric-powers-and-homogeneous-polynomials" id="toc-chapter-10.-symmetric-powers-and-homogeneous-polynomials" class="nav-link" data-scroll-target="#chapter-10.-symmetric-powers-and-homogeneous-polynomials">Chapter 10. Symmetric Powers and Homogeneous Polynomials</a>
  <ul class="collapse">
  <li><a href="#symmetric-tensors-as-polynomial-coefficients" id="toc-symmetric-tensors-as-polynomial-coefficients" class="nav-link" data-scroll-target="#symmetric-tensors-as-polynomial-coefficients">10.1 Symmetric Tensors as Polynomial Coefficients</a></li>
  <li><a href="#polarization-identities" id="toc-polarization-identities" class="nav-link" data-scroll-target="#polarization-identities">10.2 Polarization Identities</a></li>
  <li><a href="#moments-and-cumulants" id="toc-moments-and-cumulants" class="nav-link" data-scroll-target="#moments-and-cumulants">10.3 Moments and Cumulants</a></li>
  <li><a href="#low-rank-symmetric-decompositions" id="toc-low-rank-symmetric-decompositions" class="nav-link" data-scroll-target="#low-rank-symmetric-decompositions">10.4 Low-Rank Symmetric Decompositions</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-vi.-linear-maps-between-tensor-spaces" id="toc-part-vi.-linear-maps-between-tensor-spaces" class="nav-link" data-scroll-target="#part-vi.-linear-maps-between-tensor-spaces">Part VI. Linear Maps Between Tensor Spaces</a>
  <ul class="collapse">
  <li><a href="#chapter-11.-reshaping-vectorization-and-commutation" id="toc-chapter-11.-reshaping-vectorization-and-commutation" class="nav-link" data-scroll-target="#chapter-11.-reshaping-vectorization-and-commutation">Chapter 11. Reshaping, Vectorization, and Commutation</a>
  <ul class="collapse">
  <li><a href="#mode-n-unfolding-and-matricization" id="toc-mode-n-unfolding-and-matricization" class="nav-link" data-scroll-target="#mode-n-unfolding-and-matricization">11.1 Mode-n Unfolding and Matricization</a></li>
  <li><a href="#vec-operator-and-kronecker-identities" id="toc-vec-operator-and-kronecker-identities" class="nav-link" data-scroll-target="#vec-operator-and-kronecker-identities">11.2 Vec Operator and Kronecker Identities</a></li>
  <li><a href="#linear-operators-acting-on-tensors" id="toc-linear-operators-acting-on-tensors" class="nav-link" data-scroll-target="#linear-operators-acting-on-tensors">11.3 Linear Operators Acting on Tensors</a></li>
  </ul></li>
  <li><a href="#chapter-12.-metrics-forms-and-raisinglowering-indices" id="toc-chapter-12.-metrics-forms-and-raisinglowering-indices" class="nav-link" data-scroll-target="#chapter-12.-metrics-forms-and-raisinglowering-indices">Chapter 12. Metrics, Forms, and Raising/Lowering Indices</a>
  <ul class="collapse">
  <li><a href="#using-inner-products-to-move-indices" id="toc-using-inner-products-to-move-indices" class="nav-link" data-scroll-target="#using-inner-products-to-move-indices">12.1 Using Inner Products to Move Indices</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-vii.-tensor-ranks-and-decompositions" id="toc-part-vii.-tensor-ranks-and-decompositions" class="nav-link" data-scroll-target="#part-vii.-tensor-ranks-and-decompositions">Part VII. Tensor Ranks and Decompositions</a>
  <ul class="collapse">
  <li><a href="#chapter-13.-ranks-for-tensors" id="toc-chapter-13.-ranks-for-tensors" class="nav-link" data-scroll-target="#chapter-13.-ranks-for-tensors">Chapter 13. Ranks for Tensors</a>
  <ul class="collapse">
  <li><a href="#matrix-rank-vs.-tensor-rank" id="toc-matrix-rank-vs.-tensor-rank" class="nav-link" data-scroll-target="#matrix-rank-vs.-tensor-rank">13.1 Matrix Rank vs.&nbsp;Tensor Rank</a></li>
  <li><a href="#multilinear-tucker-rank-and-mode-ranks" id="toc-multilinear-tucker-rank-and-mode-ranks" class="nav-link" data-scroll-target="#multilinear-tucker-rank-and-mode-ranks">13.2 Multilinear (Tucker) Rank and Mode Ranks</a></li>
  </ul></li>
  <li><a href="#chapter-14.-cannonical-decompositions" id="toc-chapter-14.-cannonical-decompositions" class="nav-link" data-scroll-target="#chapter-14.-cannonical-decompositions">Chapter 14. Cannonical Decompositions</a>
  <ul class="collapse">
  <li><a href="#cp-candecompparafac" id="toc-cp-candecompparafac" class="nav-link" data-scroll-target="#cp-candecompparafac">14.1 CP (CANDECOMP/PARAFAC)</a></li>
  <li><a href="#tucker-and-hosvd" id="toc-tucker-and-hosvd" class="nav-link" data-scroll-target="#tucker-and-hosvd">14.2 Tucker and HOSVD</a></li>
  <li><a href="#tensor-trains-tt-and-hierarchical-formats" id="toc-tensor-trains-tt-and-hierarchical-formats" class="nav-link" data-scroll-target="#tensor-trains-tt-and-hierarchical-formats">14.3 Tensor Trains (TT) and Hierarchical Formats</a></li>
  <li><a href="#connections-to-svd-and-pca" id="toc-connections-to-svd-and-pca" class="nav-link" data-scroll-target="#connections-to-svd-and-pca">14.4 Connections to SVD and PCA</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-viii.-computation-and-numerical-practice" id="toc-part-viii.-computation-and-numerical-practice" class="nav-link" data-scroll-target="#part-viii.-computation-and-numerical-practice">Part VIII. Computation and Numerical Practice</a>
  <ul class="collapse">
  <li><a href="#chapter-15.-working-with-tensors-in-code" id="toc-chapter-15.-working-with-tensors-in-code" class="nav-link" data-scroll-target="#chapter-15.-working-with-tensors-in-code">Chapter 15. Working with Tensors in Code</a>
  <ul class="collapse">
  <li><a href="#efficient-indexing-and-memory-layout" id="toc-efficient-indexing-and-memory-layout" class="nav-link" data-scroll-target="#efficient-indexing-and-memory-layout">15.1 Efficient Indexing and Memory Layout</a></li>
  <li><a href="#exercises-53" id="toc-exercises-53" class="nav-link" data-scroll-target="#exercises-53">Exercises</a></li>
  <li><a href="#blas-einsum-performance-patterns" id="toc-blas-einsum-performance-patterns" class="nav-link" data-scroll-target="#blas-einsum-performance-patterns">15.2 BLAS, Einsum, Performance Patterns</a></li>
  <li><a href="#stability-conditioning-scaling-tricks" id="toc-stability-conditioning-scaling-tricks" class="nav-link" data-scroll-target="#stability-conditioning-scaling-tricks">15.3 Stability, Conditioning, Scaling Tricks</a></li>
  </ul></li>
  <li><a href="#chapter-16.-automatic-differentiation-and-gradients" id="toc-chapter-16.-automatic-differentiation-and-gradients" class="nav-link" data-scroll-target="#chapter-16.-automatic-differentiation-and-gradients">Chapter 16. Automatic Differentiation and Gradients</a>
  <ul class="collapse">
  <li><a href="#jacobianshessians-as-tensors" id="toc-jacobianshessians-as-tensors" class="nav-link" data-scroll-target="#jacobianshessians-as-tensors">16.1 Jacobians/Hessians as Tensors</a></li>
  <li><a href="#exercises-56" id="toc-exercises-56" class="nav-link" data-scroll-target="#exercises-56">Exercises</a></li>
  <li><a href="#backprop-as-structured-contractions" id="toc-backprop-as-structured-contractions" class="nav-link" data-scroll-target="#backprop-as-structured-contractions">16.2 Backprop as Structured Contractions</a></li>
  <li><a href="#practical-tips-for-pytorchjaxnumpy" id="toc-practical-tips-for-pytorchjaxnumpy" class="nav-link" data-scroll-target="#practical-tips-for-pytorchjaxnumpy">16.3 Practical Tips for PyTorch/JAX/NumPy</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-ix.-applications-you-can-touch" id="toc-part-ix.-applications-you-can-touch" class="nav-link" data-scroll-target="#part-ix.-applications-you-can-touch">Part IX. Applications you can touch</a>
  <ul class="collapse">
  <li><a href="#chapter-17.-data-science-and-signal-processing" id="toc-chapter-17.-data-science-and-signal-processing" class="nav-link" data-scroll-target="#chapter-17.-data-science-and-signal-processing">Chapter 17. Data Science and Signal Processing</a>
  <ul class="collapse">
  <li><a href="#multilinear-regression" id="toc-multilinear-regression" class="nav-link" data-scroll-target="#multilinear-regression">17.1 Multilinear Regression</a></li>
  <li><a href="#spatiotemporal-data-and-video-tensors" id="toc-spatiotemporal-data-and-video-tensors" class="nav-link" data-scroll-target="#spatiotemporal-data-and-video-tensors">17.2 Spatiotemporal Data and Video Tensors</a></li>
  <li><a href="#blind-source-separation" id="toc-blind-source-separation" class="nav-link" data-scroll-target="#blind-source-separation">17.3 Blind Source Separation</a></li>
  </ul></li>
  <li><a href="#chapter-18.-machine-learning-and-deep-models" id="toc-chapter-18.-machine-learning-and-deep-models" class="nav-link" data-scroll-target="#chapter-18.-machine-learning-and-deep-models">Chapter 18. Machine Learning and Deep Models</a>
  <ul class="collapse">
  <li><a href="#convolutions-as-multilinear-maps" id="toc-convolutions-as-multilinear-maps" class="nav-link" data-scroll-target="#convolutions-as-multilinear-maps">18.1 Convolutions as Multilinear Maps</a></li>
  <li><a href="#low-rank-tensor-compression-of-nets" id="toc-low-rank-tensor-compression-of-nets" class="nav-link" data-scroll-target="#low-rank-tensor-compression-of-nets">18.2 Low-Rank Tensor Compression of Nets</a></li>
  </ul></li>
  <li><a href="#chapter-19.-physics-graphics-and-beyond" id="toc-chapter-19.-physics-graphics-and-beyond" class="nav-link" data-scroll-target="#chapter-19.-physics-graphics-and-beyond">Chapter 19. Physics, Graphics, and Beyond</a>
  <ul class="collapse">
  <li><a href="#stressstrain-tensors" id="toc-stressstrain-tensors" class="nav-link" data-scroll-target="#stressstrain-tensors">19.1 Stress/Strain Tensors</a></li>
  <li><a href="#inertia-tensors-and-principal-axes" id="toc-inertia-tensors-and-principal-axes" class="nav-link" data-scroll-target="#inertia-tensors-and-principal-axes">19.2 Inertia Tensors and Principal Axes</a></li>
  <li><a href="#d-graphics-transforms-and-shading" id="toc-d-graphics-transforms-and-shading" class="nav-link" data-scroll-target="#d-graphics-transforms-and-shading">19.3 3D Graphics: Transforms and Shading</a></li>
  <li><a href="#quantum-states-and-operators" id="toc-quantum-states-and-operators" class="nav-link" data-scroll-target="#quantum-states-and-operators">19.4 Quantum States and Operators</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#part-ix.-glimpses-beyond-this-book" id="toc-part-ix.-glimpses-beyond-this-book" class="nav-link" data-scroll-target="#part-ix.-glimpses-beyond-this-book">Part IX. Glimpses Beyond This Book</a>
  <ul class="collapse">
  <li><a href="#chapter-20.-manifolds-and-tensor-fields-preview" id="toc-chapter-20.-manifolds-and-tensor-fields-preview" class="nav-link" data-scroll-target="#chapter-20.-manifolds-and-tensor-fields-preview">Chapter 20. Manifolds and Tensor Fields (Preview)</a>
  <ul class="collapse">
  <li><a href="#tangent-and-cotangent-bundles" id="toc-tangent-and-cotangent-bundles" class="nav-link" data-scroll-target="#tangent-and-cotangent-bundles">20.1 Tangent and Cotangent Bundles</a></li>
  <li><a href="#tensor-fields-and-coordinate-changes" id="toc-tensor-fields-and-coordinate-changes" class="nav-link" data-scroll-target="#tensor-fields-and-coordinate-changes">20.2 Tensor Fields and Coordinate Changes</a></li>
  <li><a href="#covariant-derivatives-and-curvature" id="toc-covariant-derivatives-and-curvature" class="nav-link" data-scroll-target="#covariant-derivatives-and-curvature">20.3 Covariant Derivatives and Curvature</a></li>
  </ul></li>
  <li><a href="#chapter-21.-representation-theory-and-invariants-preview" id="toc-chapter-21.-representation-theory-and-invariants-preview" class="nav-link" data-scroll-target="#chapter-21.-representation-theory-and-invariants-preview">Chapter 21. Representation Theory and Invariants (Preview)</a>
  <ul class="collapse">
  <li><a href="#group-actions-on-tensor-spaces" id="toc-group-actions-on-tensor-spaces" class="nav-link" data-scroll-target="#group-actions-on-tensor-spaces">21.1 Group Actions on Tensor Spaces</a></li>
  <li><a href="#invariant-tensors-and-symmetry" id="toc-invariant-tensors-and-symmetry" class="nav-link" data-scroll-target="#invariant-tensors-and-symmetry">21.2 Invariant Tensors and Symmetry</a></li>
  <li><a href="#why-invariants-matter-in-algorithms" id="toc-why-invariants-matter-in-algorithms" class="nav-link" data-scroll-target="#why-invariants-matter-in-algorithms">21.3 Why Invariants Matter in Algorithms</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#end-matter" id="toc-end-matter" class="nav-link" data-scroll-target="#end-matter">End Matter</a>
  <ul class="collapse">
  <li><a href="#a.-symbols-and-notation-cheatsheet" id="toc-a.-symbols-and-notation-cheatsheet" class="nav-link" data-scroll-target="#a.-symbols-and-notation-cheatsheet">A. Symbols and Notation Cheatsheet</a>
  <ul class="collapse">
  <li><a href="#vector-spaces-and-duals" id="toc-vector-spaces-and-duals" class="nav-link" data-scroll-target="#vector-spaces-and-duals">Vector Spaces and Duals</a></li>
  <li><a href="#tensors" id="toc-tensors" class="nav-link" data-scroll-target="#tensors">Tensors</a></li>
  <li><a href="#indices-and-summation" id="toc-indices-and-summation" class="nav-link" data-scroll-target="#indices-and-summation">Indices and Summation</a></li>
  <li><a href="#linear-maps-and-operators" id="toc-linear-maps-and-operators" class="nav-link" data-scroll-target="#linear-maps-and-operators">Linear Maps and Operators</a></li>
  <li><a href="#tensor-operations" id="toc-tensor-operations" class="nav-link" data-scroll-target="#tensor-operations">Tensor Operations</a></li>
  <li><a href="#special-objects" id="toc-special-objects" class="nav-link" data-scroll-target="#special-objects">Special Objects</a></li>
  </ul></li>
  <li><a href="#appendix-b.-proof-sketches-of-core-theorems" id="toc-appendix-b.-proof-sketches-of-core-theorems" class="nav-link" data-scroll-target="#appendix-b.-proof-sketches-of-core-theorems">Appendix B. Proof Sketches of Core Theorems</a>
  <ul class="collapse">
  <li><a href="#appendix-b.1-universal-property-of-the-tensor-product-proof-sketch" id="toc-appendix-b.1-universal-property-of-the-tensor-product-proof-sketch" class="nav-link" data-scroll-target="#appendix-b.1-universal-property-of-the-tensor-product-proof-sketch">Appendix B.1 Universal Property of the Tensor Product (Proof Sketch)</a></li>
  <li><a href="#appendix-b.2-dimension-formula-for-tensor-products-proof-sketch" id="toc-appendix-b.2-dimension-formula-for-tensor-products-proof-sketch" class="nav-link" data-scroll-target="#appendix-b.2-dimension-formula-for-tensor-products-proof-sketch">Appendix B.2 Dimension Formula for Tensor Products (Proof Sketch)</a></li>
  <li><a href="#appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch" id="toc-appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch" class="nav-link" data-scroll-target="#appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch">Appendix B.3 Decomposition of Tensors into Symmetric and Antisymmetric Parts (Proof Sketch)</a></li>
  </ul></li>
  <li><a href="#appendix-d.-identities-cookbook" id="toc-appendix-d.-identities-cookbook" class="nav-link" data-scroll-target="#appendix-d.-identities-cookbook">Appendix D. Identities &amp; “Cookbook”</a>
  <ul class="collapse">
  <li><a href="#kronecker-product-vec-identities" id="toc-kronecker-product-vec-identities" class="nav-link" data-scroll-target="#kronecker-product-vec-identities">1. Kronecker Product &amp; Vec Identities</a></li>
  <li><a href="#trace-tricks" id="toc-trace-tricks" class="nav-link" data-scroll-target="#trace-tricks">2. Trace Tricks</a></li>
  <li><a href="#tensor-contractions" id="toc-tensor-contractions" class="nav-link" data-scroll-target="#tensor-contractions">3. Tensor Contractions</a></li>
  <li><a href="#determinant-volumes" id="toc-determinant-volumes" class="nav-link" data-scroll-target="#determinant-volumes">4. Determinant &amp; Volumes</a></li>
  <li><a href="#differential-gradient-identities" id="toc-differential-gradient-identities" class="nav-link" data-scroll-target="#differential-gradient-identities">5. Differential &amp; Gradient Identities</a></li>
  <li><a href="#useful-einsum-patterns" id="toc-useful-einsum-patterns" class="nav-link" data-scroll-target="#useful-einsum-patterns">6. Useful Einsum Patterns</a></li>
  <li><a href="#symmetrization-antisymmetrization" id="toc-symmetrization-antisymmetrization" class="nav-link" data-scroll-target="#symmetrization-antisymmetrization">7. Symmetrization / Antisymmetrization</a></li>
  </ul></li>
  <li><a href="#appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors" id="toc-appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors" class="nav-link" data-scroll-target="#appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors">Appendix E.1 Mini-Project: Implement CP Decomposition for Small 3-Way Tensors</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">1. Background</a></li>
  <li><a href="#implementation-plan" id="toc-implementation-plan" class="nav-link" data-scroll-target="#implementation-plan">2. Implementation Plan</a></li>
  <li><a href="#coding-hints" id="toc-coding-hints" class="nav-link" data-scroll-target="#coding-hints">3. Coding Hints</a></li>
  <li><a href="#extensions-optional" id="toc-extensions-optional" class="nav-link" data-scroll-target="#extensions-optional">4. Extensions (Optional)</a></li>
  <li><a href="#deliverables" id="toc-deliverables" class="nav-link" data-scroll-target="#deliverables">5. Deliverables</a></li>
  </ul></li>
  <li><a href="#appendix-e.2-mini-project-tucker-decomposition-for-video-compression" id="toc-appendix-e.2-mini-project-tucker-decomposition-for-video-compression" class="nav-link" data-scroll-target="#appendix-e.2-mini-project-tucker-decomposition-for-video-compression">Appendix E.2 Mini-Project: Tucker Decomposition for Video Compression</a>
  <ul class="collapse">
  <li><a href="#background-1" id="toc-background-1" class="nav-link" data-scroll-target="#background-1">1. Background</a></li>
  <li><a href="#implementation-plan-1" id="toc-implementation-plan-1" class="nav-link" data-scroll-target="#implementation-plan-1">2. Implementation Plan</a></li>
  <li><a href="#coding-hints-1" id="toc-coding-hints-1" class="nav-link" data-scroll-target="#coding-hints-1">3. Coding Hints</a></li>
  <li><a href="#extensions-optional-1" id="toc-extensions-optional-1" class="nav-link" data-scroll-target="#extensions-optional-1">4. Extensions (Optional)</a></li>
  <li><a href="#deliverables-1" id="toc-deliverables-1" class="nav-link" data-scroll-target="#deliverables-1">5. Deliverables</a></li>
  </ul></li>
  <li><a href="#appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate" id="toc-appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate" class="nav-link" data-scroll-target="#appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate">Appendix E.3 Mini-Project: Strain Tensor for a Rotating Plate</a>
  <ul class="collapse">
  <li><a href="#background-2" id="toc-background-2" class="nav-link" data-scroll-target="#background-2">1. Background</a></li>
  <li><a href="#setup-rotating-plate" id="toc-setup-rotating-plate" class="nav-link" data-scroll-target="#setup-rotating-plate">2. Setup: Rotating Plate</a></li>
  <li><a href="#compute-strain-tensor" id="toc-compute-strain-tensor" class="nav-link" data-scroll-target="#compute-strain-tensor">3. Compute Strain Tensor</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">4. Interpretation</a></li>
  <li><a href="#coding-hints-2" id="toc-coding-hints-2" class="nav-link" data-scroll-target="#coding-hints-2">5. Coding Hints</a></li>
  <li><a href="#extensions-optional-2" id="toc-extensions-optional-2" class="nav-link" data-scroll-target="#extensions-optional-2">6. Extensions (Optional)</a></li>
  <li><a href="#deliverables-2" id="toc-deliverables-2" class="nav-link" data-scroll-target="#deliverables-2">7. Deliverables</a></li>
  </ul></li>
  <li><a href="#appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere" id="toc-appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere" class="nav-link" data-scroll-target="#appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere">Appendix E.4 Mini-Project: Parallel Transport of a Vector on a Sphere</a>
  <ul class="collapse">
  <li><a href="#background-3" id="toc-background-3" class="nav-link" data-scroll-target="#background-3">1. Background</a></li>
  <li><a href="#setup-the-sphere-s2" id="toc-setup-the-sphere-s2" class="nav-link" data-scroll-target="#setup-the-sphere-s2">2. Setup: The Sphere <span class="math inline">\(S^2\)</span></a></li>
  <li><a href="#parallel-transport-along-geodesics" id="toc-parallel-transport-along-geodesics" class="nav-link" data-scroll-target="#parallel-transport-along-geodesics">3. Parallel Transport along Geodesics</a></li>
  <li><a href="#coding-hints-3" id="toc-coding-hints-3" class="nav-link" data-scroll-target="#coding-hints-3">4. Coding Hints</a></li>
  <li><a href="#extensions-optional-3" id="toc-extensions-optional-3" class="nav-link" data-scroll-target="#extensions-optional-3">5. Extensions (Optional)</a></li>
  <li><a href="#deliverables-3" id="toc-deliverables-3" class="nav-link" data-scroll-target="#deliverables-3">6. Deliverables</a></li>
  </ul></li>
  <li><a href="#appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data" id="toc-appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data" class="nav-link" data-scroll-target="#appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data">Appendix E.5 Mini-Project: PCA vs.&nbsp;Tucker Decomposition on Real Data</a>
  <ul class="collapse">
  <li><a href="#background-4" id="toc-background-4" class="nav-link" data-scroll-target="#background-4">1. Background</a></li>
  <li><a href="#dataset-options" id="toc-dataset-options" class="nav-link" data-scroll-target="#dataset-options">2. Dataset Options</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">3. Methodology</a></li>
  <li><a href="#coding-hints-4" id="toc-coding-hints-4" class="nav-link" data-scroll-target="#coding-hints-4">4. Coding Hints</a></li>
  <li><a href="#extensions-optional-4" id="toc-extensions-optional-4" class="nav-link" data-scroll-target="#extensions-optional-4">5. Extensions (Optional)</a></li>
  <li><a href="#deliverables-4" id="toc-deliverables-4" class="nav-link" data-scroll-target="#deliverables-4">6. Deliverables</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Little Book of Multilinear Algebra</h1>
<p class="subtitle lead">Version 0.1.0</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Duc-Tam Nguyen </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 5, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="part-i.-orientation-motivation" class="level1">
<h1>Part I. Orientation &amp; Motivation</h1>
<section id="chapter-1.-what-is-multilinear" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.-what-is-multilinear">Chapter 1. What is Multilinear?</h2>
<section id="linear-vs.-multilinear-from-lines-to-volumes" class="level3">
<h3 class="anchored" data-anchor-id="linear-vs.-multilinear-from-lines-to-volumes">1.1 Linear vs.&nbsp;Multilinear: From Lines to Volumes</h3>
<p>When you first meet <em>linear algebra</em>, the word “linear” carries a specific flavor: we study maps that preserve straightness and scaling. A linear map <span class="math inline">\(f: V \to W\)</span> satisfies</p>
<p><span class="math display">\[
f(a v_1 + b v_2) = a f(v_1) + b f(v_2),
\]</span></p>
<p>for scalars <span class="math inline">\(a, b\)</span> and vectors <span class="math inline">\(v_1, v_2 \in V\)</span>. The picture is of functions that take lines to lines, planes to planes, preserving the essential structure of addition and scaling.</p>
<section id="from-linear-to-multilinear" class="level4">
<h4 class="anchored" data-anchor-id="from-linear-to-multilinear">From Linear to Multilinear</h4>
<p>A multilinear map involves <em>several vector inputs at once</em>. For instance, a bilinear map takes two vectors:</p>
<p><span class="math display">\[
B : V \times W \to \mathbb{R}, \quad B(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w),
\]</span></p>
<p>and is linear in each argument separately. More generally, a <span class="math inline">\(k\)</span>-linear map eats <span class="math inline">\(k\)</span> vectors-each input behaves linearly while the others are held fixed.</p>
</section>
<section id="geometry-of-the-shift" class="level4">
<h4 class="anchored" data-anchor-id="geometry-of-the-shift">Geometry of the Shift</h4>
<ul>
<li>Linear (1-input): Think of scaling a line, stretching it along one axis.</li>
<li>Bilinear (2-input): Now imagine two directions at once. The determinant of a <span class="math inline">\(2 \times 2\)</span> matrix is bilinear: it measures the signed *area- spanned by two vectors.</li>
<li>Trilinear (3-input): With three inputs, multilinearity measures a <em>volume</em>. The scalar triple product <span class="math inline">\(u \cdot (v \times w)\)</span> is trilinear, giving the volume of the parallelepiped formed by <span class="math inline">\(u, v, w\)</span>.</li>
<li>Higher multilinearity: Beyond three inputs, we can measure 4D hyper-volumes and higher-dimensional “content.”</li>
</ul>
</section>
<section id="intuition-from-1d-to-higher-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="intuition-from-1d-to-higher-dimensions">Intuition: From 1D to Higher Dimensions</h4>
<ul>
<li>1D (linear): One direction → length.</li>
<li>2D (bilinear): Two directions → area.</li>
<li>3D (trilinear): Three directions → volume.</li>
<li>kD (multilinear): <span class="math inline">\(k\)</span> directions → hyper-volume.</li>
</ul>
<p>Each new input adds a new dimension of measurement. In this sense, multilinear algebra is the natural extension of linear algebra: instead of studying transformations of single vectors, we study functions that combine several vectors in structured ways.</p>
</section>
<section id="everyday-examples" class="level4">
<h4 class="anchored" data-anchor-id="everyday-examples">Everyday Examples</h4>
<ul>
<li>Dot product: Bilinear form producing a scalar from two vectors.</li>
<li>Matrix multiplication: Bilinear in its row and column inputs.</li>
<li>Determinant: Multilinear in its columns (or rows), encoding volume.</li>
<li>Cross product: Bilinear but antisymmetric, giving a vector orthogonal to two inputs.</li>
<li>Neural networks: Convolutions and tensor contractions are deeply multilinear operations, reshaped for computation.</li>
</ul>
<p>Linear algebra captures what happens when you act on one direction at a time. Multilinear algebra generalizes this, letting us measure, transform, and compute with several directions simultaneously. It is the leap from lines to volumes, from single-vector transformations to the rich geometry of many interacting vectors.</p>
</section>
<section id="exercises" class="level4">
<h4 class="anchored" data-anchor-id="exercises">Exercises</h4>
<ol type="1">
<li><p>Linearity Check: Let <span class="math inline">\(f:\mathbb{R}^2 \to \mathbb{R}^2\)</span> be defined by <span class="math inline">\(f(x,y) = (2x, 3y)\)</span>. Show that <span class="math inline">\(f\)</span> is linear.</p></li>
<li><p>Bilinear Dot Product: Verify that the dot product <span class="math inline">\(\langle u, v \rangle = u_1 v_1 + u_2 v_2 + u_3 v_3\)</span> is bilinear by checking linearity in each argument separately.</p></li>
<li><p>Area via Determinant: For vectors <span class="math inline">\(u = (1,0)\)</span> and <span class="math inline">\(v = (1,2)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>, compute the determinant of the <span class="math inline">\(2 \times 2\)</span> matrix with columns <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>. Interpret the result geometrically.</p></li>
<li><p>Volume via Scalar Triple Product: Compute the scalar triple product <span class="math inline">\(u \cdot (v \times w)\)</span> for <span class="math inline">\(u = (1,0,0)\)</span>, <span class="math inline">\(v = (0,1,0)\)</span>, and <span class="math inline">\(w = (0,0,1)\)</span>. Explain why the result makes sense in terms of volume.</p></li>
<li><p>Generalization Thought Experiment: Imagine you have four independent vectors in <span class="math inline">\(\mathbb{R}^4\)</span>. What kind of geometric quantity does a multilinear map on these four inputs measure? (Hint: think of the analogy with length, area, and volume.)</p></li>
</ol>
</section>
</section>
<section id="three-faces-of-a-tensor-array-map-and-element-of-a-product-space" class="level3">
<h3 class="anchored" data-anchor-id="three-faces-of-a-tensor-array-map-and-element-of-a-product-space">1.2 Three Faces of a Tensor: Array, Map, and Element of a Product Space</h3>
<p>A tensor can feel slippery at first, because it wears different “faces” depending on how you meet it. Each face is valid and useful. Together they form the three standard viewpoints:</p>
<section id="array-view-numbers-in-a-box" class="level4">
<h4 class="anchored" data-anchor-id="array-view-numbers-in-a-box">1. Array View: Numbers in a Box</h4>
<p>At the simplest level, a tensor looks like a multidimensional array of numbers.</p>
<ul>
<li>A vector is a 1-dimensional array: <span class="math inline">\([v_i]\)</span>.</li>
<li>A matrix is a 2-dimensional array: <span class="math inline">\([a_{ij}]\)</span>.</li>
<li>A general tensor of order <span class="math inline">\(k\)</span> is like a <span class="math inline">\(k\)</span>-dimensional grid of numbers: <span class="math inline">\([t_{i_1 i_2 \dots i_k}]\)</span>.</li>
</ul>
<p>This viewpoint is intuitive for computation, storage, and indexing. For example, in machine learning an image is a 3rd-order tensor: height × width × color channels.</p>
</section>
<section id="map-view-multilinear-functions" class="level4">
<h4 class="anchored" data-anchor-id="map-view-multilinear-functions">2. Map View: Multilinear Functions</h4>
<p>Another way to see tensors is as multilinear maps.</p>
<ul>
<li><p>A bilinear form takes two vectors and outputs a number, e.g., the dot product.</p></li>
<li><p>A trilinear form takes three vectors and outputs a number, e.g., the scalar triple product.</p></li>
<li><p>In general, a tensor <span class="math inline">\(T\)</span> of type <span class="math inline">\((0,k)\)</span> is a map</p>
<p><span class="math display">\[
T: V \times V \times \cdots \times V \to \mathbb{R},
\]</span></p>
<p>linear in each slot separately.</p></li>
</ul>
<p>This viewpoint highlights <em>behavior</em>: how the tensor interacts with vectors. It is central in geometry and physics, where tensors encode measurable relationships.</p>
</section>
<section id="product-space-view-elements-of-v-otimes-w-otimes-cdots" class="level4">
<h4 class="anchored" data-anchor-id="product-space-view-elements-of-v-otimes-w-otimes-cdots">3. Product Space View: Elements of <span class="math inline">\(V \otimes W \otimes \cdots\)</span></h4>
<p>The third viewpoint places tensors as elements of tensor product spaces.</p>
<ul>
<li>Given vector spaces <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, their tensor product <span class="math inline">\(V \otimes W\)</span> is a new space built to capture bilinear maps.</li>
<li>A simple (decomposable) tensor looks like <span class="math inline">\(v \otimes w\)</span>.</li>
<li>General tensors are linear combinations of these simple pieces.</li>
</ul>
<p>This is the most abstract but also the most powerful perspective. It makes precise statements about dimension, bases, and transformation laws. It also unifies the array and map viewpoints: the array entries are just the coordinates of a tensor element with respect to a basis, and the map behavior is encoded in how the element acts under evaluation.</p>
</section>
<section id="reconciling-the-views" class="level4">
<h4 class="anchored" data-anchor-id="reconciling-the-views">Reconciling the Views</h4>
<ul>
<li>Array: concrete for computation.</li>
<li>Map: functional and geometric meaning.</li>
<li>Product space: rigorous foundation.</li>
</ul>
<p>A beginner should be comfortable switching between them: “the same object, three different languages.”</p>
</section>
<section id="exercises-1" class="level4">
<h4 class="anchored" data-anchor-id="exercises-1">Exercises</h4>
<ol type="1">
<li><p>Array Identification: Write down a 3rd-order tensor <span class="math inline">\(T\)</span> with entries <span class="math inline">\(t_{ijk}\)</span> where <span class="math inline">\(i,j,k \in \{1,2\}\)</span>. How many numbers are needed to specify <span class="math inline">\(T\)</span>?</p></li>
<li><p>Dot Product as a Tensor: Show that the dot product on <span class="math inline">\(\mathbb{R}^3\)</span> can be viewed both as a bilinear map and as an array (matrix).</p></li>
<li><p>Simple Tensor Construction: Given <span class="math inline">\(u=(1,2)\in \mathbb{R}^2\)</span> and <span class="math inline">\(v=(3,4)\in \mathbb{R}^2\)</span>, form the tensor <span class="math inline">\(u \otimes v\)</span>. Write its coordinates as a <span class="math inline">\(2 \times 2\)</span> array.</p></li>
<li><p>Switching Perspectives: Consider the determinant of a <span class="math inline">\(2\times2\)</span> matrix: <span class="math inline">\(\det([a_{ij}]) = a_{11}a_{22} - a_{12}a_{21}\)</span>. Explain how this determinant can be seen as a bilinear map of the two column vectors of the matrix.</p></li>
<li><p>Thought Experiment: Suppose you store an RGB image of size <span class="math inline">\(100 \times 200\)</span>.</p>
<ul>
<li>In the array view, what is the order (number of indices) of the tensor representing the image?</li>
<li>In the product space view, which vector spaces might this tensor live in?</li>
</ul></li>
</ol>
</section>
</section>
<section id="why-it-matters-graphics-physics-ml-data-compression" class="level3">
<h3 class="anchored" data-anchor-id="why-it-matters-graphics-physics-ml-data-compression">1.3 Why It Matters: Graphics, Physics, ML, Data Compression</h3>
<p>Tensors may sound abstract, but they appear everywhere in modern science and technology. Understanding *why- they matter helps motivate the study of multilinear algebra.</p>
<section id="in-computer-graphics" class="level4">
<h4 class="anchored" data-anchor-id="in-computer-graphics">In Computer Graphics</h4>
<ul>
<li>Transformations: 3D graphics use matrices (2nd-order tensors) to rotate, scale, and project objects.</li>
<li>Lighting Models: Many shading equations combine vectors of light, normal directions, and material properties in bilinear or trilinear ways.</li>
<li>Animation: Deformations of 3D models often rely on multilinear blending of control parameters.</li>
</ul>
<p>Tensors let us express geometry and transformations in compact formulas that computers can process efficiently.</p>
</section>
<section id="in-physics-and-engineering" class="level4">
<h4 class="anchored" data-anchor-id="in-physics-and-engineering">In Physics and Engineering</h4>
<ul>
<li>Stress and Strain: The stress tensor (2nd-order) relates internal forces to orientations in a material. The elasticity tensor (4th-order) relates stress and strain in solids.</li>
<li>Electromagnetism: The electromagnetic field tensor encodes electric and magnetic fields in a relativistic framework.</li>
<li>Inertia Tensor: Describes how an object resists rotational acceleration depending on its mass distribution.</li>
</ul>
<p>Tensors naturally capture “laws that hold in every direction,” which is why they dominate in mechanics and field theories.</p>
</section>
<section id="in-machine-learning" class="level4">
<h4 class="anchored" data-anchor-id="in-machine-learning">In Machine Learning</h4>
<ul>
<li>Neural Networks: Input data (images, videos, audio) are tensors of order 3, 4, or higher.</li>
<li>Convolutions: The convolution kernel is a small tensor sliding across a larger tensor, producing a new one.</li>
<li>Attention Mechanisms: Core operations in transformers are tensor contractions that combine multiple input arrays.</li>
</ul>
<p>Understanding tensor decompositions helps compress models, speed up computation, and reveal hidden structures in data.</p>
</section>
<section id="in-data-compression-and-signal-processing" class="level4">
<h4 class="anchored" data-anchor-id="in-data-compression-and-signal-processing">In Data Compression and Signal Processing</h4>
<ul>
<li>PCA and SVD: Classic matrix decompositions are 2D tensor methods.</li>
<li>Tensor Decompositions (CP, Tucker, TT): These extend compression ideas to multidimensional data, useful for video, hyperspectral imaging, and big-data analysis.</li>
<li>Multiway Data Analysis: Tensors allow us to uncover patterns across several “modes” simultaneously-like user × time × product in recommendation systems.</li>
</ul>
</section>
<section id="the-big-picture" class="level4">
<h4 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h4>
<p>Linear algebra lets us describe single-direction transformations (lines). Multilinear algebra extends this to multiple interacting directions (areas, volumes, hyper-volumes). These structures appear whenever we handle multi-dimensional data or physical laws.</p>
<p>Learning to *think tensorially- is the key to navigating modern applied mathematics, science, and AI.</p>
</section>
<section id="exercises-2" class="level4">
<h4 class="anchored" data-anchor-id="exercises-2">Exercises</h4>
<ol type="1">
<li><p>Graphics Example: A 3D rotation matrix is a 2nd-order tensor. Explain why it is linear in its input vector but not multilinear.</p></li>
<li><p>Physics Example: The stress tensor <span class="math inline">\(\sigma\)</span> maps a direction vector <span class="math inline">\(n\)</span> to a force vector <span class="math inline">\(\sigma n\)</span>. Why is this operation linear in <span class="math inline">\(n\)</span>?</p></li>
<li><p>Machine Learning Example: An RGB image of size <span class="math inline">\(32 \times 32\)</span> has 3 color channels.</p>
<ul>
<li>What is the order of the tensor representing this image?</li>
<li>How many entries does it have in total?</li>
</ul></li>
<li><p>Data Compression Example: PCA reduces a data matrix (2nd-order tensor) to a low-rank approximation. Suggest what a “low-rank” tensor decomposition might achieve for video data (3rd-order tensor: frame × width × height).</p></li>
<li><p>Thought Experiment: Suppose you could only use vectors and matrices, not higher-order tensors. Which of the following applications would be impossible or very awkward:</p>
<ul>
<li>Representing the interaction of three forces at once.</li>
<li>Compressing a color video.</li>
<li>Encoding the stress-strain relationship in 3D materials. ### 1.4 A First Walk-Through: Color Images and 3-Way Arrays</li>
</ul></li>
</ol>
<p>Let’s make tensors concrete with an everyday example: digital images.</p>
</section>
<section id="from-grayscale-to-color" class="level4">
<h4 class="anchored" data-anchor-id="from-grayscale-to-color">From Grayscale to Color</h4>
<ul>
<li><p>A grayscale image of size <span class="math inline">\(100 \times 200\)</span> can be seen as a matrix (2nd-order tensor). Each entry stores the brightness of a pixel.</p></li>
<li><p>A color image has three channels: red, green, and blue (RGB). Now every pixel carries three values. This naturally forms a 3rd-order tensor:</p>
<p><span class="math display">\[
I \in \mathbb{R}^{100 \times 200 \times 3}.
\]</span></p>
<p>The three indices correspond to row, column, color channel.</p></li>
</ul>
</section>
<section id="the-three-index-roles" class="level4">
<h4 class="anchored" data-anchor-id="the-three-index-roles">The Three Index Roles</h4>
<ol type="1">
<li>Row (height): vertical position in the image.</li>
<li>Column (width): horizontal position in the image.</li>
<li>Channel: one of the RGB color intensities.</li>
</ol>
<p>Together, <span class="math inline">\((i,j,k)\)</span> points to a single number: the intensity of color <span class="math inline">\(k\)</span> at pixel <span class="math inline">\((i,j)\)</span>.</p>
</section>
<section id="operations-as-tensor-manipulations" class="level4">
<h4 class="anchored" data-anchor-id="operations-as-tensor-manipulations">Operations as Tensor Manipulations</h4>
<ul>
<li>Flattening: We can reshape the 3D tensor into a 2D matrix, useful for feeding into algorithms that expect vectors or matrices.</li>
<li>Contraction (summing over an index): If we sum over the color channel, we turn an RGB image into a grayscale image.</li>
<li>Outer products: A colored checkerboard pattern can be constructed by taking tensor products of row and column vectors, then adding a color channel vector.</li>
</ul>
</section>
<section id="why-this-example-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-this-example-matters">Why This Example Matters</h4>
<ul>
<li>It shows how natural data can have more than two indices.</li>
<li>It illustrates why matrices (2D tensors) are not enough for modern problems.</li>
<li>It connects tensor operations with practical tasks: filtering, compression, feature extraction.</li>
</ul>
<p>In fact, video data adds one more index: time. A video is a 4th-order tensor: frame × height × width × channel.</p>
</section>
<section id="exercises-3" class="level4">
<h4 class="anchored" data-anchor-id="exercises-3">Exercises</h4>
<ol type="1">
<li><p>Counting Entries: How many numbers are required to store a color image of size <span class="math inline">\(64 \times 64\)</span>?</p></li>
<li><p>Slicing: For a color image tensor <span class="math inline">\(I \in \mathbb{R}^{100 \times 200 \times 3}\)</span>, what is the shape of:</p>
<ul>
<li>a single row across all columns and channels?</li>
<li>a single color channel across all pixels?</li>
</ul></li>
<li><p>Flattening Practice: A <span class="math inline">\(32 \times 32 \times 3\)</span> image is flattened into a vector. What is the length of this vector?</p></li>
<li><p>Grayscale Conversion: Define a grayscale image <span class="math inline">\(G(i,j)\)</span> from a color image tensor <span class="math inline">\(I(i,j,k)\)</span> by averaging across channels:</p>
<p><span class="math display">\[
G(i,j) = \frac{1}{3} \sum_{k=1}^3 I(i,j,k).
\]</span></p>
<p>Why is this operation an example of contraction?</p></li>
<li><p>Video as Tensor: Suppose you have a 10-second video at 30 frames per second, each frame <span class="math inline">\(128 \times 128\)</span> with 3 color channels.</p>
<ul>
<li>What is the order of the video tensor?</li>
<li>How many entries does it contain in total?</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="chapter-2.-minimal-prerequisites" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.-minimal-prerequisites">Chapter 2. Minimal Prerequisites</h2>
<section id="vector-spaces-bases-dimension" class="level3">
<h3 class="anchored" data-anchor-id="vector-spaces-bases-dimension">2.1 Vector Spaces, Bases, Dimension</h3>
<p>Before diving deeper into multilinear algebra, we need a short refresher on the basic building blocks of linear algebra: vector spaces.</p>
<section id="what-is-a-vector-space" class="level4">
<h4 class="anchored" data-anchor-id="what-is-a-vector-space">What is a Vector Space?</h4>
<p>A vector space is a collection of objects (called <em>vectors</em>) that can be:</p>
<ol type="1">
<li>Added: <span class="math inline">\(u + v\)</span> is again a vector.</li>
<li>Scaled: <span class="math inline">\(a v\)</span> (where <span class="math inline">\(a\)</span> is a scalar) is again a vector.</li>
</ol>
<p>The rules of addition and scaling follow natural laws: associativity, commutativity, distributivity, and the existence of a zero vector.</p>
<p>Examples:</p>
<ul>
<li><span class="math inline">\(\mathbb{R}^n\)</span>: all <span class="math inline">\(n\)</span>-tuples of real numbers.</li>
<li>Polynomials of degree ≤ <span class="math inline">\(d\)</span>.</li>
<li>Continuous functions on an interval.</li>
</ul>
</section>
<section id="bases-and-coordinates" class="level4">
<h4 class="anchored" data-anchor-id="bases-and-coordinates">Bases and Coordinates</h4>
<p>A basis of a vector space is a set of vectors that:</p>
<ol type="1">
<li>Are linearly independent (no one is a linear combination of the others).</li>
<li>Span the entire space (every vector can be expressed as a linear combination of them).</li>
</ol>
<p>For <span class="math inline">\(\mathbb{R}^3\)</span>, the standard basis is:</p>
<p><span class="math display">\[
e_1 = (1,0,0), \quad e_2 = (0,1,0), \quad e_3 = (0,0,1).
\]</span></p>
<p>Every vector <span class="math inline">\(v \in \mathbb{R}^3\)</span> can be uniquely written as:</p>
<p><span class="math display">\[
v = x e_1 + y e_2 + z e_3,
\]</span></p>
<p>with coordinates <span class="math inline">\((x,y,z)\)</span>.</p>
</section>
<section id="dimension" class="level4">
<h4 class="anchored" data-anchor-id="dimension">Dimension</h4>
<p>The dimension of a vector space is the number of vectors in any basis.</p>
<ul>
<li><span class="math inline">\(\mathbb{R}^n\)</span> has dimension <span class="math inline">\(n\)</span>.</li>
<li>Polynomials of degree ≤ <span class="math inline">\(d\)</span> form a vector space of dimension <span class="math inline">\(d+1\)</span>.</li>
<li>A trivial space <span class="math inline">\(\{0\}\)</span> has dimension 0.</li>
</ul>
<p>Dimension gives the “number of independent directions” in the space.</p>
</section>
<section id="why-this-matters-for-multilinear-algebra" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-multilinear-algebra">Why This Matters for Multilinear Algebra</h4>
<ul>
<li>Tensors live in spaces built from vector spaces (tensor products).</li>
<li>Understanding bases and dimensions is crucial for counting entries of tensors.</li>
<li>Coordinates provide the link between abstract definitions and concrete arrays.</li>
</ul>
</section>
<section id="exercises-4" class="level4">
<h4 class="anchored" data-anchor-id="exercises-4">Exercises</h4>
<ol type="1">
<li><p>Checking Vector Spaces: Decide whether each of the following is a vector space over <span class="math inline">\(\mathbb{R}\)</span>:</p>
<ul>
<li><ol type="a">
<li>All <span class="math inline">\(2 \times 2\)</span> real matrices.</li>
</ol></li>
<li><ol start="2" type="a">
<li>All positive real numbers.</li>
</ol></li>
<li><ol start="3" type="a">
<li>All polynomials with real coefficients.</li>
</ol></li>
</ul></li>
<li><p>Basis in <span class="math inline">\(\mathbb{R}^2\)</span>: Show that <span class="math inline">\((1,1)\)</span> and <span class="math inline">\((1,-1)\)</span> form a basis for <span class="math inline">\(\mathbb{R}^2\)</span>. Express the vector <span class="math inline">\((3,2)\)</span> in this basis.</p></li>
<li><p>Counting Dimension: What is the dimension of the space of all real polynomials of degree ≤ 4? Suggest a natural basis.</p></li>
<li><p>Uniqueness of Representation: In <span class="math inline">\(\mathbb{R}^3\)</span>, write <span class="math inline">\((2,3,5)\)</span> as a combination of <span class="math inline">\(e_1, e_2, e_3\)</span>. Why is this representation unique?</p></li>
<li><p>Application to Tensors: If <span class="math inline">\(V = \mathbb{R}^2\)</span> and <span class="math inline">\(W = \mathbb{R}^3\)</span>, what is the dimension of the product space <span class="math inline">\(V \otimes W\)</span>?</p></li>
</ol>
</section>
</section>
<section id="linear-maps-matrices-change-of-basis" class="level3">
<h3 class="anchored" data-anchor-id="linear-maps-matrices-change-of-basis">2.2 Linear Maps, Matrices, Change of Basis</h3>
<p>Having reviewed vector spaces, we now turn to linear maps-the main actors in linear algebra.</p>
<section id="linear-maps" class="level4">
<h4 class="anchored" data-anchor-id="linear-maps">Linear Maps</h4>
<p>A linear map <span class="math inline">\(T: V \to W\)</span> between vector spaces satisfies:</p>
<p><span class="math display">\[
T(av + bw) = aT(v) + bT(w),
\]</span></p>
<p>for all scalars <span class="math inline">\(a,b\)</span> and vectors <span class="math inline">\(v,w \in V\)</span>.</p>
<p>Examples:</p>
<ul>
<li>Scaling: <span class="math inline">\(T(x) = 3x\)</span>.</li>
<li>Rotation: <span class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> rotates vectors by 90°.</li>
<li>Derivative: <span class="math inline">\(D: P_3 \to P_2\)</span> (maps a polynomial of degree ≤ 3 to its derivative).</li>
</ul>
<p>Linear maps preserve the structure of vector spaces.</p>
</section>
<section id="matrices-as-representations" class="level4">
<h4 class="anchored" data-anchor-id="matrices-as-representations">Matrices as Representations</h4>
<p>Given bases of <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, a linear map <span class="math inline">\(T: V \to W\)</span> can be represented by a matrix.</p>
<ul>
<li>Columns of the matrix are just the images of the basis vectors of <span class="math inline">\(V\)</span>.</li>
<li>If <span class="math inline">\(T(e_i) = \sum_j a_{ji} f_j\)</span>, then the matrix entries are <span class="math inline">\(a_{ji}\)</span>.</li>
</ul>
<p>Thus, matrices are coordinate-based representations of abstract linear maps.</p>
</section>
<section id="composition-and-matrix-multiplication" class="level4">
<h4 class="anchored" data-anchor-id="composition-and-matrix-multiplication">Composition and Matrix Multiplication</h4>
<ul>
<li>Composing linear maps corresponds to multiplying their matrices.</li>
<li>The identity map corresponds to the identity matrix.</li>
<li>Inverse maps correspond to inverse matrices (when they exist).</li>
</ul>
<p>This makes linear maps concrete and computable.</p>
</section>
<section id="change-of-basis" class="level4">
<h4 class="anchored" data-anchor-id="change-of-basis">Change of Basis</h4>
<p>Suppose we change basis in a vector space <span class="math inline">\(V\)</span>:</p>
<ul>
<li>Old basis: <span class="math inline">\(\{e_1, \dots, e_n\}\)</span>.</li>
<li>New basis: <span class="math inline">\(\{e'_1, \dots, e'_n\}\)</span>.</li>
</ul>
<p>The change-of-basis matrix <span class="math inline">\(P\)</span> expresses each new basis vector as a combination of the old ones.</p>
<p>For a linear operator <span class="math inline">\(T: V \to V\)</span>:</p>
<p><span class="math display">\[
[T]_{new} = P^{-1} [T]_{old} P.
\]</span></p>
<p>This formula is fundamental for tensors, since tensors must transform consistently under basis changes.</p>
</section>
<section id="why-this-matters-for-multilinear-algebra-1" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-multilinear-algebra-1">Why This Matters for Multilinear Algebra</h4>
<ul>
<li>Linear maps are 2nd-order tensors.</li>
<li>Understanding how matrices change under new coordinates sets the stage for how higher-order tensors transform.</li>
<li>The concept of basis change ensures that tensors encode *intrinsic- information, not just numbers in an array.</li>
</ul>
</section>
</section>
<section id="exercises-5" class="level3">
<h3 class="anchored" data-anchor-id="exercises-5">Exercises</h3>
<ol type="1">
<li><p>Linear or Not? Decide whether each map is linear:</p>
<ul>
<li><ol type="a">
<li><span class="math inline">\(T(x,y) = (2x,3y)\)</span>.</li>
</ol></li>
<li><ol start="2" type="a">
<li><span class="math inline">\(S(x,y) = (x^2,y)\)</span>.</li>
</ol></li>
<li><ol start="3" type="a">
<li><span class="math inline">\(R(x,y) = (y,x)\)</span>.</li>
</ol></li>
</ul></li>
<li><p>Matrix Representation: Let <span class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> be defined by <span class="math inline">\(T(x,y) = (x+2y,3x+y)\)</span>. Find the matrix of <span class="math inline">\(T\)</span> with respect to the standard basis.</p></li>
<li><p>Composition Practice: If <span class="math inline">\(A = \begin{bmatrix}1 &amp; 2\\0 &amp; 1\end{bmatrix}\)</span> and <span class="math inline">\(B = \begin{bmatrix}0 &amp; 1\\1 &amp; 0\end{bmatrix}\)</span>, compute <span class="math inline">\(AB\)</span>. Interpret the action of <span class="math inline">\(AB\)</span> as a linear map.</p></li>
<li><p>Change of Basis: In <span class="math inline">\(\mathbb{R}^2\)</span>, let the old basis be <span class="math inline">\(e_1 = (1,0), e_2=(0,1)\)</span>. The new basis is <span class="math inline">\(e'_1=(1,1), e'_2=(1,-1)\)</span>.</p>
<ul>
<li>Find the change-of-basis matrix <span class="math inline">\(P\)</span>.</li>
<li>Verify that <span class="math inline">\(P^{-1}\)</span> transforms coordinates back to the old basis.</li>
</ul></li>
<li><p>Tensor Connection: Explain why a linear map <span class="math inline">\(T: V \to W\)</span> can be viewed as an element of <span class="math inline">\(V^- \otimes W\)</span>. (Hint: it eats a vector and produces another vector, which can be encoded by pairing with covectors.)</p></li>
</ol>
</section>
<section id="inner-products-dual-spaces-adjoint" class="level3">
<h3 class="anchored" data-anchor-id="inner-products-dual-spaces-adjoint">2.3 Inner Products, Dual Spaces, Adjoint</h3>
<p>Linear maps and vector spaces give the structure. To measure <em>angles, lengths, and projections</em>, we need inner products. To generalize “coordinates” beyond a chosen basis, we need dual spaces. These two ideas connect directly in multilinear algebra.</p>
<section id="inner-products" class="level4">
<h4 class="anchored" data-anchor-id="inner-products">Inner Products</h4>
<p>An inner product on a real vector space <span class="math inline">\(V\)</span> is a function</p>
<p><span class="math display">\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]</span></p>
<p>satisfying:</p>
<ol type="1">
<li>Linearity in each slot: <span class="math inline">\(\langle av+ bw, u\rangle = a\langle v,u\rangle + b\langle w,u\rangle\)</span>.</li>
<li>Symmetry: <span class="math inline">\(\langle v, w \rangle = \langle w, v \rangle\)</span>.</li>
<li>Positive definiteness: <span class="math inline">\(\langle v,v\rangle \geq 0\)</span>, with equality only when <span class="math inline">\(v=0\)</span>.</li>
</ol>
<p>This structure gives:</p>
<ul>
<li>Length: <span class="math inline">\(\|v\| = \sqrt{\langle v,v\rangle}\)</span>.</li>
<li>Angle: <span class="math inline">\(\cos\theta = \frac{\langle v,w\rangle}{\|v\|\|w\|}\)</span>.</li>
<li>Orthogonality: <span class="math inline">\(\langle v,w\rangle=0\)</span>.</li>
</ul>
<p>Example: the dot product in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</section>
<section id="dual-spaces" class="level4">
<h4 class="anchored" data-anchor-id="dual-spaces">Dual Spaces</h4>
<p>The dual space <span class="math inline">\(V^*\)</span> is the set of all linear functionals <span class="math inline">\(f: V \to \mathbb{R}\)</span>.</p>
<ul>
<li><p>Elements of <span class="math inline">\(V^*\)</span> are called covectors.</p></li>
<li><p>If <span class="math inline">\(V=\mathbb{R}^n\)</span>, then <span class="math inline">\(V^- \cong \mathbb{R}^n\)</span>, but conceptually they are different:</p>
<ul>
<li>Vectors: “arrows” in space.</li>
<li>Covectors: “measuring devices” that output numbers when fed a vector.</li>
</ul></li>
</ul>
<p>The dual basis:</p>
<ul>
<li><p>If <span class="math inline">\(\{e_1,\dots,e_n\}\)</span> is a basis of <span class="math inline">\(V\)</span>, then there is a unique dual basis <span class="math inline">\(\{e^1,\dots,e^n\}\)</span> in <span class="math inline">\(V^*\)</span> with</p>
<p><span class="math display">\[
e^i(e_j) = \delta^i_j.
\]</span></p></li>
</ul>
<p>This duality underpins how tensor indices “live up or down” (contravariant vs.&nbsp;covariant).</p>
</section>
<section id="adjoint-of-a-linear-map" class="level4">
<h4 class="anchored" data-anchor-id="adjoint-of-a-linear-map">Adjoint of a Linear Map</h4>
<p>Given a linear map <span class="math inline">\(T: V \to V\)</span> on an inner product space, the adjoint <span class="math inline">\(T^*\)</span> is defined by:</p>
<p><span class="math display">\[
\langle Tv, w \rangle = \langle v, T^*w \rangle \quad \forall v,w \in V.
\]</span></p>
<ul>
<li>If <span class="math inline">\(T\)</span> is represented by a matrix <span class="math inline">\(A\)</span> in an orthonormal basis, then <span class="math inline">\(T^*\)</span> corresponds to the transpose <span class="math inline">\(A^\top\)</span>.</li>
<li>Adjoint maps generalize the idea of “transpose” to arbitrary inner product spaces.</li>
</ul>
</section>
<section id="why-this-matters-for-multilinear-algebra-2" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-multilinear-algebra-2">Why This Matters for Multilinear Algebra</h4>
<ul>
<li>Inner products allow us to raise or lower indices (switch between vectors and covectors).</li>
<li>Dual spaces are essential for defining general tensors (mixing vectors and covectors).</li>
<li>Adjoint operators appear everywhere in applications: projections, least squares, and symmetry in physical laws.</li>
</ul>
</section>
<section id="exercises-6" class="level4">
<h4 class="anchored" data-anchor-id="exercises-6">Exercises</h4>
<ol type="1">
<li><p>Inner Product Verification: Show that <span class="math inline">\(\langle (x_1,y_1),(x_2,y_2)\rangle = 2x_1x_2 + y_1y_2\)</span> defines an inner product on <span class="math inline">\(\mathbb{R}^2\)</span>.</p></li>
<li><p>Length and Angle: For <span class="math inline">\(u=(1,2,2)\)</span> and <span class="math inline">\(v=(2,0,1)\)</span> in <span class="math inline">\(\mathbb{R}^3\)</span>, compute:</p>
<ul>
<li><span class="math inline">\(\|u\|\)</span>, <span class="math inline">\(\|v\|\)</span>.</li>
<li>The cosine of the angle between them.</li>
</ul></li>
<li><p>Dual Basis: Let <span class="math inline">\(V=\mathbb{R}^2\)</span> with basis <span class="math inline">\(e_1=(1,0), e_2=(0,1)\)</span>.</p>
<ul>
<li>Write the dual basis <span class="math inline">\(e^1, e^2\)</span>.</li>
<li>Compute <span class="math inline">\(e^1(3,4)\)</span> and <span class="math inline">\(e^2(3,4)\)</span>.</li>
</ul></li>
<li><p>Adjoint Map: Let <span class="math inline">\(T:\mathbb{R}^2 \to \mathbb{R}^2\)</span> with matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 0 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Find the matrix of <span class="math inline">\(T^*\)</span> under the standard dot product.</p></li>
<li><p>Tensor Connection: Explain why an element of <span class="math inline">\(V^- \otimes V\)</span> can be interpreted as a matrix, and why adjointness naturally appears when working with inner products.</p></li>
</ol>
</section>
</section>
<section id="bilinear-forms-and-quadratic-forms" class="level3">
<h3 class="anchored" data-anchor-id="bilinear-forms-and-quadratic-forms">2.4 Bilinear Forms and Quadratic Forms</h3>
<p>Now that we have inner products and dual spaces, we can introduce two important types of multilinear maps that already appear in basic linear algebra: bilinear forms and quadratic forms.</p>
<section id="bilinear-forms" class="level4">
<h4 class="anchored" data-anchor-id="bilinear-forms">Bilinear Forms</h4>
<p>A bilinear form on a vector space <span class="math inline">\(V\)</span> is a function</p>
<p><span class="math display">\[
B: V \times V \to \mathbb{R}
\]</span></p>
<p>that is linear in each argument separately:</p>
<ul>
<li><span class="math inline">\(B(av_1 + bv_2, w) = aB(v_1, w) + bB(v_2, w)\)</span>,</li>
<li><span class="math inline">\(B(v, aw_1 + bw_2) = aB(v, w_1) + bB(v, w_2)\)</span>.</li>
</ul>
<p>Examples:</p>
<ul>
<li><p>Dot product: <span class="math inline">\(\langle v,w\rangle\)</span> is symmetric and bilinear.</p></li>
<li><p>Matrix form: Any matrix <span class="math inline">\(A\)</span> defines a bilinear form by</p>
<p><span class="math display">\[
B(v,w) = v^\top A w.
\]</span></p></li>
</ul>
<p>Properties:</p>
<ul>
<li>Symmetric: if <span class="math inline">\(B(v,w) = B(w,v)\)</span>.</li>
<li>Skew-symmetric: if <span class="math inline">\(B(v,w) = -B(w,v)\)</span>.</li>
</ul>
</section>
<section id="quadratic-forms" class="level4">
<h4 class="anchored" data-anchor-id="quadratic-forms">Quadratic Forms</h4>
<p>A quadratic form is a special case obtained by feeding the *same- vector into both slots of a bilinear form:</p>
<p><span class="math display">\[
Q(v) = B(v,v).
\]</span></p>
<p>In coordinates, with a matrix <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
Q(v) = v^\top A v.
\]</span></p>
<p>Examples:</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math inline">\(Q(x,y) = x^2 + y^2\)</span> corresponds to the identity matrix.</li>
<li>In optimization, quadratic forms represent energy functions, error functions, or cost functions.</li>
</ul>
</section>
<section id="geometric-meaning" class="level4">
<h4 class="anchored" data-anchor-id="geometric-meaning">Geometric Meaning</h4>
<ul>
<li>Quadratic forms define conic sections (ellipses, hyperbolas, parabolas) in 2D and quadric surfaces in higher dimensions.</li>
<li>They also measure “curvature” locally in multivariable functions (via the Hessian matrix).</li>
</ul>
</section>
<section id="why-this-matters-for-multilinear-algebra-3" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-multilinear-algebra-3">Why This Matters for Multilinear Algebra</h4>
<ul>
<li>Bilinear forms are 2nd-order tensors: one covector for each input.</li>
<li>Quadratic forms connect multilinearity with geometry (shapes, volumes, energy).</li>
<li>The distinction between symmetric and skew-symmetric bilinear forms leads to exterior algebra and inner product structures later.</li>
</ul>
</section>
<section id="exercises-7" class="level4">
<h4 class="anchored" data-anchor-id="exercises-7">Exercises</h4>
<ol type="1">
<li><p>Matrix Bilinear Form: Let</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\ 1 &amp; 3\end{bmatrix}.
\]</span></p>
<p>Define <span class="math inline">\(B(v,w) = v^\top A w\)</span>.</p>
<ul>
<li>Compute <span class="math inline">\(B((1,0),(0,1))\)</span>.</li>
<li>Compute <span class="math inline">\(B((1,2),(3,4))\)</span>.</li>
</ul></li>
<li><p>Symmetry Check: For the above <span class="math inline">\(B\)</span>, show that <span class="math inline">\(B(v,w) = B(w,v)\)</span>.</p></li>
<li><p>Quadratic Form: Compute <span class="math inline">\(Q(x,y) = [x \; y] A [x \; y]^\top\)</span> for the same matrix <span class="math inline">\(A\)</span>. Write the explicit formula.</p></li>
<li><p>Geometric Interpretation: Consider <span class="math inline">\(Q(x,y) = 4x^2 + y^2\)</span>. Sketch (or describe) the curve <span class="math inline">\(Q(x,y)=1\)</span>. What kind of conic section is it?</p></li>
<li><p>Tensor Connection: Explain why a bilinear form <span class="math inline">\(B: V \times V \to \mathbb{R}\)</span> can be seen as an element of <span class="math inline">\(V^- \otimes V^*\)</span>. What does this mean in terms of indices (covariant slots)?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-ii.-tensors" class="level1">
<h1>Part II. Tensors</h1>
<section id="chapter-3.-tensors-as-indexed-arrays" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.-tensors-as-indexed-arrays">Chapter 3. Tensors as Indexed Arrays</h2>
<section id="order-arity-shape-and-indices" class="level3">
<h3 class="anchored" data-anchor-id="order-arity-shape-and-indices">3.1 Order (Arity), Shape, and Indices</h3>
<p>We now begin exploring tensors directly, starting with the array viewpoint. This perspective treats tensors as multi-dimensional generalizations of matrices.</p>
<section id="order-arity-of-a-tensor" class="level4">
<h4 class="anchored" data-anchor-id="order-arity-of-a-tensor">Order (Arity) of a Tensor</h4>
<p>The order (or arity) of a tensor is the number of indices needed to locate one of its entries.</p>
<ul>
<li>0th-order: A scalar, e.g.&nbsp;<span class="math inline">\(5\)</span>.</li>
<li>1st-order: A vector <span class="math inline">\(v_i\)</span>, with one index.</li>
<li>2nd-order: A matrix <span class="math inline">\(A_{ij}\)</span>, with two indices.</li>
<li>3rd-order: A block of numbers <span class="math inline">\(T_{ijk}\)</span>.</li>
<li>kth-order: An array with <span class="math inline">\(k\)</span> indices.</li>
</ul>
<p>Thus, the order tells us how many “directions” or “modes” the tensor has.</p>
</section>
<section id="shape-dimensions-of-each-mode" class="level4">
<h4 class="anchored" data-anchor-id="shape-dimensions-of-each-mode">Shape (Dimensions of Each Mode)</h4>
<p>Each index ranges over some set of values, defining the shape of the tensor.</p>
<ul>
<li>A vector in <span class="math inline">\(\mathbb{R}^n\)</span> has shape <span class="math inline">\((n)\)</span>.</li>
<li>A matrix of size <span class="math inline">\(m \times n\)</span> has shape <span class="math inline">\((m,n)\)</span>.</li>
<li>A color image of size <span class="math inline">\(100 \times 200\)</span> with 3 channels has shape <span class="math inline">\((100,200,3)\)</span>.</li>
</ul>
<p>The shape is just the list of sizes of each mode.</p>
</section>
<section id="indices-and-notation" class="level4">
<h4 class="anchored" data-anchor-id="indices-and-notation">Indices and Notation</h4>
<p>Indices label positions in the tensor:</p>
<p><span class="math display">\[
T_{i_1 i_2 \dots i_k}.
\]</span></p>
<p>Example: A 3rd-order tensor <span class="math inline">\(T_{ijk}\)</span> with shape <span class="math inline">\((2,3,4)\)</span> has:</p>
<ul>
<li><span class="math inline">\(i \in \{1,2\}\)</span>,</li>
<li><span class="math inline">\(j \in \{1,2,3\}\)</span>,</li>
<li><span class="math inline">\(k \in \{1,2,3,4\}\)</span>.</li>
</ul>
<p>So it contains <span class="math inline">\(2 \times 3 \times 4 = 24\)</span> entries.</p>
</section>
<section id="visual-intuition" class="level4">
<h4 class="anchored" data-anchor-id="visual-intuition">Visual Intuition</h4>
<ul>
<li>Scalars are points.</li>
<li>Vectors are arrows (1D arrays).</li>
<li>Matrices are grids (2D arrays).</li>
<li>Higher-order tensors are cubes, hypercubes, or higher-dimensional arrays, which we can’t fully draw but can still manipulate symbolically.</li>
</ul>
</section>
<section id="why-this-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters">Why This Matters</h4>
<ul>
<li>The array view is the most concrete: you can store tensors in memory, index them, and manipulate them in code.</li>
<li>It provides the language of shapes that data science, ML, and physics use constantly.</li>
<li>Later, we will see that these indices correspond to *slots- in multilinear maps and tensor product spaces.</li>
</ul>
</section>
<section id="exercises-8" class="level4">
<h4 class="anchored" data-anchor-id="exercises-8">Exercises</h4>
<ol type="1">
<li><p>Counting Entries: How many entries does a tensor of shape <span class="math inline">\((3,4,5)\)</span> have?</p></li>
<li><p>Order Identification: Identify the order and shape of each object:</p>
<ul>
<li><ol type="a">
<li>A grayscale image <span class="math inline">\(64 \times 64\)</span>.</li>
</ol></li>
<li><ol start="2" type="a">
<li>A video: 30 frames, each <span class="math inline">\(128 \times 128\)</span> RGB.</li>
</ol></li>
<li><ol start="3" type="a">
<li>A dataset with 1000 samples, each a <span class="math inline">\(20 \times 20\)</span> grayscale image.</li>
</ol></li>
</ul></li>
<li><p>Index Practice: For a tensor <span class="math inline">\(T_{ijk}\)</span> with shape <span class="math inline">\((2,2,2)\)</span>, list explicitly all the index triples <span class="math inline">\((i,j,k)\)</span>.</p></li>
<li><p>Shape Transformation: Flatten a tensor with shape <span class="math inline">\((5,4,3)\)</span> into a matrix. What two possible shapes could the matrix have (depending on how you group the indices)?</p></li>
<li><p>Thought Experiment: Why is a scalar sometimes called a “0th-order tensor”? How does this viewpoint help unify the hierarchy of scalars, vectors, matrices, and higher-order tensors?</p></li>
</ol>
</section>
</section>
<section id="covariant-vs.-contravariant-indices" class="level3">
<h3 class="anchored" data-anchor-id="covariant-vs.-contravariant-indices">3.2 Covariant vs.&nbsp;Contravariant Indices</h3>
<p>So far, we’ve treated indices as simple “positions in an array.” But in multilinear algebra, indices carry roles. Some belong to vectors (contravariant), others to covectors (covariant). Understanding this distinction is crucial for how tensors behave under change of basis.</p>
<section id="vectors-vs.-covectors" class="level4">
<h4 class="anchored" data-anchor-id="vectors-vs.-covectors">Vectors vs.&nbsp;Covectors</h4>
<ul>
<li>Vectors are elements of a space <span class="math inline">\(V\)</span>. They transform with the basis.</li>
<li>Covectors (linear functionals) are elements of the dual space <span class="math inline">\(V^*\)</span>. They transform with the *inverse transpose- of the basis change.</li>
</ul>
<p>This leads to two kinds of indices:</p>
<ul>
<li>Contravariant indices (upper): <span class="math inline">\(v^i\)</span>, coordinates of a vector.</li>
<li>Covariant indices (lower): <span class="math inline">\(\omega_j\)</span>, coordinates of a covector.</li>
</ul>
</section>
<section id="tensors-mixing-both" class="level4">
<h4 class="anchored" data-anchor-id="tensors-mixing-both">Tensors Mixing Both</h4>
<p>A tensor may have both types of indices:</p>
<p><span class="math display">\[
T^{i_1 i_2 \dots i_p}_{j_1 j_2 \dots j_q},
\]</span></p>
<p>which means it accepts <span class="math inline">\(q\)</span> vectors and <span class="math inline">\(p\)</span> covectors as inputs (or outputs, depending on interpretation).</p>
<p>Examples:</p>
<ul>
<li>A vector <span class="math inline">\(v^i\)</span> → contravariant (upper index).</li>
<li>A covector <span class="math inline">\(\omega_j\)</span> → covariant (lower index).</li>
<li>A bilinear form <span class="math inline">\(B_{ij}\)</span> → two covariant indices.</li>
<li>A linear map <span class="math inline">\(A^i{}_j\)</span> → one up and one down (it eats a vector, gives back a vector).</li>
</ul>
</section>
<section id="why-two-types" class="level4">
<h4 class="anchored" data-anchor-id="why-two-types">Why Two Types?</h4>
<p>This distinction is not cosmetic:</p>
<ul>
<li>When we change basis, vectors and covectors transform in “opposite” ways.</li>
<li>Having both ensures that tensor equations describe intrinsic relationships, independent of coordinates.</li>
</ul>
<p>Example: Inner product</p>
<p><span class="math display">\[
\langle v, w \rangle = g_{ij} v^i w^j.
\]</span></p>
<p>Here <span class="math inline">\(g_{ij}\)</span> is a metric tensor (covariant), combining two contravariant vectors into a scalar.</p>
</section>
<section id="pictures-before-symbols" class="level4">
<h4 class="anchored" data-anchor-id="pictures-before-symbols">Pictures Before Symbols</h4>
<ul>
<li>Contravariant: arrows pointing “outward” (directions in space).</li>
<li>Covariant: measuring devices pointing “inward” (hyperplanes that assign numbers to arrows).</li>
<li>Tensors: diagrams with arrows in and out, representing how they connect inputs to outputs.</li>
</ul>
<p>This picture-based intuition helps prevent index mistakes when writing formulas.</p>
</section>
<section id="why-this-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-1">Why This Matters</h4>
<ul>
<li>Covariant vs.&nbsp;contravariant indices explain the geometry of tensors, not just their array form.</li>
<li>It prepares us for raising and lowering indices with inner products.</li>
<li>It ensures we can handle basis changes correctly (Chapter 12 will revisit this in detail).</li>
</ul>
</section>
<section id="exercises-9" class="level4">
<h4 class="anchored" data-anchor-id="exercises-9">Exercises</h4>
<ol type="1">
<li><p>Identify Index Type: For each object, say whether its indices are covariant, contravariant, or mixed:</p>
<ul>
<li><ol type="a">
<li><span class="math inline">\(v^i\)</span>.</li>
</ol></li>
<li><ol start="2" type="a">
<li><span class="math inline">\(\omega_j\)</span>.</li>
</ol></li>
<li><ol start="3" type="a">
<li><span class="math inline">\(A^i{}_j\)</span>.</li>
</ol></li>
<li><ol start="4" type="a">
<li><span class="math inline">\(B_{ij}\)</span>.</li>
</ol></li>
</ul></li>
<li><p>Dual Basis Practice: In <span class="math inline">\(\mathbb{R}^2\)</span> with basis <span class="math inline">\(e_1, e_2\)</span> and dual basis <span class="math inline">\(e^1, e^2\)</span>:</p>
<ul>
<li>Write a vector <span class="math inline">\(v = 3e_1 + 4e_2\)</span> in coordinates <span class="math inline">\(v^i\)</span>.</li>
<li>Evaluate <span class="math inline">\(\omega(v)\)</span> for <span class="math inline">\(\omega = 2e^1 - e^2\)</span>.</li>
</ul></li>
<li><p>Basis Change Intuition: Suppose we scale the basis of <span class="math inline">\(\mathbb{R}^2\)</span> by 2: <span class="math inline">\(e'_i = 2 e_i\)</span>.</p>
<ul>
<li>How do the contravariant coordinates <span class="math inline">\(v^i\)</span> of a vector change?</li>
<li>How do the covariant coordinates <span class="math inline">\(\omega_i\)</span> of a covector change?</li>
</ul></li>
<li><p>Mixed Tensor Example: Interpret the meaning of a tensor <span class="math inline">\(T^i{}_j\)</span> acting on a vector <span class="math inline">\(v^j\)</span>. What kind of object is the result?</p></li>
<li><p>Thought Experiment: Why do we need both contravariant and covariant indices to describe something like the dot product? What would go wrong if we only allowed one type?</p></li>
</ol>
</section>
</section>
<section id="change-of-basis-rules-in-coordinates" class="level3">
<h3 class="anchored" data-anchor-id="change-of-basis-rules-in-coordinates">3.3 Change of Basis Rules in Coordinates</h3>
<p>So far we have seen that indices can be contravariant (upper) or covariant (lower). The key difference appears when we change basis. Multilinear algebra is all about writing rules that remain valid regardless of coordinates, and basis transformations reveal why the distinction is essential.</p>
<section id="vectors-under-change-of-basis" class="level4">
<h4 class="anchored" data-anchor-id="vectors-under-change-of-basis">Vectors Under Change of Basis</h4>
<p>Let <span class="math inline">\(V = \mathbb{R}^n\)</span>. Suppose we change from an old basis <span class="math inline">\(\{e_i\}\)</span> to a new basis <span class="math inline">\(\{e'_i\}\)</span>:</p>
<p><span class="math display">\[
e'_i = P^j{}_i e_j,
\]</span></p>
<p>where <span class="math inline">\(P\)</span> is the change-of-basis matrix.</p>
<ul>
<li>A vector <span class="math inline">\(v\)</span> has coordinates <span class="math inline">\(v^i\)</span> in the old basis and <span class="math inline">\(v'^i\)</span> in the new basis.</li>
<li>The relation is:</li>
</ul>
<p><span class="math display">\[
v'^i = (P^{-1})^i{}_j v^j.
\]</span></p>
<p>Thus, contravariant components transform with the inverse of the basis change.</p>
</section>
<section id="covectors-under-change-of-basis" class="level4">
<h4 class="anchored" data-anchor-id="covectors-under-change-of-basis">Covectors Under Change of Basis</h4>
<p>Now consider a covector <span class="math inline">\(\omega \in V^*\)</span>, expressed in the old basis as <span class="math inline">\(\omega_i\)</span>. Under the same change of basis:</p>
<p><span class="math display">\[
\omega'_i = P^j{}_i \, \omega_j.
\]</span></p>
<p>Thus, covariant components transform directly with the basis change matrix.</p>
</section>
<section id="general-tensor-transformation" class="level4">
<h4 class="anchored" data-anchor-id="general-tensor-transformation">General Tensor Transformation</h4>
<p>A tensor with both covariant and contravariant indices transforms by applying these rules to each index:</p>
<p><span class="math display">\[
T'^{i_1 \dots i_p}{}_{j_1 \dots j_q} =
(P^{-1})^{i_1}{}_{k_1} \cdots (P^{-1})^{i_p}{}_{k_p}
\, P^{\ell_1}{}_{j_1} \cdots P^{\ell_q}{}_{j_q}
\, T^{k_1 \dots k_p}{}_{\ell_1 \dots \ell_q}.
\]</span></p>
<ul>
<li>Upper indices get <span class="math inline">\(P^{-1}\)</span>.</li>
<li>Lower indices get <span class="math inline">\(P\)</span>.</li>
<li>This ensures tensor equations remain coordinate-independent.</li>
</ul>
</section>
<section id="simple-example-linear-maps" class="level4">
<h4 class="anchored" data-anchor-id="simple-example-linear-maps">Simple Example: Linear Maps</h4>
<p>A linear map <span class="math inline">\(A: V \to V\)</span> has components <span class="math inline">\(A^i{}_j\)</span>. Under change of basis:</p>
<p><span class="math display">\[
A'^i{}_j = (P^{-1})^i{}_k \, A^k{}_\ell \, P^\ell{}_j.
\]</span></p>
<p>This is the familiar similarity transformation:</p>
<p><span class="math display">\[
[A]_{new} = P^{-1} [A]_{old} P.
\]</span></p>
</section>
<section id="why-this-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-2">Why This Matters</h4>
<ul>
<li>Basis changes test whether your formulas are intrinsic or just coordinate artifacts.</li>
<li>The distinction between covariant and contravariant is precisely what makes tensor equations survive basis changes intact.</li>
<li>In physics, this explains why laws (like Maxwell’s equations or stress-strain relations) remain valid no matter what coordinates we choose.</li>
</ul>
</section>
<section id="exercises-10" class="level4">
<h4 class="anchored" data-anchor-id="exercises-10">Exercises</h4>
<ol type="1">
<li><p>Vector Transformation: Let <span class="math inline">\(P = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix}\)</span>.</p>
<ul>
<li>If <span class="math inline">\(v = (4,6)\)</span> in the old basis, what are its coordinates in the new basis?</li>
</ul></li>
<li><p>Covector Transformation: With the same <span class="math inline">\(P\)</span>, let <span class="math inline">\(\omega = (1,2)\)</span> in the old basis. What are its coordinates in the new basis?</p></li>
<li><p>Matrix Transformation: Let</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 0 &amp; 1\end{bmatrix}, \quad
P = \begin{bmatrix}1 &amp; 1 \\ 0 &amp; 1\end{bmatrix}.
\]</span></p>
<p>Compute <span class="math inline">\(A' = P^{-1} A P\)</span>.</p></li>
<li><p>Tensor Component Count: How many transformation matrices <span class="math inline">\(P\)</span> and <span class="math inline">\(P^{-1}\)</span> appear in the formula for a tensor of type <span class="math inline">\((2,1)\)</span>?</p></li>
<li><p>Thought Experiment: Why would formulas break if we treated all indices as the same type (ignoring covariant vs.&nbsp;contravariant)? Consider the dot product as an example.</p></li>
</ol>
</section>
</section>
<section id="einstein-summation-and-index-hygiene" class="level3">
<h3 class="anchored" data-anchor-id="einstein-summation-and-index-hygiene">3.4 Einstein Summation and Index Hygiene</h3>
<p>When working with tensors, writing every summation explicitly quickly becomes messy. To keep formulas clean, mathematicians and physicists use the Einstein summation convention and a set of informal “index hygiene” rules.</p>
<section id="einstein-summation-convention" class="level4">
<h4 class="anchored" data-anchor-id="einstein-summation-convention">Einstein Summation Convention</h4>
<p>The rule is simple: whenever an index appears once up and once down, you sum over it.</p>
<p>Example in <span class="math inline">\(\mathbb{R}^3\)</span>:</p>
<p><span class="math display">\[
y^i = A^i{}_j x^j
\]</span></p>
<p>means</p>
<p><span class="math display">\[
y^i = \sum_{j=1}^3 A^i{}_j x^j.
\]</span></p>
<p>This compact notation hides the summation symbol but makes multilinear expressions much easier to read.</p>
</section>
<section id="free-vs.-dummy-indices" class="level4">
<h4 class="anchored" data-anchor-id="free-vs.-dummy-indices">Free vs.&nbsp;Dummy Indices</h4>
<ul>
<li>Free indices: appear only once in a term; they label the components of the result.</li>
<li>Dummy indices: appear exactly twice (once up, once down); they are summed over and can be renamed arbitrarily.</li>
</ul>
<p>Example:</p>
<p><span class="math display">\[
z^i = B^i{}_j x^j
\]</span></p>
<p>Here <span class="math inline">\(i\)</span> is free (labels components of <span class="math inline">\(z\)</span>), and <span class="math inline">\(j\)</span> is a dummy index (summed).</p>
</section>
<section id="index-hygiene-rules" class="level4">
<h4 class="anchored" data-anchor-id="index-hygiene-rules">Index Hygiene Rules</h4>
<ol type="1">
<li>Never use the same index more than twice in a term.</li>
<li>Never mix up free and dummy indices.</li>
<li>Rename dummy indices freely if it helps clarity.</li>
</ol>
<p>Example of bad hygiene:</p>
<p><span class="math display">\[
A^i{}_i \, x^i
\]</span></p>
<p>This is ambiguous, because <span class="math inline">\(i\)</span> appears three times. Correct it by renaming:</p>
<p><span class="math display">\[
(A^i{}_i) \, x^j.
\]</span></p>
</section>
<section id="examples-of-einstein-notation" class="level4">
<h4 class="anchored" data-anchor-id="examples-of-einstein-notation">Examples of Einstein Notation</h4>
<ul>
<li>Dot product: <span class="math inline">\(\langle v,w \rangle = v^i w_i\)</span>.</li>
<li>Matrix-vector product: <span class="math inline">\(y^i = A^i{}_j x^j\)</span>.</li>
<li>Bilinear form: <span class="math inline">\(B(v,w) = B_{ij} v^i w^j\)</span>.</li>
<li>Trace of a matrix: <span class="math inline">\(\mathrm{tr}(A) = A^i{}_i\)</span>.</li>
</ul>
</section>
<section id="why-this-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-3">Why This Matters</h4>
<ul>
<li>Einstein summation is the language of tensors: concise, unambiguous, and basis-independent.</li>
<li>It allows formulas to be read structurally, focusing on how indices connect rather than on summation symbols.</li>
<li>Practicing good index hygiene prevents mistakes when manipulating complicated expressions.</li>
</ul>
</section>
<section id="exercises-11" class="level4">
<h4 class="anchored" data-anchor-id="exercises-11">Exercises</h4>
<ol type="1">
<li><p>Summation Practice: Expand the Einstein-summed expression</p>
<p><span class="math display">\[
y^i = A^i{}_j x^j
\]</span></p>
<p>explicitly for <span class="math inline">\(i=1,2\)</span> when</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}, \quad x=(5,6).
\]</span></p></li>
<li><p>Dot Product Check: Show that <span class="math inline">\(v^i w_i\)</span> is invariant under a change of dummy index name (e.g.&nbsp;rewrite with <span class="math inline">\(j\)</span> instead of <span class="math inline">\(i\)</span>).</p></li>
<li><p>Trace Calculation: For</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 1 \\ 0 &amp; -3\end{bmatrix},
\]</span></p>
<p>compute <span class="math inline">\(\mathrm{tr}(A)\)</span> using Einstein notation.</p></li>
<li><p>Index Hygiene: Identify the mistake in the following expression and correct it:</p>
<p><span class="math display">\[
C^i = A^i{}_j B^j{}_j x^j.
\]</span></p></li>
<li><p>Thought Experiment: Why does Einstein notation require one index up and one down to imply summation? What would go wrong if we summed whenever indices simply repeated, regardless of position?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-4.-tensors-as-multilinear-maps" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.-tensors-as-multilinear-maps">Chapter 4. Tensors as Multilinear Maps</h2>
<section id="multilinearity-and-currying" class="level3">
<h3 class="anchored" data-anchor-id="multilinearity-and-currying">4.1 Multilinearity and Currying</h3>
<p>So far, we treated tensors as arrays of numbers. Now we switch to the map viewpoint: tensors as functions that are linear in each argument separately. This is the most natural way to see how tensors <em>act</em>.</p>
<section id="what-does-multilinear-mean" class="level4">
<h4 class="anchored" data-anchor-id="what-does-multilinear-mean">What Does Multilinear Mean?</h4>
<p>A map <span class="math inline">\(T: V_1 \times V_2 \times \cdots \times V_k \to \mathbb{R}\)</span> (or to another vector space) is multilinear if it is linear in each slot, while the others are fixed.</p>
<p>Example (bilinear):</p>
<p><span class="math display">\[
B(av_1 + bv_2, w) = a B(v_1, w) + b B(v_2, w).
\]</span></p>
<p>The same rule holds in each argument.</p>
<ul>
<li>Linear → 1 slot.</li>
<li>Bilinear → 2 slots.</li>
<li>Trilinear → 3 slots.</li>
<li>k-linear → k slots.</li>
</ul>
</section>
<section id="examples-of-multilinear-maps" class="level4">
<h4 class="anchored" data-anchor-id="examples-of-multilinear-maps">Examples of Multilinear Maps</h4>
<ul>
<li>Dot product: <span class="math inline">\(\langle v,w\rangle\)</span>, bilinear.</li>
<li>Matrix-vector action: <span class="math inline">\(A(v)\)</span>, linear (1 slot).</li>
<li>Determinant in <span class="math inline">\(\mathbb{R}^2\)</span>: <span class="math inline">\(\det(u,v) = u_1 v_2 - u_2 v_1\)</span>, bilinear.</li>
<li>Scalar triple product: <span class="math inline">\(u \cdot (v \times w)\)</span>, trilinear.</li>
</ul>
<p>These all satisfy linearity in each argument separately.</p>
</section>
<section id="currying-viewpoint" class="level4">
<h4 class="anchored" data-anchor-id="currying-viewpoint">Currying Viewpoint</h4>
<p>Another way to think about multilinear maps is through currying:</p>
<ul>
<li><p>A bilinear map <span class="math inline">\(B: V \times W \to \mathbb{R}\)</span> can be seen as a function</p>
<p><span class="math display">\[
B(v,-): W \to \mathbb{R}, \quad w \mapsto B(v,w).
\]</span></p>
<p>So, fixing one input gives a linear map in the remaining argument.</p></li>
<li><p>In general, a <span class="math inline">\(k\)</span>-linear map can be seen as a nested sequence of linear maps, each taking one input at a time.</p></li>
</ul>
<p>This perspective helps connect multilinear maps with ordinary linear maps, by viewing them as “linear maps into linear maps.”</p>
</section>
<section id="why-this-matters-4" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-4">Why This Matters</h4>
<ul>
<li>This viewpoint clarifies how tensors can be “evaluated” by plugging in vectors.</li>
<li>It connects with functional programming ideas (currying and partial application).</li>
<li>It bridges between arrays (coordinates) and abstract multilinear functionals.</li>
</ul>
</section>
<section id="exercises-12" class="level4">
<h4 class="anchored" data-anchor-id="exercises-12">Exercises</h4>
<ol type="1">
<li><p>Bilinearity Check: Verify directly that the dot product <span class="math inline">\(\langle (x_1,y_1),(x_2,y_2)\rangle = x_1x_2 + y_1y_2\)</span> is bilinear.</p></li>
<li><p>Currying Example: Let <span class="math inline">\(B(u,v) = u_1v_1 + 2u_2v_2\)</span>. For a fixed <span class="math inline">\(u=(1,2)\)</span>, write the resulting linear functional on <span class="math inline">\(v\)</span>.</p></li>
<li><p>Determinant as Bilinear Form: Show that <span class="math inline">\(\det(u,v) = u_1v_2 - u_2v_1\)</span> is bilinear in <span class="math inline">\(\mathbb{R}^2\)</span>.</p></li>
<li><p>Trilinear Example: Prove that the scalar triple product <span class="math inline">\(u \cdot (v \times w)\)</span> is trilinear by checking linearity in <span class="math inline">\(u\)</span>.</p></li>
<li><p>Thought Experiment: Why might it be useful to think of a bilinear map as a linear map into the dual space, i.e.&nbsp;<span class="math inline">\(B: V \to W^*\)</span>?</p></li>
</ol>
</section>
</section>
<section id="evaluation-with-vectors-and-covectors" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-with-vectors-and-covectors">4.2 Evaluation with Vectors and Covectors</h3>
<p>In the map viewpoint, tensors are best understood by how they act on inputs. Depending on their type (covariant or contravariant indices), tensors expect vectors, covectors, or both.</p>
<section id="feeding-vectors-into-covariant-slots" class="level4">
<h4 class="anchored" data-anchor-id="feeding-vectors-into-covariant-slots">Feeding Vectors into Covariant Slots</h4>
<p>A purely covariant tensor <span class="math inline">\(T_{ij}\)</span> is a multilinear map</p>
<p><span class="math display">\[
T: V \times V \to \mathbb{R}.
\]</span></p>
<ul>
<li>You feed in two vectors <span class="math inline">\(u, v \in V\)</span>.</li>
<li>The output is a scalar: <span class="math inline">\(T(u,v) = T_{ij} u^i v^j\)</span>.</li>
</ul>
<p>Example:</p>
<ul>
<li>A bilinear form (like an inner product) is a covariant 2-tensor.</li>
</ul>
</section>
<section id="feeding-covectors-into-contravariant-slots" class="level4">
<h4 class="anchored" data-anchor-id="feeding-covectors-into-contravariant-slots">Feeding Covectors into Contravariant Slots</h4>
<p>A purely contravariant tensor <span class="math inline">\(T^{ij}\)</span> is a multilinear map</p>
<p><span class="math display">\[
T: V^- \times V^- \to \mathbb{R}.
\]</span></p>
<ul>
<li>You feed in two covectors <span class="math inline">\(\alpha, \beta \in V^*\)</span>.</li>
<li>The output is a scalar: <span class="math inline">\(T(\alpha,\beta) = T^{ij}\alpha_i \beta_j\)</span>.</li>
</ul>
</section>
<section id="mixed-tensors-both-types-of-inputs" class="level4">
<h4 class="anchored" data-anchor-id="mixed-tensors-both-types-of-inputs">Mixed Tensors: Both Types of Inputs</h4>
<p>A mixed tensor <span class="math inline">\(T^i{}_j\)</span> acts as a map:</p>
<p><span class="math display">\[
T: V \times V^- \to \mathbb{R}.
\]</span></p>
<p>But more naturally, it can be seen as:</p>
<ul>
<li>taking a vector and giving back a vector,</li>
<li>or taking a covector and giving back a covector.</li>
</ul>
<p>Example:</p>
<ul>
<li>A linear operator <span class="math inline">\(A: V \to V\)</span> has components <span class="math inline">\(A^i{}_j\)</span>. Given <span class="math inline">\(v^j\)</span>, it produces another vector <span class="math inline">\(w^i = A^i{}_j v^j\)</span>.</li>
</ul>
</section>
<section id="evaluating-step-by-step" class="level4">
<h4 class="anchored" data-anchor-id="evaluating-step-by-step">Evaluating Step by Step</h4>
<ul>
<li>Pick a tensor.</li>
<li>Plug in the right kind of inputs (vector or covector) into the right slots.</li>
<li>Contract the matching indices (up with down).</li>
<li>The result is either a scalar, a vector, or another tensor (depending on how many free indices remain).</li>
</ul>
</section>
<section id="why-this-matters-5" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-5">Why This Matters</h4>
<ul>
<li>This clarifies the action of tensors, not just their coordinates.</li>
<li>Evaluation explains why indices are placed up or down: they indicate what kind of input the tensor expects.</li>
<li>It connects naturally with Einstein summation: every evaluation is just an index contraction.</li>
</ul>
</section>
<section id="exercises-13" class="level4">
<h4 class="anchored" data-anchor-id="exercises-13">Exercises</h4>
<ol type="1">
<li><p>Evaluation of Bilinear Form: Let <span class="math inline">\(B_{ij} = \begin{bmatrix}1 &amp; 2 \\ 2 &amp; 3\end{bmatrix}\)</span>. Compute <span class="math inline">\(B(u,v)\)</span> for <span class="math inline">\(u=(1,1), v=(2,3)\)</span>.</p></li>
<li><p>Linear Operator Action: Let <span class="math inline">\(A^i{}_j = \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>.</p>
<ul>
<li>Apply <span class="math inline">\(A\)</span> to <span class="math inline">\(v=(4,5)\)</span>.</li>
<li>Interpret the result.</li>
</ul></li>
<li><p>Covector Evaluation: If <span class="math inline">\(\omega = (2, -1)\)</span> and <span class="math inline">\(v=(3,4)\)</span>, compute <span class="math inline">\(\omega(v)\)</span>.</p></li>
<li><p>Mixed Tensor Evaluation: For <span class="math inline">\(T^i{}_j = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix}\)</span>, find the output vector when applied to <span class="math inline">\(v=(1,2)\)</span>.</p></li>
<li><p>Thought Experiment: Why does the distinction between covariant and contravariant slots matter when evaluating tensors? What would go wrong if we treated them the same?</p></li>
</ol>
</section>
</section>
<section id="from-maps-to-arrays-and-back-via-a-basis" class="level3">
<h3 class="anchored" data-anchor-id="from-maps-to-arrays-and-back-via-a-basis">4.3 From Maps to Arrays (and Back) via a Basis</h3>
<p>So far, we’ve looked at tensors as multilinear maps (abstract) and as arrays of numbers (concrete). The bridge between these viewpoints is a choice of basis.</p>
<section id="step-1-start-with-a-multilinear-map" class="level4">
<h4 class="anchored" data-anchor-id="step-1-start-with-a-multilinear-map">Step 1: Start with a Multilinear Map</h4>
<p>Suppose <span class="math inline">\(T: V \times W \to \mathbb{R}\)</span> is bilinear.</p>
<ul>
<li>On its own, <span class="math inline">\(T\)</span> is just a rule for combining vectors into a number.</li>
<li>Example: in <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math inline">\(T((x_1,x_2),(y_1,y_2)) = x_1 y_1 + 2x_2 y_2\)</span>.</li>
</ul>
</section>
<section id="step-2-choose-bases" class="level4">
<h4 class="anchored" data-anchor-id="step-2-choose-bases">Step 2: Choose Bases</h4>
<p>Let <span class="math inline">\(\{e_i\}\)</span> be a basis for <span class="math inline">\(V\)</span> and <span class="math inline">\(\{f_j\}\)</span> a basis for <span class="math inline">\(W\)</span>.</p>
<ul>
<li>We can evaluate <span class="math inline">\(T(e_i,f_j)\)</span> for each pair of basis vectors.</li>
<li>These numbers form an array of components <span class="math inline">\(T_{ij}\)</span>.</li>
</ul>
<p>So in a basis, the multilinear map has a coordinate table.</p>
</section>
<section id="step-3-using-the-array-to-reconstruct-the-map" class="level4">
<h4 class="anchored" data-anchor-id="step-3-using-the-array-to-reconstruct-the-map">Step 3: Using the Array to Reconstruct the Map</h4>
<p>For general vectors <span class="math inline">\(v = v^i e_i\)</span> and <span class="math inline">\(w = w^j f_j\)</span>:</p>
<p><span class="math display">\[
T(v,w) = T_{ij} v^i w^j.
\]</span></p>
<ul>
<li>Here, the coefficients <span class="math inline">\(v^i, w^j\)</span> are the coordinates of <span class="math inline">\(v, w\)</span>.</li>
<li>The array entries <span class="math inline">\(T_{ij}\)</span> tell us how the map acts on basis vectors.</li>
</ul>
<p>Thus:</p>
<ul>
<li>Abstract definition: multilinear rule.</li>
<li>Concrete representation: an array of numbers (depends on basis).</li>
</ul>
</section>
<section id="step-4-general-tensors" class="level4">
<h4 class="anchored" data-anchor-id="step-4-general-tensors">Step 4: General Tensors</h4>
<p>For a tensor of type <span class="math inline">\((p,q)\)</span>:</p>
<ul>
<li>Choose a basis for <span class="math inline">\(V\)</span>.</li>
<li>Evaluate the tensor on all combinations of <span class="math inline">\(q\)</span> basis vectors and <span class="math inline">\(p\)</span> dual basis covectors.</li>
<li>The results form a multidimensional array <span class="math inline">\(T^{i_1 \dots i_p}{}_{j_1 \dots j_q}\)</span>.</li>
</ul>
<p>In coordinates:</p>
<p><span class="math display">\[
T(v_1, \dots, v_q, \omega^1, \dots, \omega^p) =
T^{i_1 \dots i_p}{}_{j_1 \dots j_q}
\, v_1^{j_1} \cdots v_q^{j_q} \, \omega^1_{i_1} \cdots \omega^p_{i_p}.
\]</span></p>
</section>
<section id="why-this-matters-6" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-6">Why This Matters</h4>
<ul>
<li>It explains why tensors can be stored as arrays of numbers: those numbers are just evaluations on basis elements.</li>
<li>It shows why basis choice matters for components but not for the tensor itself.</li>
<li>It unifies the map viewpoint and the array viewpoint into one consistent framework.</li>
</ul>
</section>
<section id="exercises-14" class="level4">
<h4 class="anchored" data-anchor-id="exercises-14">Exercises</h4>
<ol type="1">
<li><p>Matrix from Bilinear Form: Let <span class="math inline">\(T((x_1,x_2),(y_1,y_2)) = 3x_1y_1 + 2x_1y_2 + x_2y_1\)</span>.</p>
<ul>
<li>Write down the matrix <span class="math inline">\([T_{ij}]\)</span>.</li>
<li>Compute <span class="math inline">\(T((1,2),(3,4))\)</span> using both the formula and the matrix.</li>
</ul></li>
<li><p>Basis Choice Effect: In <span class="math inline">\(\mathbb{R}^2\)</span>, let <span class="math inline">\(T\)</span> be the dot product.</p>
<ul>
<li>What is the matrix of <span class="math inline">\(T\)</span> in the standard basis?</li>
<li>What is the matrix of <span class="math inline">\(T\)</span> in the basis <span class="math inline">\((1,1),(1,-1)\)</span>?</li>
</ul></li>
<li><p>Coordinate Reconstruction: Let <span class="math inline">\(T_{ij} = \begin{bmatrix}1 &amp; 2 \\ 0 &amp; 3\end{bmatrix}\)</span>. Compute <span class="math inline">\(T(v,w)\)</span> for <span class="math inline">\(v=(2,1), w=(1,4)\)</span>.</p></li>
<li><p>Higher Order Example: Suppose <span class="math inline">\(T\)</span> is a 3rd-order tensor with components <span class="math inline">\(T_{ijk}\)</span>. Write the formula for <span class="math inline">\(T(u,v,w)\)</span> in terms of <span class="math inline">\(T_{ijk}\)</span>, <span class="math inline">\(u^i\)</span>, <span class="math inline">\(v^j\)</span>, and <span class="math inline">\(w^k\)</span>.</p></li>
<li><p>Thought Experiment: Why can the same abstract tensor have different component arrays depending on the basis? What stays the same across all choices?</p></li>
</ol>
</section>
</section>
<section id="universal-examples-bilinear-forms-trilinear-mixing" class="level3">
<h3 class="anchored" data-anchor-id="universal-examples-bilinear-forms-trilinear-mixing">4.4 Universal Examples: Bilinear Forms, Trilinear Mixing</h3>
<p>To make the map viewpoint concrete, let’s look at a few universal examples. These are classic multilinear maps that keep reappearing in mathematics, physics, and data science.</p>
<section id="example-1-bilinear-forms" class="level4">
<h4 class="anchored" data-anchor-id="example-1-bilinear-forms">Example 1: Bilinear Forms</h4>
<p>A bilinear form is a covariant 2-tensor:</p>
<p><span class="math display">\[
B: V \times V \to \mathbb{R}, \quad B(u,v) = B_{ij} u^i v^j.
\]</span></p>
<ul>
<li>Dot product: <span class="math inline">\(\langle u,v \rangle = \delta_{ij} u^i v^j\)</span>.</li>
<li>General quadratic form: <span class="math inline">\(Q(v) = v^\top A v\)</span>.</li>
<li>Applications: measuring angles, energy, distance.</li>
</ul>
</section>
<section id="example-2-determinant-alternating-multilinear-form" class="level4">
<h4 class="anchored" data-anchor-id="example-2-determinant-alternating-multilinear-form">Example 2: Determinant (Alternating Multilinear Form)</h4>
<p>The determinant in <span class="math inline">\(\mathbb{R}^n\)</span> is an n-linear map of the column vectors:</p>
<p><span class="math display">\[
\det(v_1,\dots,v_n).
\]</span></p>
<p>It is:</p>
<ul>
<li>Multilinear: linear in each column separately.</li>
<li>Alternating: if two inputs are the same, the determinant vanishes.</li>
<li>Geometric meaning: volume of the parallelepiped spanned by the vectors.</li>
</ul>
</section>
<section id="example-3-scalar-triple-product-trilinear" class="level4">
<h4 class="anchored" data-anchor-id="example-3-scalar-triple-product-trilinear">Example 3: Scalar Triple Product (Trilinear)</h4>
<p>In <span class="math inline">\(\mathbb{R}^3\)</span>:</p>
<p><span class="math display">\[
[u,v,w] = u \cdot (v \times w).
\]</span></p>
<ul>
<li>Trilinear: linear in each of <span class="math inline">\(u,v,w\)</span>.</li>
<li>Geometric meaning: signed volume of the parallelepiped formed by <span class="math inline">\(u,v,w\)</span>.</li>
<li>Appears in mechanics (torques, volumes, orientation tests).</li>
</ul>
</section>
<section id="example-4-tensor-contraction-in-applications" class="level4">
<h4 class="anchored" data-anchor-id="example-4-tensor-contraction-in-applications">Example 4: Tensor Contraction in Applications</h4>
<p>Suppose <span class="math inline">\(T: V \times W \times X \to \mathbb{R}\)</span> with components <span class="math inline">\(T_{ijk}\)</span>.</p>
<ul>
<li>Fixing one input reduces <span class="math inline">\(T\)</span> to a bilinear form.</li>
<li>Fixing two inputs reduces <span class="math inline">\(T\)</span> to a linear functional.</li>
</ul>
<p>This is how general multilinear maps can be “partially evaluated.”</p>
</section>
<section id="example-5-mixing-signals-trilinear-mixing" class="level4">
<h4 class="anchored" data-anchor-id="example-5-mixing-signals-trilinear-mixing">Example 5: Mixing Signals (Trilinear Mixing)</h4>
<p>In signal processing and ML, a trilinear map appears naturally:</p>
<p><span class="math display">\[
M(u,v,w) = \sum_{i,j,k} T_{ijk} u^i v^j w^k.
\]</span></p>
<ul>
<li>Example: a 3D convolution kernel.</li>
<li>Applications: image processing, tensor regression, multiway data analysis.</li>
</ul>
</section>
<section id="why-these-examples-matter" class="level4">
<h4 class="anchored" data-anchor-id="why-these-examples-matter">Why These Examples Matter</h4>
<ul>
<li>They show how multilinear maps generalize familiar ideas (dot product → determinant → triple product).</li>
<li>They illustrate how multilinearity encodes geometry (lengths, areas, volumes).</li>
<li>They connect abstract tensor notation to real applications (ML, physics, graphics).</li>
</ul>
</section>
<section id="exercises-15" class="level4">
<h4 class="anchored" data-anchor-id="exercises-15">Exercises</h4>
<ol type="1">
<li><p>Dot Product as Bilinear Form: Show that the dot product <span class="math inline">\(\langle u,v \rangle = u^i v^i\)</span> is bilinear.</p></li>
<li><p>Determinant in 2D: For <span class="math inline">\(u=(1,0), v=(1,2)\)</span>, compute <span class="math inline">\(\det(u,v)\)</span>. Interpret geometrically.</p></li>
<li><p>Triple Product: Compute <span class="math inline">\([u,v,w]\)</span> for <span class="math inline">\(u=(1,0,0), v=(0,1,0), w=(0,0,1)\)</span>.</p></li>
<li><p>Signal Mixing Example: Let <span class="math inline">\(T_{ijk} = 1\)</span> if <span class="math inline">\(i=j=k\)</span>, and <span class="math inline">\(0\)</span> otherwise, for indices <span class="math inline">\(1 \leq i,j,k \leq 2\)</span>. Compute <span class="math inline">\(M(u,v,w)\)</span> for <span class="math inline">\(u=(1,2), v=(3,4), w=(5,6)\)</span>.</p></li>
<li><p>Thought Experiment: Why do determinants and triple products count as multilinear maps, not just algebraic formulas?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-5.-tensors-as-elements-of-tensor-products" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5.-tensors-as-elements-of-tensor-products">Chapter 5. Tensors as Elements of Tensor Products</h2>
<section id="constructing-v-otimes-w-intuition-and-goals" class="level3">
<h3 class="anchored" data-anchor-id="constructing-v-otimes-w-intuition-and-goals">5.1 Constructing <span class="math inline">\(V \otimes W\)</span>: Intuition and Goals</h3>
<p>So far, we’ve looked at tensors as arrays and as multilinear maps. The third viewpoint places them as elements of tensor product spaces. This perspective is abstract, but it unifies everything: it explains <em>why- multilinear maps correspond to arrays and </em>how- they transform consistently.</p>
<section id="motivation" class="level4">
<h4 class="anchored" data-anchor-id="motivation">Motivation</h4>
<p>Suppose you want to encode a bilinear map <span class="math inline">\(B: V \times W \to \mathbb{R}\)</span>.</p>
<ul>
<li>You could record all values of <span class="math inline">\(B(e_i, f_j)\)</span> on basis elements.</li>
<li>You could write it as an array <span class="math inline">\(B_{ij}\)</span>.</li>
<li>Or you could find a new space where a single element encodes all the bilinear behavior.</li>
</ul>
<p>That new space is the tensor product space <span class="math inline">\(V \otimes W\)</span>.</p>
</section>
<section id="intuition-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="intuition-building-blocks">Intuition: Building Blocks</h4>
<ul>
<li><p>Given <span class="math inline">\(v \in V\)</span> and <span class="math inline">\(w \in W\)</span>, we form a simple tensor <span class="math inline">\(v \otimes w\)</span>.</p></li>
<li><p>Think of <span class="math inline">\(v \otimes w\)</span> as a “formal symbol” that remembers both vectors at once.</p></li>
<li><p>Then we allow linear combinations:</p>
<p><span class="math display">\[
u = \sum_k v_k \otimes w_k.
\]</span></p></li>
<li><p>These combinations form a new vector space: <span class="math inline">\(V \otimes W\)</span>.</p></li>
</ul>
</section>
<section id="universal-property-informal" class="level4">
<h4 class="anchored" data-anchor-id="universal-property-informal">Universal Property (informal)</h4>
<p>The tensor product is defined so that:</p>
<ul>
<li>Every bilinear map <span class="math inline">\(B: V \times W \to U\)</span> factors uniquely through a linear map <span class="math inline">\(\tilde{B}: V \otimes W \to U\)</span>.</li>
<li>In plain words: “Bilinear maps are the same thing as linear maps out of a tensor product.”</li>
</ul>
<p>This property makes <span class="math inline">\(V \otimes W\)</span> the *right- space to house tensors.</p>
</section>
<section id="example" class="level4">
<h4 class="anchored" data-anchor-id="example">Example</h4>
<p>If <span class="math inline">\(V = \mathbb{R}^2, W = \mathbb{R}^3\)</span>, then <span class="math inline">\(V \otimes W\)</span> has dimension <span class="math inline">\(2 \times 3 = 6\)</span>.</p>
<ul>
<li><p>Basis: <span class="math inline">\(e_i \otimes f_j\)</span> with <span class="math inline">\(i=1,2; j=1,2,3\)</span>.</p></li>
<li><p>A general element:</p>
<p><span class="math display">\[
u = \sum_{i=1}^2 \sum_{j=1}^3 c_{ij} (e_i \otimes f_j).
\]</span></p></li>
<li><p>Coordinates <span class="math inline">\(c_{ij}\)</span> are exactly the array components of a bilinear map.</p></li>
</ul>
</section>
<section id="goals-of-this-viewpoint" class="level4">
<h4 class="anchored" data-anchor-id="goals-of-this-viewpoint">Goals of This Viewpoint</h4>
<ul>
<li>Provide a basis-independent foundation for tensors.</li>
<li>Unify maps, arrays, and algebra into one framework.</li>
<li>Generalize easily: <span class="math inline">\(V_1 \otimes \cdots \otimes V_k\)</span> encodes <span class="math inline">\(k\)</span>-linear maps.</li>
</ul>
</section>
<section id="exercises-16" class="level4">
<h4 class="anchored" data-anchor-id="exercises-16">Exercises</h4>
<ol type="1">
<li><p>Dimension Count: If <span class="math inline">\(\dim V = 3\)</span> and <span class="math inline">\(\dim W = 4\)</span>, what is <span class="math inline">\(\dim(V \otimes W)\)</span>?</p></li>
<li><p>Simple Tensor Example: In <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^2\)</span>, write the simple tensor <span class="math inline">\((1,2) \otimes (3,4)\)</span> as a linear combination of basis elements <span class="math inline">\(e_i \otimes e_j\)</span>.</p></li>
<li><p>Array to Tensor: For the bilinear form <span class="math inline">\(B((x_1,x_2),(y_1,y_2)) = 2x_1y_1 + 3x_2y_2\)</span>, find the corresponding tensor in <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^2\)</span>.</p></li>
<li><p>Universal Property Check: Suppose <span class="math inline">\(B: \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}\)</span> is given by <span class="math inline">\(B(u,v) = u^\top v\)</span>.</p>
<ul>
<li>Show how <span class="math inline">\(B\)</span> factors through a linear map <span class="math inline">\(\tilde{B}: \mathbb{R}^2 \otimes \mathbb{R}^2 \to \mathbb{R}\)</span>.</li>
</ul></li>
<li><p>Thought Experiment: Why is it useful to replace “bilinear maps” with “linear maps from a bigger space”? How does this simplify reasoning and computation?</p></li>
</ol>
</section>
</section>
<section id="simple-vs.-general-tensors" class="level3">
<h3 class="anchored" data-anchor-id="simple-vs.-general-tensors">5.2 Simple vs.&nbsp;General Tensors</h3>
<p>Now that we’ve seen how to build the tensor product space, we need to distinguish between its basic “atoms” and more complicated elements.</p>
<section id="simple-decomposable-tensors" class="level4">
<h4 class="anchored" data-anchor-id="simple-decomposable-tensors">Simple (Decomposable) Tensors</h4>
<p>A simple tensor (also called pure or decomposable) is one that can be written as a single tensor product:</p>
<p><span class="math display">\[
u = v \otimes w.
\]</span></p>
<p>Examples:</p>
<ul>
<li><p>In <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^3\)</span>, <span class="math inline">\((1,2) \otimes (0,1,1)\)</span> is simple.</p></li>
<li><p>Its array of components is an outer product:</p>
<p><span class="math display">\[
\begin{bmatrix}1 \\ 2\end{bmatrix}
\otimes
\begin{bmatrix}0 &amp; 1 &amp; 1\end{bmatrix}
=
\begin{bmatrix}0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 2\end{bmatrix}.
\]</span></p></li>
</ul>
<p>So simple tensors correspond to rank-one arrays (outer products).</p>
</section>
<section id="general-tensors" class="level4">
<h4 class="anchored" data-anchor-id="general-tensors">General Tensors</h4>
<p>A general tensor in <span class="math inline">\(V \otimes W\)</span> is a linear combination of simple tensors:</p>
<p><span class="math display">\[
T = \sum_{k=1}^m v_k \otimes w_k.
\]</span></p>
<ul>
<li>Some tensors can be expressed as a single simple tensor.</li>
<li>Most cannot; they require a sum of several simple tensors.</li>
<li>The minimal number of terms in such a sum is called the tensor rank (analogous to matrix rank for order-2 tensors).</li>
</ul>
</section>
<section id="matrix-analogy" class="level4">
<h4 class="anchored" data-anchor-id="matrix-analogy">Matrix Analogy</h4>
<ul>
<li>Simple tensors ↔︎ rank-one matrices (outer products of a column and a row).</li>
<li>General tensors ↔︎ arbitrary matrices (sums of rank-one pieces).</li>
</ul>
<p>For higher-order tensors, the situation is the same:</p>
<ul>
<li>Simple tensor = one outer product of vectors.</li>
<li>General tensor = sum of several such products.</li>
</ul>
</section>
<section id="why-this-matters-7" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-7">Why This Matters</h4>
<ul>
<li>The distinction underlies tensor decomposition methods (CP, Tucker, TT).</li>
<li>Simple tensors capture the “building blocks” of tensor spaces.</li>
<li>General tensors encode richer structure, but often we approximate them by a small number of simple ones.</li>
</ul>
</section>
<section id="exercises-17" class="level4">
<h4 class="anchored" data-anchor-id="exercises-17">Exercises</h4>
<ol type="1">
<li><p>Simple or Not? In <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^2\)</span>, determine whether</p>
<p><span class="math display">\[
T = e_1 \otimes e_1 + e_2 \otimes e_2
\]</span></p>
<p>is a simple tensor.</p></li>
<li><p>Outer Product Calculation: Compute the outer product of <span class="math inline">\(u=(1,2)\)</span> and <span class="math inline">\(v=(3,4,5)\)</span>. Write the resulting <span class="math inline">\(2 \times 3\)</span> matrix.</p></li>
<li><p>Rank-One Check: Given the matrix</p>
<p><span class="math display">\[
M = \begin{bmatrix}2 &amp; 4 \\ 3 &amp; 6\end{bmatrix},
\]</span></p>
<p>show that <span class="math inline">\(M\)</span> is a simple tensor (rank one) by writing it as <span class="math inline">\(u \otimes v\)</span>.</p></li>
<li><p>General Tensor Example: Express</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
\]</span></p>
<p>as a sum of two simple tensors in <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^2\)</span>.</p></li>
<li><p>Thought Experiment: Why are most tensors not simple? What does this imply about the usefulness of tensor decompositions in applications like machine learning?</p></li>
</ol>
</section>
</section>
<section id="dimension-and-bases-of-tensor-products" class="level3">
<h3 class="anchored" data-anchor-id="dimension-and-bases-of-tensor-products">5.3 Dimension and Bases of Tensor Products</h3>
<p>Having distinguished simple tensors from general ones, let’s now describe the structure of a tensor product space: its dimension and basis.</p>
<section id="dimension-formula" class="level4">
<h4 class="anchored" data-anchor-id="dimension-formula">Dimension Formula</h4>
<p>If <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are finite-dimensional vector spaces, then:</p>
<p><span class="math display">\[
\dim(V \otimes W) = \dim(V) \cdot \dim(W).
\]</span></p>
<p>More generally, for <span class="math inline">\(V_1, V_2, \dots, V_k\)</span>:</p>
<p><span class="math display">\[
\dim(V_1 \otimes V_2 \otimes \cdots \otimes V_k) = \prod_{i=1}^k \dim(V_i).
\]</span></p>
<p>This matches our intuition that each index multiplies the number of “slots” in the array representation.</p>
</section>
<section id="basis-of-a-tensor-product" class="level4">
<h4 class="anchored" data-anchor-id="basis-of-a-tensor-product">Basis of a Tensor Product</h4>
<p>Suppose <span class="math inline">\(\{e_i\}\)</span> is a basis for <span class="math inline">\(V\)</span> and <span class="math inline">\(\{f_j\}\)</span> is a basis for <span class="math inline">\(W\)</span>.</p>
<ul>
<li><p>Then the set</p>
<p><span class="math display">\[
\{e_i \otimes f_j : 1 \leq i \leq \dim V, \, 1 \leq j \leq \dim W \}
\]</span></p>
<p>forms a basis for <span class="math inline">\(V \otimes W\)</span>.</p></li>
</ul>
<p>Example:</p>
<ul>
<li>If <span class="math inline">\(\dim V = 2, \dim W = 3\)</span>: <span class="math inline">\(\{e_1 \otimes f_1, e_1 \otimes f_2, e_1 \otimes f_3, e_2 \otimes f_1, e_2 \otimes f_2, e_2 \otimes f_3\}\)</span> is a basis (6 elements).</li>
</ul>
</section>
<section id="coordinates-in-the-basis" class="level4">
<h4 class="anchored" data-anchor-id="coordinates-in-the-basis">Coordinates in the Basis</h4>
<p>A general tensor can be expressed as:</p>
<p><span class="math display">\[
T = \sum_{i,j} c_{ij} \, (e_i \otimes f_j),
\]</span></p>
<p>where the coefficients <span class="math inline">\(c_{ij}\)</span> form the familiar matrix (array) representation.</p>
<p>For higher-order tensor products, the same logic applies:</p>
<p><span class="math display">\[
T = \sum_{i,j,k} c_{ijk} \, (e_i \otimes f_j \otimes g_k).
\]</span></p>
</section>
<section id="analogy-with-arrays" class="level4">
<h4 class="anchored" data-anchor-id="analogy-with-arrays">Analogy with Arrays</h4>
<ul>
<li>Basis elements correspond to array positions.</li>
<li>Coefficients are the entries.</li>
<li>The dimension formula tells us the total number of independent entries.</li>
</ul>
<p>Thus, the product-space viewpoint and the array viewpoint are perfectly aligned.</p>
</section>
<section id="why-this-matters-8" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-8">Why This Matters</h4>
<ul>
<li>The basis description explains why tensor product spaces have the same “shape” as arrays.</li>
<li>It provides a rigorous foundation for the component representation of tensors.</li>
<li>It prepares us to reconcile all three viewpoints (array, map, product space).</li>
</ul>
</section>
<section id="exercises-18" class="level4">
<h4 class="anchored" data-anchor-id="exercises-18">Exercises</h4>
<ol type="1">
<li><p>Dimension Count: Compute <span class="math inline">\(\dim(\mathbb{R}^2 \otimes \mathbb{R}^3)\)</span>. List a basis explicitly.</p></li>
<li><p>Basis Expansion: Express <span class="math inline">\(T = (1,2) \otimes (3,4,5)\)</span> in terms of the standard basis <span class="math inline">\(e_i \otimes f_j\)</span>.</p></li>
<li><p>Higher-Order Example: If <span class="math inline">\(\dim U = 2, \dim V = 2, \dim W = 2\)</span>, what is <span class="math inline">\(\dim(U \otimes V \otimes W)\)</span>?</p></li>
<li><p>Coefficient Identification: Let <span class="math inline">\(T = e_1 \otimes f_1 + 2 e_2 \otimes f_3\)</span>. What are the nonzero coefficients <span class="math inline">\(c_{ij}\)</span>?</p></li>
<li><p>Thought Experiment: Why does the formula <span class="math inline">\(\dim(V \otimes W) = \dim V \cdot \dim W\)</span> make sense if you think of arrays? How does this match the earlier idea of “shape”?</p></li>
</ol>
</section>
</section>
<section id="three-viewpoints-reconciled" class="level3">
<h3 class="anchored" data-anchor-id="three-viewpoints-reconciled">5.4 Three Viewpoints Reconciled</h3>
<p>We now have three different ways to understand tensors: as arrays, as multilinear maps, and as elements of tensor product spaces. Each looks different, but they are all equivalent perspectives on the same object.</p>
<section id="array-view-concrete" class="level4">
<h4 class="anchored" data-anchor-id="array-view-concrete">1. Array View (Concrete)</h4>
<ul>
<li>A tensor is a multi-dimensional array of numbers.</li>
<li>Its entries depend on a choice of basis.</li>
<li>Operations are done by indexing and summing (Einstein notation).</li>
<li>Example: a <span class="math inline">\(2 \times 3 \times 4\)</span> tensor is just a 3D block of <span class="math inline">\(24\)</span> numbers.</li>
</ul>
<p>Strengths: intuitive, easy to compute. Limitations: depends heavily on coordinates.</p>
</section>
<section id="map-view-functional" class="level4">
<h4 class="anchored" data-anchor-id="map-view-functional">2. Map View (Functional)</h4>
<ul>
<li><p>A tensor is a multilinear function that takes vectors and covectors as inputs and produces scalars or other tensors.</p></li>
<li><p>Example:</p>
<ul>
<li>Dot product: bilinear map.</li>
<li>Determinant: alternating multilinear map.</li>
<li>Linear operators: mixed tensors.</li>
</ul></li>
</ul>
<p>Strengths: coordinate-free, emphasizes action. Limitations: abstract, less computational.</p>
</section>
<section id="product-space-view-foundational" class="level4">
<h4 class="anchored" data-anchor-id="product-space-view-foundational">3. Product Space View (Foundational)</h4>
<ul>
<li>A tensor is an element of a tensor product space <span class="math inline">\(V^{\otimes p} \otimes (V^*)^{\otimes q}\)</span>.</li>
<li>Basis choice identifies this element with an array of components.</li>
<li>Universal property: multilinear maps from vectors ↔︎ linear maps out of tensor products.</li>
</ul>
<p>Strengths: rigorous, unifying, handles all cases. Limitations: abstract at first, harder to visualize.</p>
</section>
<section id="reconciling-them" class="level4">
<h4 class="anchored" data-anchor-id="reconciling-them">Reconciling Them</h4>
<ul>
<li>Array = product-space + basis.</li>
<li>Map = product-space + evaluation on inputs.</li>
<li>Product-space = abstract “home” that makes the other two consistent.</li>
</ul>
<p>So:</p>
<ul>
<li>If you want computation, use arrays.</li>
<li>If you want geometry or physics intuition, use maps.</li>
<li>If you want rigor and generality, use tensor products.</li>
</ul>
<p>They are not rivals but complementary languages describing the same object.</p>
</section>
<section id="why-this-matters-9" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-9">Why This Matters</h4>
<p>Understanding all three viewpoints and moving fluidly between them is the hallmark of real mastery. It’s like being fluent in three languages: you choose the one that fits the context, but you know they describe the same reality.</p>
</section>
</section>
<section id="exercises-19" class="level3">
<h3 class="anchored" data-anchor-id="exercises-19">Exercises</h3>
<ol type="1">
<li><p>Array ↔︎ Map: Let <span class="math inline">\(T_{ij} = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}\)</span>.</p>
<ul>
<li>Write <span class="math inline">\(T(u,v) = T_{ij} u^i v^j\)</span>.</li>
<li>Evaluate for <span class="math inline">\(u=(1,0), v=(0,1)\)</span>.</li>
</ul></li>
<li><p>Map ↔︎ Product-Space: Show that the bilinear form <span class="math inline">\(B(u,v) = u_1v_1 + 2u_2v_2\)</span> corresponds to an element of <span class="math inline">\(\mathbb{R}^2 \otimes \mathbb{R}^2\)</span>.</p></li>
<li><p>Array ↔︎ Product-Space: If <span class="math inline">\(T = 3 e_1 \otimes f_2 + 5 e_2 \otimes f_3\)</span>, what are the nonzero array components <span class="math inline">\(T_{ij}\)</span>?</p></li>
<li><p>Three Languages: Write the dot product of <span class="math inline">\(u,v \in \mathbb{R}^2\)</span> in each of the three viewpoints:</p>
<ul>
<li>Array form</li>
<li>Multilinear map</li>
<li>Tensor product element</li>
</ul></li>
<li><p>Thought Experiment: Why is it valuable to know all three viewpoints instead of just one? Give an application where each viewpoint is the most natural.</p></li>
</ol>
</section>
</section>
</section>
<section id="part-iii.-core-operations-on-tensors" class="level1">
<h1>Part III. Core Operations on Tensors</h1>
<section id="chapter-6.-building-blocks" class="level2">
<h2 class="anchored" data-anchor-id="chapter-6.-building-blocks">Chapter 6. Building Blocks</h2>
<section id="tensor-outer-product" class="level3">
<h3 class="anchored" data-anchor-id="tensor-outer-product">6.1 Tensor (Outer) Product</h3>
<p>Now that we’ve reconciled the three viewpoints, we can explore the core operations on tensors. The first and most fundamental is the tensor product itself, also known as the outer product in the array viewpoint.</p>
<section id="definition" class="level4">
<h4 class="anchored" data-anchor-id="definition">Definition</h4>
<p>Given two vectors <span class="math inline">\(u \in V\)</span> and <span class="math inline">\(v \in W\)</span>, their tensor product is</p>
<p><span class="math display">\[
u \otimes v \in V \otimes W.
\]</span></p>
<ul>
<li>It is bilinear in <span class="math inline">\(u,v\)</span>.</li>
<li>In a basis, it looks like an array of rank one (outer product).</li>
</ul>
</section>
<section id="example-with-vectors" class="level4">
<h4 class="anchored" data-anchor-id="example-with-vectors">Example with Vectors</h4>
<p>If <span class="math inline">\(u = (1,2)\)</span> and <span class="math inline">\(v = (3,4,5)\)</span>:</p>
<p><span class="math display">\[
u \otimes v =
\begin{bmatrix}1 \\ 2\end{bmatrix}
\otimes
\begin{bmatrix}3 &amp; 4 &amp; 5\end{bmatrix}
=
\begin{bmatrix}
3 &amp; 4 &amp; 5 \\
6 &amp; 8 &amp; 10
\end{bmatrix}.
\]</span></p>
<p>This is a <span class="math inline">\(2 \times 3\)</span> array - exactly the outer product.</p>
</section>
<section id="higher-order-outer-products" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-outer-products">Higher-Order Outer Products</h4>
<p>We can extend this:</p>
<p><span class="math display">\[
u \otimes v \otimes w,
\]</span></p>
<p>produces a 3rd-order tensor, with entries</p>
<p><span class="math display">\[
(u \otimes v \otimes w)_{ijk} = u_i v_j w_k.
\]</span></p>
<p>In general, the outer product of <span class="math inline">\(k\)</span> vectors is a simple tensor of order <span class="math inline">\(k\)</span>.</p>
</section>
<section id="properties" class="level4">
<h4 class="anchored" data-anchor-id="properties">Properties</h4>
<ul>
<li>Bilinearity: <span class="math inline">\((au + bu') \otimes v = a(u \otimes v) + b(u' \otimes v)\)</span>.</li>
<li>Noncommutativity: <span class="math inline">\(u \otimes v \neq v \otimes u\)</span> (unless dimensions align and we impose symmetry).</li>
<li>Rank-One Structure: Outer products produce the simplest tensors, forming the building blocks of general tensors.</li>
</ul>
</section>
<section id="why-this-matters-10" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-10">Why This Matters</h4>
<ul>
<li>The tensor product (outer product) is the construction rule for higher-order tensors.</li>
<li>It connects vector multiplication with multidimensional arrays.</li>
<li>Many decompositions (CP, Tucker, tensor train) rely on sums of outer products.</li>
</ul>
</section>
<section id="exercises-20" class="level4">
<h4 class="anchored" data-anchor-id="exercises-20">Exercises</h4>
<ol type="1">
<li><p>Basic Outer Product: Compute <span class="math inline">\((2,1) \otimes (1,3)\)</span>. Write it as a <span class="math inline">\(2 \times 2\)</span> matrix.</p></li>
<li><p>Higher Order: Compute <span class="math inline">\((1,2) \otimes (0,1) \otimes (3,4)\)</span>. What is the shape of the resulting tensor, and how many entries does it have?</p></li>
<li><p>Linearity Check: Verify that <span class="math inline">\((u+u') \otimes v = u \otimes v + u' \otimes v\)</span> for <span class="math inline">\(u=(1,0), u'=(0,1), v=(2,3)\)</span>.</p></li>
<li><p>Rank-One Matrix: Show that every outer product <span class="math inline">\(u \otimes v\)</span> is a rank-one matrix. Give an example.</p></li>
<li><p>Thought Experiment: Why does the outer product produce the “simplest” tensors? How does this idea generalize from matrices to higher-order arrays?</p></li>
</ol>
</section>
</section>
<section id="contraction-summing-paired-indices" class="level3">
<h3 class="anchored" data-anchor-id="contraction-summing-paired-indices">6.2 Contraction: Summing Paired Indices</h3>
<p>If the tensor product (outer product) builds bigger tensors, then contraction is the operation that reduces them by “pairing up” an upper and a lower index and summing over it. This is the natural generalization of the dot product and the trace.</p>
<section id="definition-1" class="level4">
<h4 class="anchored" data-anchor-id="definition-1">Definition</h4>
<p>Given a tensor <span class="math inline">\(T^{i}{}_{j}\)</span>, contracting on the index <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> means:</p>
<p><span class="math display">\[
\mathrm{contr}(T) = T^{i}{}_{i}.
\]</span></p>
<ul>
<li>You match one upper and one lower index.</li>
<li>You sum over that index.</li>
<li>The result is a new tensor of lower order.</li>
</ul>
</section>
<section id="familiar-examples" class="level4">
<h4 class="anchored" data-anchor-id="familiar-examples">Familiar Examples</h4>
<ol type="1">
<li><p>Dot Product: For vectors <span class="math inline">\(u^i\)</span> and <span class="math inline">\(v_i\)</span>:</p>
<p><span class="math display">\[
u^i v_i
\]</span></p>
<p>is a contraction, producing a scalar.</p></li>
<li><p>Matrix-Vector Multiplication:</p>
<p><span class="math display">\[
y^i = A^i{}_j x^j
\]</span></p>
<p>contracts on <span class="math inline">\(j\)</span>, leaving a vector <span class="math inline">\(y^i\)</span>.</p></li>
<li><p>Trace of a Matrix:</p>
<p><span class="math display">\[
\mathrm{tr}(A) = A^i{}_i
\]</span></p>
<p>contracts on the row and column indices, producing a scalar.</p></li>
</ol>
</section>
<section id="general-contraction" class="level4">
<h4 class="anchored" data-anchor-id="general-contraction">General Contraction</h4>
<p>If <span class="math inline">\(T^{i_1 i_2 \dots i_p}{}_{j_1 j_2 \dots j_q}\)</span> is a tensor, contracting on <span class="math inline">\(i_k\)</span> and <span class="math inline">\(j_\ell\)</span> yields a tensor of type <span class="math inline">\((p-1, q-1)\)</span>.</p>
<p>This is how tensor calculus generalizes dot products, matrix multiplication, and traces into one operation.</p>
</section>
<section id="why-this-matters-11" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-11">Why This Matters</h4>
<ul>
<li>Contraction is the dual operation to the tensor product.</li>
<li>Outer product increases order, contraction decreases order.</li>
<li>Together, they generate most tensor operations in mathematics, physics, and machine learning.</li>
</ul>
</section>
<section id="exercises-21" class="level4">
<h4 class="anchored" data-anchor-id="exercises-21">Exercises</h4>
<ol type="1">
<li><p>Dot Product as Contraction: Show explicitly that <span class="math inline">\(u \cdot v = u^i v_i\)</span> is a contraction.</p></li>
<li><p>Matrix-Vector Multiplication: Let <span class="math inline">\(A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}\)</span>, <span class="math inline">\(x=(5,6)\)</span>. Compute <span class="math inline">\(y^i = A^i{}_j x^j\)</span>.</p></li>
<li><p>Trace Calculation: For <span class="math inline">\(A = \begin{bmatrix}2 &amp; 1 \\ 0 &amp; -3\end{bmatrix}\)</span>, compute <span class="math inline">\(\mathrm{tr}(A)\)</span> via contraction.</p></li>
<li><p>Higher-Order Contraction: Suppose <span class="math inline">\(T^{ij}{}_{k\ell}\)</span> is a 4th-order tensor. What is the order and type of the tensor obtained by contracting on <span class="math inline">\(j\)</span> and <span class="math inline">\(\ell\)</span>?</p></li>
<li><p>Thought Experiment: Why can contraction only happen between one upper and one lower index? What would break if you tried to contract two uppers or two lowers?</p></li>
</ol>
</section>
</section>
<section id="permutations-of-modes-transpose-unfold-matricize" class="level3">
<h3 class="anchored" data-anchor-id="permutations-of-modes-transpose-unfold-matricize">6.3 Permutations of Modes: Transpose, Unfold, Matricize</h3>
<p>Tensors often need to be rearranged so that their structure fits the operation we want to perform. This section introduces three related operations: permutation of modes, transpose, and matricization (unfolding).</p>
<section id="permuting-modes" class="level4">
<h4 class="anchored" data-anchor-id="permuting-modes">Permuting Modes</h4>
<p>A tensor of order <span class="math inline">\(k\)</span> has <span class="math inline">\(k\)</span> indices:</p>
<p><span class="math display">\[
T_{i_1 i_2 \dots i_k}.
\]</span></p>
<p>A permutation of modes reorders these indices.</p>
<ul>
<li>Example: if <span class="math inline">\(T\)</span> is order 3, with entries <span class="math inline">\(T_{ijk}\)</span>, then a permutation might yield <span class="math inline">\(T_{kij}\)</span>.</li>
<li>This doesn’t change the data, just how we view the axes.</li>
</ul>
</section>
<section id="transpose-matrix-case" class="level4">
<h4 class="anchored" data-anchor-id="transpose-matrix-case">Transpose (Matrix Case)</h4>
<p>For a 2nd-order tensor (matrix), permutation of the two indices corresponds to the familiar transpose:</p>
<p><span class="math display">\[
A_{ij} \mapsto A_{ji}.
\]</span></p>
<p>This is just a special case of permuting modes.</p>
</section>
<section id="matricization-unfolding" class="level4">
<h4 class="anchored" data-anchor-id="matricization-unfolding">Matricization (Unfolding)</h4>
<p>For higher-order tensors, it is often useful to flatten them into matrices.</p>
<ul>
<li>Choose one index (or a group of indices) to form the rows.</li>
<li>The remaining indices form the columns.</li>
</ul>
<p>Example: For a 3rd-order tensor <span class="math inline">\(T_{ijk}\)</span>:</p>
<ul>
<li>Mode-1 unfolding: rows indexed by <span class="math inline">\(i\)</span>, columns by <span class="math inline">\((j,k)\)</span>.</li>
<li>Mode-2 unfolding: rows indexed by <span class="math inline">\(j\)</span>, columns by <span class="math inline">\((i,k)\)</span>.</li>
<li>Mode-3 unfolding: rows indexed by <span class="math inline">\(k\)</span>, columns by <span class="math inline">\((i,j)\)</span>.</li>
</ul>
<p>This is widely used in numerical linear algebra and machine learning.</p>
</section>
<section id="why-permutations-matter" class="level4">
<h4 class="anchored" data-anchor-id="why-permutations-matter">Why Permutations Matter</h4>
<ul>
<li>They allow us to reinterpret tensors for algorithms (e.g., apply matrix methods to unfolded tensors).</li>
<li>They help align indices correctly before contraction.</li>
<li>They reveal symmetry or structure hidden in the raw array.</li>
</ul>
</section>
<section id="exercises-22" class="level4">
<h4 class="anchored" data-anchor-id="exercises-22">Exercises</h4>
<ol type="1">
<li><p>Permutation Practice: If <span class="math inline">\(T_{ijk}\)</span> has shape <span class="math inline">\((2,3,4)\)</span>, what is the shape of the permuted tensor <span class="math inline">\(T_{kij}\)</span>?</p></li>
<li><p>Matrix Transpose: Write the transpose of</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}.
\]</span></p></li>
<li><p>Mode-1 Unfolding: For a tensor <span class="math inline">\(T_{ijk}\)</span> of shape <span class="math inline">\((2,2,2)\)</span>, what is the shape of its mode-1 unfolding?</p></li>
<li><p>Reversibility: Show that permuting modes twice with inverse permutations returns the original tensor.</p></li>
<li><p>Thought Experiment: Why might unfolding a tensor into a matrix help in data analysis (e.g., PCA, SVD)? What do we lose by unfolding?</p></li>
</ol>
</section>
</section>
<section id="kronecker-product-vs.-tensor-product" class="level3">
<h3 class="anchored" data-anchor-id="kronecker-product-vs.-tensor-product">6.4 Kronecker Product vs.&nbsp;Tensor Product</h3>
<p>The tensor product and the Kronecker product are closely related, but they live in slightly different worlds. Beginners often confuse them because they both “blow up” dimensions in similar ways. Let’s carefully separate them.</p>
<section id="tensor-product-abstract" class="level4">
<h4 class="anchored" data-anchor-id="tensor-product-abstract">Tensor Product (Abstract)</h4>
<ul>
<li>Given two vector spaces <span class="math inline">\(V, W\)</span>, their tensor product <span class="math inline">\(V \otimes W\)</span> is a new vector space.</li>
<li>If <span class="math inline">\(\dim V = m\)</span> and <span class="math inline">\(\dim W = n\)</span>, then <span class="math inline">\(\dim(V \otimes W) = mn\)</span>.</li>
<li>Basis: <span class="math inline">\(e_i \otimes f_j\)</span>.</li>
<li>Purpose: encodes bilinear maps as linear maps.</li>
</ul>
<p>Example: For <span class="math inline">\(u=(u_1,u_2)\)</span>, <span class="math inline">\(v=(v_1,v_2,v_3)\)</span>:</p>
<p><span class="math display">\[
u \otimes v =
\begin{bmatrix}
u_1v_1 &amp; u_1v_2 &amp; u_1v_3 \\
u_2v_1 &amp; u_2v_2 &amp; u_2v_3
\end{bmatrix}.
\]</span></p>
</section>
<section id="kronecker-product-matrix-operation" class="level4">
<h4 class="anchored" data-anchor-id="kronecker-product-matrix-operation">Kronecker Product (Matrix Operation)</h4>
<ul>
<li>Given two matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{p \times q}\)</span>, the Kronecker product is:</li>
</ul>
<p><span class="math display">\[
A \otimes B =
\begin{bmatrix}
a_{11}B &amp; a_{12}B &amp; \cdots &amp; a_{1n}B \\
a_{21}B &amp; a_{22}B &amp; \cdots &amp; a_{2n}B \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\
a_{m1}B &amp; a_{m2}B &amp; \cdots &amp; a_{mn}B
\end{bmatrix}.
\]</span></p>
<ul>
<li>This produces a block matrix of size <span class="math inline">\((mp) \times (nq)\)</span>.</li>
<li>Used heavily in linear algebra identities, signal processing, and ML.</li>
</ul>
</section>
<section id="relation-between-the-two" class="level4">
<h4 class="anchored" data-anchor-id="relation-between-the-two">Relation Between the Two</h4>
<ul>
<li><p>The Kronecker product is the matrix representation of the tensor product once bases are chosen.</p></li>
<li><p>In other words:</p>
<ul>
<li>Tensor product = basis-independent, abstract.</li>
<li>Kronecker product = concrete array form.</li>
</ul></li>
</ul>
</section>
<section id="examples" class="level4">
<h4 class="anchored" data-anchor-id="examples">Examples</h4>
<ol type="1">
<li><p>Vector Tensor Product: <span class="math inline">\((1,2) \otimes (3,4) = \begin{bmatrix}3 &amp; 4 \\ 6 &amp; 8\end{bmatrix}.\)</span></p></li>
<li><p>Matrix Kronecker Product:</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix} \otimes
\begin{bmatrix}0 &amp; 5 \\ 6 &amp; 7\end{bmatrix}
=
\begin{bmatrix}
1\cdot B &amp; 2\cdot B \\
3\cdot B &amp; 4\cdot B
\end{bmatrix}.
\]</span></p></li>
</ol>
</section>
<section id="why-this-matters-12" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-12">Why This Matters</h4>
<ul>
<li>Tensor product explains the theory of multilinearity.</li>
<li>Kronecker product is the computational tool, letting us work with actual arrays.</li>
<li>Keeping them distinct prevents confusion between <em>concept- and </em>representation*.</li>
</ul>
</section>
<section id="exercises-23" class="level4">
<h4 class="anchored" data-anchor-id="exercises-23">Exercises</h4>
<ol type="1">
<li><p>Basic Kronecker: Compute</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1\end{bmatrix} \otimes
\begin{bmatrix}a &amp; b \\ c &amp; d\end{bmatrix}.
\]</span></p></li>
<li><p>Dimension Check: If <span class="math inline">\(A\)</span> is <span class="math inline">\(2 \times 3\)</span> and <span class="math inline">\(B\)</span> is <span class="math inline">\(3 \times 4\)</span>, what is the size of <span class="math inline">\(A \otimes B\)</span>?</p></li>
<li><p>Tensor vs.&nbsp;Kronecker: Show explicitly that for vectors <span class="math inline">\(u,v\)</span>, the tensor product <span class="math inline">\(u \otimes v\)</span> corresponds to the Kronecker product of <span class="math inline">\(u\)</span> and <span class="math inline">\(v^\top\)</span>.</p></li>
<li><p>Outer Product Relation: Express the outer product <span class="math inline">\(u \otimes v\)</span> as a Kronecker product when <span class="math inline">\(u,v\)</span> are vectors.</p></li>
<li><p>Thought Experiment: Why is it useful to separate the abstract tensor product from the concrete Kronecker product? What problems might arise if we conflated them?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-7.-symmetry-and-antisymmetry" class="level2">
<h2 class="anchored" data-anchor-id="chapter-7.-symmetry-and-antisymmetry">Chapter 7. Symmetry and (Anti)Symmetry</h2>
<section id="symmetrization-operators" class="level3">
<h3 class="anchored" data-anchor-id="symmetrization-operators">7.1 Symmetrization Operators</h3>
<p>One of the most important features of tensors is how they behave under permutations of indices. Some tensors stay the same when you swap indices (symmetric), others change sign (antisymmetric). To formalize this, we introduce symmetrization operators.</p>
<section id="symmetric-tensors" class="level4">
<h4 class="anchored" data-anchor-id="symmetric-tensors">Symmetric Tensors</h4>
<p>A tensor <span class="math inline">\(T_{ij}\)</span> is symmetric if swapping indices does not change it:</p>
<p><span class="math display">\[
T_{ij} = T_{ji}.
\]</span></p>
<p>More generally, a <span class="math inline">\(k\)</span>-tensor <span class="math inline">\(T_{i_1 i_2 \dots i_k}\)</span> is symmetric if:</p>
<p><span class="math display">\[
T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}} = T_{i_1 i_2 \dots i_k}
\]</span></p>
<p>for every permutation <span class="math inline">\(\pi\)</span>.</p>
<p>Example:</p>
<ul>
<li>The Hessian matrix of a scalar function is symmetric.</li>
<li>A covariance matrix is symmetric.</li>
</ul>
</section>
<section id="symmetrization-operator" class="level4">
<h4 class="anchored" data-anchor-id="symmetrization-operator">Symmetrization Operator</h4>
<p>Given any tensor <span class="math inline">\(T\)</span>, its symmetrized version is obtained by averaging over all permutations of indices:</p>
<p><span class="math display">\[
\mathrm{Sym}(T)_{i_1 i_2 \dots i_k} = \frac{1}{k!} \sum_{\pi \in S_k} T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}.
\]</span></p>
<ul>
<li><span class="math inline">\(S_k\)</span> is the symmetric group on <span class="math inline">\(k\)</span> indices.</li>
<li>This ensures the result is symmetric, even if the original tensor was not.</li>
</ul>
</section>
<section id="example-symmetrizing-a-2nd-order-tensor" class="level4">
<h4 class="anchored" data-anchor-id="example-symmetrizing-a-2nd-order-tensor">Example: Symmetrizing a 2nd-Order Tensor</h4>
<p>Given a matrix <span class="math inline">\(A\)</span>, its symmetrization is:</p>
<p><span class="math display">\[
\mathrm{Sym}(A) = \frac{1}{2}(A + A^\top).
\]</span></p>
<p>This extracts the symmetric part of a matrix.</p>
</section>
<section id="properties-1" class="level4">
<h4 class="anchored" data-anchor-id="properties-1">Properties</h4>
<ul>
<li>Symmetrization is a projection operator: applying it twice gives the same result.</li>
<li>The space of symmetric tensors is a subspace of the full tensor space.</li>
<li>Symmetric tensors are often much smaller in dimension than general tensors.</li>
</ul>
</section>
<section id="why-this-matters-13" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-13">Why This Matters</h4>
<ul>
<li>Symmetrization leads to symmetric tensor spaces, central in polynomial algebra and statistics.</li>
<li>It prepares us for the study of alternating tensors (exterior algebra) in the next part.</li>
<li>Many applications (covariance, stress, Hessians) naturally produce symmetric tensors.</li>
</ul>
</section>
<section id="exercises-24" class="level4">
<h4 class="anchored" data-anchor-id="exercises-24">Exercises</h4>
<ol type="1">
<li><p>Matrix Symmetrization: Compute the symmetrized version of</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}.
\]</span></p></li>
<li><p>Check Symmetry: Is the tensor <span class="math inline">\(T_{ij}\)</span> with entries <span class="math inline">\(T_{12}=1, T_{21}=2, T_{11}=0, T_{22}=3\)</span> symmetric?</p></li>
<li><p>Symmetrizing a 3rd-Order Tensor: Write the formula for the symmetrized version of a 3rd-order tensor <span class="math inline">\(T_{ijk}\)</span>.</p></li>
<li><p>Projection Property: Show that if <span class="math inline">\(T\)</span> is already symmetric, then <span class="math inline">\(\mathrm{Sym}(T) = T\)</span>.</p></li>
<li><p>Thought Experiment: Why might symmetrization be important in statistics (e.g., covariance estimation) or in physics (e.g., stress tensors)?</p></li>
</ol>
</section>
</section>
<section id="alternation-antisymmetrization" class="level3">
<h3 class="anchored" data-anchor-id="alternation-antisymmetrization">7.2 Alternation (Antisymmetrization)</h3>
<p>If symmetrization enforces invariance under index swaps, alternation (or antisymmetrization) enforces the opposite: swapping two indices flips the sign. This construction is central to exterior algebra and geometric concepts like orientation and volume.</p>
<section id="antisymmetric-tensors" class="level4">
<h4 class="anchored" data-anchor-id="antisymmetric-tensors">Antisymmetric Tensors</h4>
<p>A tensor <span class="math inline">\(T_{ij}\)</span> is antisymmetric if</p>
<p><span class="math display">\[
T_{ij} = -T_{ji}.
\]</span></p>
<ul>
<li><p>In particular, <span class="math inline">\(T_{ii} = 0\)</span>.</p></li>
<li><p>For higher orders:</p>
<p><span class="math display">\[
T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}} = \mathrm{sgn}(\pi) \, T_{i_1 i_2 \dots i_k},
\]</span></p>
<p>where <span class="math inline">\(\mathrm{sgn}(\pi)\)</span> is the sign of the permutation <span class="math inline">\(\pi\)</span>.</p></li>
</ul>
<p>Examples:</p>
<ul>
<li>The determinant is based on a fully antisymmetric tensor.</li>
<li>The cross product in <span class="math inline">\(\mathbb{R}^3\)</span> can be expressed using an antisymmetric tensor.</li>
</ul>
</section>
<section id="alternation-operator" class="level4">
<h4 class="anchored" data-anchor-id="alternation-operator">Alternation Operator</h4>
<p>Given any tensor <span class="math inline">\(T\)</span>, its alternated version is obtained by summing with signs:</p>
<p><span class="math display">\[
\mathrm{Alt}(T)_{i_1 i_2 \dots i_k} = \frac{1}{k!} \sum_{\pi \in S_k} \mathrm{sgn}(\pi) \, T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}.
\]</span></p>
<ul>
<li>This forces antisymmetry.</li>
<li>If <span class="math inline">\(T\)</span> was already antisymmetric, <span class="math inline">\(\mathrm{Alt}(T) = T\)</span>.</li>
</ul>
</section>
<section id="example-alternating-a-2nd-order-tensor" class="level4">
<h4 class="anchored" data-anchor-id="example-alternating-a-2nd-order-tensor">Example: Alternating a 2nd-Order Tensor</h4>
<p>For a matrix <span class="math inline">\(A\)</span>, alternation gives:</p>
<p><span class="math display">\[
\mathrm{Alt}(A) = \frac{1}{2}(A - A^\top).
\]</span></p>
<p>This extracts the skew-symmetric part of a matrix.</p>
</section>
<section id="properties-2" class="level4">
<h4 class="anchored" data-anchor-id="properties-2">Properties</h4>
<ul>
<li>Alternation is a projection operator: <span class="math inline">\(\mathrm{Alt}(\mathrm{Alt}(T)) = \mathrm{Alt}(T)\)</span>.</li>
<li>The space of antisymmetric <span class="math inline">\(k\)</span>-tensors is called <span class="math inline">\(\Lambda^k(V)\)</span>, the exterior power.</li>
<li>Its dimension is <span class="math inline">\(\binom{n}{k}\)</span>, much smaller than the full tensor space.</li>
</ul>
</section>
<section id="why-this-matters-14" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-14">Why This Matters</h4>
<ul>
<li>Alternating tensors represent oriented areas, volumes, and higher-dimensional analogues.</li>
<li>They are the foundation of differential forms and exterior algebra.</li>
<li>Physics applications: angular momentum, electromagnetism, determinants.</li>
</ul>
</section>
<section id="exercises-25" class="level4">
<h4 class="anchored" data-anchor-id="exercises-25">Exercises</h4>
<ol type="1">
<li><p>Skew-Symmetric Matrix: Compute the alternated version of</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}.
\]</span></p></li>
<li><p>Check Antisymmetry: Is the tensor <span class="math inline">\(T_{ij}\)</span> with <span class="math inline">\(T_{12}=1, T_{21}=-1, T_{11}=0, T_{22}=0\)</span> antisymmetric?</p></li>
<li><p>Alternating a 3rd-Order Tensor: Write the formula for <span class="math inline">\(\mathrm{Alt}(T)_{ijk}\)</span> explicitly in terms of the six permutations of <span class="math inline">\((i,j,k)\)</span>.</p></li>
<li><p>Zero Property: Show that if two indices are equal in an antisymmetric tensor, the component must vanish.</p></li>
<li><p>Thought Experiment: Why is antisymmetry essential for defining oriented volume (via determinants) and exterior algebra?</p></li>
</ol>
</section>
</section>
<section id="decompositions-by-symmetry-type" class="level3">
<h3 class="anchored" data-anchor-id="decompositions-by-symmetry-type">7.3 Decompositions by Symmetry Type</h3>
<p>So far we’ve seen how to symmetrize and antisymmetrize tensors. The next step is to notice that *any- tensor can be decomposed into symmetric and antisymmetric parts. This is very similar to how every matrix can be written as the sum of a symmetric and a skew-symmetric matrix.</p>
<section id="the-2nd-order-case-matrices" class="level4">
<h4 class="anchored" data-anchor-id="the-2nd-order-case-matrices">The 2nd-Order Case (Matrices)</h4>
<p>Given a matrix <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
A = \tfrac{1}{2}(A + A^\top) + \tfrac{1}{2}(A - A^\top).
\]</span></p>
<ul>
<li>The first term is symmetric.</li>
<li>The second term is antisymmetric.</li>
<li>Together, they reconstruct the original matrix.</li>
</ul>
<p>This is the simplest case of decomposition by symmetry type.</p>
</section>
<section id="higher-order-generalization" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-generalization">Higher-Order Generalization</h4>
<p>For a tensor <span class="math inline">\(T_{i_1 \dots i_k}\)</span>:</p>
<ul>
<li>Apply the symmetrization operator to get the symmetric part.</li>
<li>Apply the alternation operator to get the antisymmetric part.</li>
</ul>
<p>But for <span class="math inline">\(k &gt; 2\)</span>, there are many possible symmetry types (not just symmetric vs.&nbsp;antisymmetric).</p>
</section>
<section id="young-diagrams-and-symmetry-types-preview" class="level4">
<h4 class="anchored" data-anchor-id="young-diagrams-and-symmetry-types-preview">Young Diagrams and Symmetry Types (Preview)</h4>
<p>In general, the ways indices can be symmetrized/antisymmetrized correspond to representations of the symmetric group.</p>
<ul>
<li>Fully symmetric: indices invariant under any swap.</li>
<li>Fully antisymmetric: indices change sign under swaps.</li>
<li>Mixed types: some indices symmetric among themselves, others antisymmetric.</li>
</ul>
<p>These patterns can be visualized with Young diagrams (a deeper subject in representation theory).</p>
</section>
<section id="why-decomposition-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-decomposition-matters">Why Decomposition Matters</h4>
<ul>
<li><p>Symmetry often reduces the effective dimension of tensor spaces.</p></li>
<li><p>Many natural tensors fall into specific symmetry classes (e.g., metric tensors are symmetric, differential forms are antisymmetric).</p></li>
<li><p>In applications, splitting into symmetric/antisymmetric parts clarifies the geometric meaning:</p>
<ul>
<li>Symmetric part → stretching, scaling (like stress/strain).</li>
<li>Antisymmetric part → rotation, orientation (like angular momentum).</li>
</ul></li>
</ul>
</section>
<section id="exercises-26" class="level4">
<h4 class="anchored" data-anchor-id="exercises-26">Exercises</h4>
<ol type="1">
<li><p>Matrix Decomposition: Decompose</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 3 \\ 2 &amp; 4\end{bmatrix}
\]</span></p>
<p>into symmetric and antisymmetric parts.</p></li>
<li><p>Sym + Anti Check: Verify that every <span class="math inline">\(2 \times 2\)</span> matrix can be written uniquely as <span class="math inline">\(S + K\)</span>, with <span class="math inline">\(S\)</span> symmetric and <span class="math inline">\(K\)</span> antisymmetric.</p></li>
<li><p>Symmetric Projection: For <span class="math inline">\(T_{ijk}\)</span>, write the formula for its symmetric part <span class="math inline">\(\mathrm{Sym}(T)_{ijk}\)</span>.</p></li>
<li><p>Antisymmetric Projection: For <span class="math inline">\(T_{ijk}\)</span>, write the formula for its antisymmetric part <span class="math inline">\(\mathrm{Alt}(T)_{ijk}\)</span>.</p></li>
<li><p>Thought Experiment: Why does decomposing tensors by symmetry type help in physics and engineering (e.g., stress tensors, electromagnetic fields)? ### 7.4 Applications: Determinants and Oriented Volumes</p></li>
</ol>
<p>Now we put the ideas of symmetry and antisymmetry into practice. Some of the most important geometric and physical quantities are built from alternating tensors - especially the determinant and oriented volumes.</p>
</section>
<section id="determinant-as-an-antisymmetric-multilinear-map" class="level4">
<h4 class="anchored" data-anchor-id="determinant-as-an-antisymmetric-multilinear-map">Determinant as an Antisymmetric Multilinear Map</h4>
<p>The determinant of an <span class="math inline">\(n \times n\)</span> matrix can be seen as an alternating <span class="math inline">\(n\)</span>-linear form:</p>
<p><span class="math display">\[
\det(v_1, v_2, \dots, v_n),
\]</span></p>
<p>where <span class="math inline">\(v_i\)</span> are the column vectors of the matrix.</p>
<ul>
<li>Multilinear: linear in each column.</li>
<li>Alternating: if two columns are equal, the determinant vanishes.</li>
<li>Geometric meaning: signed volume of the parallelepiped spanned by the columns.</li>
</ul>
</section>
<section id="oriented-areas-in-mathbbr2" class="level4">
<h4 class="anchored" data-anchor-id="oriented-areas-in-mathbbr2">Oriented Areas in <span class="math inline">\(\mathbb{R}^2\)</span></h4>
<p>Given two vectors <span class="math inline">\(u, v \in \mathbb{R}^2\)</span>, the area of the parallelogram they span is:</p>
<p><span class="math display">\[
\mathrm{Area}(u,v) = | \det(u,v) |.
\]</span></p>
<p>The sign of <span class="math inline">\(\det(u,v)\)</span> indicates orientation (clockwise vs.&nbsp;counterclockwise).</p>
</section>
<section id="oriented-volumes-in-mathbbr3" class="level4">
<h4 class="anchored" data-anchor-id="oriented-volumes-in-mathbbr3">Oriented Volumes in <span class="math inline">\(\mathbb{R}^3\)</span></h4>
<p>Given three vectors <span class="math inline">\(u,v,w \in \mathbb{R}^3\)</span>, the scalar triple product</p>
<p><span class="math display">\[
[u,v,w] = u \cdot (v \times w)
\]</span></p>
<p>is the determinant of the <span class="math inline">\(3 \times 3\)</span> matrix with columns <span class="math inline">\(u,v,w\)</span>.</p>
<ul>
<li>Magnitude: volume of the parallelepiped.</li>
<li>Sign: orientation (right-handed vs.&nbsp;left-handed system).</li>
</ul>
</section>
<section id="exterior-powers-and-general-volumes" class="level4">
<h4 class="anchored" data-anchor-id="exterior-powers-and-general-volumes">Exterior Powers and General Volumes</h4>
<p>The antisymmetric space <span class="math inline">\(\Lambda^k(V)\)</span> encodes oriented <span class="math inline">\(k\)</span>-dimensional volumes:</p>
<ul>
<li>For <span class="math inline">\(k=2\)</span>: oriented areas (parallelograms).</li>
<li>For <span class="math inline">\(k=3\)</span>: oriented volumes (parallelepipeds).</li>
<li>For general <span class="math inline">\(k\)</span>: higher-dimensional “hyper-volumes.”</li>
</ul>
<p>Thus, alternating tensors generalize determinants and orientation to any dimension.</p>
</section>
<section id="why-this-matters-15" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-15">Why This Matters</h4>
<ul>
<li>Determinants, areas, and volumes are not just numbers - they are built from antisymmetric tensors.</li>
<li>This viewpoint makes clear why determinants vanish when vectors are linearly dependent: antisymmetry forces collapse.</li>
<li>Orientation is encoded algebraically, which is crucial in physics (torques, flux) and geometry (orientation of manifolds).</li>
</ul>
</section>
<section id="exercises-27" class="level4">
<h4 class="anchored" data-anchor-id="exercises-27">Exercises</h4>
<ol type="1">
<li><p>2D Determinant: Compute <span class="math inline">\(\det\big((1,2),(3,4)\big)\)</span>. Interpret the result geometrically.</p></li>
<li><p>Area from Determinant: Find the area of the parallelogram spanned by <span class="math inline">\(u=(2,0)\)</span> and <span class="math inline">\(v=(1,3)\)</span> using determinants.</p></li>
<li><p>Triple Product: Compute <span class="math inline">\([u,v,w]\)</span> for <span class="math inline">\(u=(1,0,0)\)</span>, <span class="math inline">\(v=(0,2,0)\)</span>, <span class="math inline">\(w=(0,0,3)\)</span>. What volume does this represent?</p></li>
<li><p>Linear Dependence: Show that if <span class="math inline">\(v_1, v_2, v_3\)</span> are linearly dependent in <span class="math inline">\(\mathbb{R}^3\)</span>, then <span class="math inline">\([v_1,v_2,v_3] = 0\)</span>.</p></li>
<li><p>Thought Experiment: Why does antisymmetry naturally encode the geometric idea of orientation? How does this link back to the sign of a determinant?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-iv.-exterior-algebra" class="level1">
<h1>Part IV. Exterior Algebra</h1>
<section id="chapter-8.-wedge-products-and-k-vectors" class="level2">
<h2 class="anchored" data-anchor-id="chapter-8.-wedge-products-and-k-vectors">Chapter 8. Wedge Products and k-Vectors</h2>
<section id="the-wedge-product-and-geometric-meaning" class="level3">
<h3 class="anchored" data-anchor-id="the-wedge-product-and-geometric-meaning">8.1 The Wedge Product and Geometric Meaning</h3>
<p>We now begin Part IV: Exterior Algebra, which formalizes antisymmetric tensors into a powerful algebraic system. The key operation here is the wedge product (<span class="math inline">\(\wedge\)</span>).</p>
<section id="definition-of-the-wedge-product" class="level4">
<h4 class="anchored" data-anchor-id="definition-of-the-wedge-product">Definition of the Wedge Product</h4>
<p>Given two vectors <span class="math inline">\(u, v \in V\)</span>, their wedge product is:</p>
<p><span class="math display">\[
u \wedge v = u \otimes v - v \otimes u.
\]</span></p>
<ul>
<li>It is bilinear: linear in each input.</li>
<li>It is antisymmetric: <span class="math inline">\(u \wedge v = -v \wedge u\)</span>.</li>
<li>It vanishes if <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are linearly dependent.</li>
</ul>
</section>
<section id="geometric-meaning-1" class="level4">
<h4 class="anchored" data-anchor-id="geometric-meaning-1">Geometric Meaning</h4>
<ul>
<li><p><span class="math inline">\(u \wedge v\)</span> represents the oriented parallelogram spanned by <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>.</p></li>
<li><p>The magnitude corresponds to the area:</p>
<p><span class="math display">\[
\|u \wedge v\| = \|u\| \, \|v\| \, \sin\theta,
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>.</p></li>
<li><p>The sign encodes orientation.</p></li>
</ul>
</section>
<section id="higher-wedge-products" class="level4">
<h4 class="anchored" data-anchor-id="higher-wedge-products">Higher Wedge Products</h4>
<p>For <span class="math inline">\(k\)</span> vectors <span class="math inline">\(u_1, u_2, \dots, u_k\)</span>:</p>
<p><span class="math display">\[
u_1 \wedge u_2 \wedge \cdots \wedge u_k
\]</span></p>
<p>represents the oriented <span class="math inline">\(k\)</span>-dimensional volume element (parallelepiped).</p>
<ul>
<li><p>Antisymmetry ensures the wedge vanishes if the vectors are linearly dependent.</p></li>
<li><p>Thus, wedge products capture the idea of dimension of span:</p>
<ul>
<li>Two vectors → area.</li>
<li>Three vectors → volume.</li>
<li>More vectors → higher-dimensional volume.</li>
</ul></li>
</ul>
</section>
<section id="algebraic-properties" class="level4">
<h4 class="anchored" data-anchor-id="algebraic-properties">Algebraic Properties</h4>
<ol type="1">
<li>Anticommutativity: <span class="math inline">\(u \wedge v = -v \wedge u\)</span>.</li>
<li>Associativity (up to sign): <span class="math inline">\((u \wedge v) \wedge w = u \wedge (v \wedge w)\)</span>.</li>
<li>Alternating Property: <span class="math inline">\(u \wedge u = 0\)</span>.</li>
<li>Distributivity: <span class="math inline">\((u+v) \wedge w = u \wedge w + v \wedge w\)</span>.</li>
</ol>
<p>Together, these rules define the exterior algebra <span class="math inline">\(\Lambda(V)\)</span>.</p>
</section>
<section id="why-this-matters-16" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-16">Why This Matters</h4>
<ul>
<li>The wedge product is the algebraic foundation of determinants, areas, and volumes.</li>
<li>It provides a coordinate-free way to work with oriented geometry.</li>
<li>In advanced topics, wedge products lead directly to differential forms and integration on manifolds.</li>
</ul>
</section>
<section id="exercises-28" class="level4">
<h4 class="anchored" data-anchor-id="exercises-28">Exercises</h4>
<ol type="1">
<li><p>Basic Wedge: Compute <span class="math inline">\((1,0) \wedge (0,1)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. What does it represent?</p></li>
<li><p>Antisymmetry Check: Show that <span class="math inline">\((1,2,3) \wedge (4,5,6) = - (4,5,6) \wedge (1,2,3)\)</span>.</p></li>
<li><p>Area via Wedge: Use wedge products to find the area of the parallelogram spanned by <span class="math inline">\(u=(2,0)\)</span> and <span class="math inline">\(v=(1,3)\)</span>.</p></li>
<li><p>Zero Wedge: Show that <span class="math inline">\(u \wedge v = 0\)</span> when <span class="math inline">\(u=(1,2)\)</span>, <span class="math inline">\(v=(2,4)\)</span>. Interpret geometrically.</p></li>
<li><p>Thought Experiment: Why does antisymmetry make wedge products vanish for linearly dependent vectors? How does this encode the idea of dimension reduction?</p></li>
</ol>
</section>
</section>
<section id="areas-volumes-orientation-determinant" class="level3">
<h3 class="anchored" data-anchor-id="areas-volumes-orientation-determinant">8.2 Areas, Volumes, Orientation, Determinant</h3>
<p>Now that we’ve introduced the wedge product, we can connect it directly to geometry: areas, volumes, and orientation. The wedge product is the algebraic language for these concepts.</p>
<section id="d-areas-from-the-wedge-product" class="level4">
<h4 class="anchored" data-anchor-id="d-areas-from-the-wedge-product">2D: Areas from the Wedge Product</h4>
<p>For vectors <span class="math inline">\(u, v \in \mathbb{R}^2\)</span>, the wedge product <span class="math inline">\(u \wedge v\)</span> represents the oriented area of the parallelogram they span.</p>
<ul>
<li><p>Algebraically:</p>
<p><span class="math display">\[
u \wedge v = \det \begin{bmatrix} u_1 &amp; v_1 \\ u_2 &amp; v_2 \end{bmatrix}.
\]</span></p></li>
<li><p>Geometric meaning: <span class="math inline">\(|u \wedge v|\)</span> is the area, the sign encodes clockwise vs.&nbsp;counterclockwise orientation.</p></li>
</ul>
</section>
<section id="d-volumes-via-triple-wedge" class="level4">
<h4 class="anchored" data-anchor-id="d-volumes-via-triple-wedge">3D: Volumes via Triple Wedge</h4>
<p>For <span class="math inline">\(u, v, w \in \mathbb{R}^3\)</span>:</p>
<p><span class="math display">\[
u \wedge v \wedge w
\]</span></p>
<p>represents the oriented volume of the parallelepiped spanned by them.</p>
<ul>
<li><p>Algebraically:</p>
<p><span class="math display">\[
u \wedge v \wedge w = \det \begin{bmatrix} u_1 &amp; v_1 &amp; w_1 \\ u_2 &amp; v_2 &amp; w_2 \\ u_3 &amp; v_3 &amp; w_3 \end{bmatrix}.
\]</span></p></li>
<li><p>Geometric meaning: <span class="math inline">\(|u \wedge v \wedge w|\)</span> is the volume, the sign encodes right- vs.&nbsp;left-handed orientation.</p></li>
</ul>
</section>
<section id="general-n-dimensional-case" class="level4">
<h4 class="anchored" data-anchor-id="general-n-dimensional-case">General <span class="math inline">\(n\)</span>-Dimensional Case</h4>
<p>For <span class="math inline">\(n\)</span> vectors <span class="math inline">\(v_1, \dots, v_n \in \mathbb{R}^n\)</span>:</p>
<p><span class="math display">\[
v_1 \wedge v_2 \wedge \cdots \wedge v_n = \det(v_1, v_2, \dots, v_n),
\]</span></p>
<p>the determinant of the matrix with those vectors as columns.</p>
<ul>
<li>Magnitude: <span class="math inline">\(n\)</span>-dimensional volume of the parallelepiped.</li>
<li>Sign: orientation (positive if vectors preserve orientation, negative if they reverse it).</li>
</ul>
</section>
<section id="orientation" class="level4">
<h4 class="anchored" data-anchor-id="orientation">Orientation</h4>
<ul>
<li>Orientation is the choice of “handedness” (right-hand vs.&nbsp;left-hand rule).</li>
<li>The wedge product encodes orientation automatically via antisymmetry.</li>
<li>Swapping two vectors flips the sign, meaning the orientation is reversed.</li>
</ul>
</section>
<section id="why-this-matters-17" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-17">Why This Matters</h4>
<ul>
<li>Wedge products generalize determinants to all dimensions.</li>
<li>They provide a uniform way to compute areas, volumes, and higher-dimensional volumes.</li>
<li>Orientation, crucial in geometry and physics, is naturally handled through antisymmetry.</li>
</ul>
</section>
<section id="exercises-29" class="level4">
<h4 class="anchored" data-anchor-id="exercises-29">Exercises</h4>
<ol type="1">
<li><p>Area in 2D: Compute the oriented area of the parallelogram spanned by <span class="math inline">\(u=(1,2)\)</span>, <span class="math inline">\(v=(3,4)\)</span> using <span class="math inline">\(u \wedge v\)</span>.</p></li>
<li><p>Volume in 3D: Compute <span class="math inline">\(u \wedge v \wedge w\)</span> for <span class="math inline">\(u=(1,0,0), v=(0,2,0), w=(0,0,3)\)</span>. Interpret geometrically.</p></li>
<li><p>Orientation Flip: Show that <span class="math inline">\(u \wedge v = -v \wedge u\)</span> changes the orientation of the area.</p></li>
<li><p>Determinant Link: Verify that <span class="math inline">\(u \wedge v \wedge w\)</span> in <span class="math inline">\(\mathbb{R}^3\)</span> equals <span class="math inline">\(\det[u,v,w]\)</span>.</p></li>
<li><p>Thought Experiment: Why does the wedge product vanish if the set of vectors is linearly dependent? What does this mean geometrically for areas/volumes?</p></li>
</ol>
</section>
</section>
<section id="basis-dimension-grassmann-algebra" class="level3">
<h3 class="anchored" data-anchor-id="basis-dimension-grassmann-algebra">8.3 Basis, Dimension, Grassmann Algebra</h3>
<p>We’ve seen that wedge products describe oriented areas, volumes, and higher-dimensional analogues. To make this systematic, we construct exterior powers of a vector space, also known as Grassmann algebras.</p>
<section id="exterior-powers-lambdakv" class="level4">
<h4 class="anchored" data-anchor-id="exterior-powers-lambdakv">Exterior Powers <span class="math inline">\(\Lambda^k(V)\)</span></h4>
<p>For a vector space <span class="math inline">\(V\)</span>:</p>
<ul>
<li><span class="math inline">\(\Lambda^k(V)\)</span> is the space of all antisymmetric <span class="math inline">\(k\)</span>-tensors (wedge products of <span class="math inline">\(k\)</span> vectors).</li>
<li><span class="math inline">\(\Lambda^0(V) \cong \mathbb{R}\)</span> (scalars).</li>
<li><span class="math inline">\(\Lambda^1(V) \cong V\)</span> (vectors).</li>
<li><span class="math inline">\(\Lambda^2(V)\)</span>: oriented areas.</li>
<li><span class="math inline">\(\Lambda^3(V)\)</span>: oriented volumes.</li>
<li>In general, <span class="math inline">\(\Lambda^k(V)\)</span> encodes oriented <span class="math inline">\(k\)</span>-dimensional “volume elements.”</li>
</ul>
</section>
<section id="basis-of-lambdakv" class="level4">
<h4 class="anchored" data-anchor-id="basis-of-lambdakv">Basis of <span class="math inline">\(\Lambda^k(V)\)</span></h4>
<p>Suppose <span class="math inline">\(V\)</span> has dimension <span class="math inline">\(n\)</span> with basis <span class="math inline">\(e_1, e_2, \dots, e_n\)</span>.</p>
<ul>
<li><p>A basis for <span class="math inline">\(\Lambda^k(V)\)</span> is given by wedge products:</p>
<p><span class="math display">\[
e_{i_1} \wedge e_{i_2} \wedge \cdots \wedge e_{i_k}, \quad i_1 &lt; i_2 &lt; \cdots &lt; i_k.
\]</span></p></li>
<li><p>These are linearly independent and span all of <span class="math inline">\(\Lambda^k(V)\)</span>.</p></li>
</ul>
</section>
<section id="dimension-formula-1" class="level4">
<h4 class="anchored" data-anchor-id="dimension-formula-1">Dimension Formula</h4>
<p>The dimension of <span class="math inline">\(\Lambda^k(V)\)</span> is:</p>
<p><span class="math display">\[
\dim \Lambda^k(V) = \binom{n}{k}.
\]</span></p>
<ul>
<li>Example: If <span class="math inline">\(n=4\)</span>, then <span class="math inline">\(\dim \Lambda^2(V) = \binom{4}{2} = 6\)</span>.</li>
<li>This matches the number of independent ways to choose <span class="math inline">\(k\)</span> basis vectors from <span class="math inline">\(n\)</span>.</li>
</ul>
</section>
<section id="grassmann-exterior-algebra" class="level4">
<h4 class="anchored" data-anchor-id="grassmann-exterior-algebra">Grassmann (Exterior) Algebra</h4>
<p>The direct sum of all exterior powers:</p>
<p><span class="math display">\[
\Lambda(V) = \Lambda^0(V) \oplus \Lambda^1(V) \oplus \cdots \oplus \Lambda^n(V),
\]</span></p>
<p>is called the exterior algebra (or Grassmann algebra).</p>
<ul>
<li>Multiplication in this algebra is given by the wedge product.</li>
<li>Anticommutativity ensures <span class="math inline">\(u \wedge u = 0\)</span>.</li>
<li>This makes <span class="math inline">\(\Lambda(V)\)</span> into a graded algebra, organized by degree.</li>
</ul>
</section>
<section id="why-this-matters-18" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-18">Why This Matters</h4>
<ul>
<li>Exterior algebra gives a systematic framework for working with antisymmetric tensors.</li>
<li>The dimension formula explains why areas, volumes, etc. fit neatly into subspaces.</li>
<li>Grassmann algebra underlies modern geometry, topology, and physics (differential forms, integration on manifolds).</li>
</ul>
</section>
<section id="exercises-30" class="level4">
<h4 class="anchored" data-anchor-id="exercises-30">Exercises</h4>
<ol type="1">
<li><p>Basis in <span class="math inline">\(\Lambda^2\)</span>: For <span class="math inline">\(V = \mathbb{R}^3\)</span> with basis <span class="math inline">\((e_1,e_2,e_3)\)</span>, list a basis of <span class="math inline">\(\Lambda^2(V)\)</span>.</p></li>
<li><p>Dimension Count: If <span class="math inline">\(\dim V = 5\)</span>, compute <span class="math inline">\(\dim \Lambda^2(V)\)</span>, <span class="math inline">\(\dim \Lambda^3(V)\)</span>, and <span class="math inline">\(\dim \Lambda^4(V)\)</span>.</p></li>
<li><p>Simple Wedge Expansion: In <span class="math inline">\(\mathbb{R}^3\)</span>, expand <span class="math inline">\((e_1 + e_2) \wedge e_3\)</span>.</p></li>
<li><p>Grassmann Algebra Structure: Identify which degrees (<span class="math inline">\(k\)</span>) of <span class="math inline">\(\Lambda^k(V)\)</span> correspond to:</p>
<ul>
<li>scalars,</li>
<li>vectors,</li>
<li>oriented areas,</li>
<li>oriented volumes.</li>
</ul></li>
<li><p>Thought Experiment: Why does the formula <span class="math inline">\(\dim \Lambda^k(V) = \binom{n}{k}\)</span> match the combinatorial idea of “choosing <span class="math inline">\(k\)</span> directions from <span class="math inline">\(n\)</span>”?</p></li>
</ol>
</section>
</section>
<section id="differential-forms-gentle-preview" class="level3">
<h3 class="anchored" data-anchor-id="differential-forms-gentle-preview">8.4 Differential Forms (Gentle Preview)</h3>
<p>We close Part IV with a glimpse of how wedge products extend beyond linear algebra into calculus on manifolds. This leads to differential forms, the language of modern geometry and physics.</p>
<section id="differential-1-forms" class="level4">
<h4 class="anchored" data-anchor-id="differential-1-forms">Differential 1-Forms</h4>
<ul>
<li><p>A 1-form is a covector field: at each point, it eats a vector and gives a number.</p></li>
<li><p>Example in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<p><span class="math display">\[
\omega = x \, dy - y \, dx,
\]</span></p>
<p>where <span class="math inline">\(dx, dy\)</span> are basis 1-forms.</p></li>
</ul>
</section>
<section id="higher-degree-forms" class="level4">
<h4 class="anchored" data-anchor-id="higher-degree-forms">Higher-Degree Forms</h4>
<ul>
<li><p>A k-form is an antisymmetric multilinear map that takes <span class="math inline">\(k\)</span> vectors (directions) and outputs a scalar.</p></li>
<li><p>Built using wedge products of 1-forms:</p>
<ul>
<li><span class="math inline">\(dx \wedge dy\)</span> is a 2-form (measures oriented area in the <span class="math inline">\(xy\)</span>-plane).</li>
<li><span class="math inline">\(dx \wedge dy \wedge dz\)</span> is a 3-form (measures oriented volume in <span class="math inline">\(\mathbb{R}^3\)</span>).</li>
</ul></li>
</ul>
</section>
<section id="exterior-derivative-d" class="level4">
<h4 class="anchored" data-anchor-id="exterior-derivative-d">Exterior Derivative <span class="math inline">\(d\)</span></h4>
<p>Differential forms come with a derivative operator <span class="math inline">\(d\)</span>:</p>
<ul>
<li><p>Takes a <span class="math inline">\(k\)</span>-form to a <span class="math inline">\((k+1)\)</span>-form.</p></li>
<li><p>Generalizes gradient, curl, and divergence in one unified framework.</p></li>
<li><p>Example:</p>
<ul>
<li><p>For <span class="math inline">\(f(x,y)\)</span> a scalar function (0-form):</p>
<p><span class="math display">\[
df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy,
\]</span></p>
<p>which is the gradient written as a 1-form.</p></li>
</ul></li>
</ul>
</section>
<section id="integration-of-forms" class="level4">
<h4 class="anchored" data-anchor-id="integration-of-forms">Integration of Forms</h4>
<ul>
<li>Differential forms can be integrated over curves, surfaces, and higher-dimensional manifolds.</li>
<li>Example: integrating a 1-form along a curve gives the work done by a force field.</li>
<li>Integrating a 2-form over a surface gives the flux through the surface.</li>
</ul>
</section>
<section id="why-this-matters-19" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-19">Why This Matters</h4>
<ul>
<li>Differential forms unify multivariable calculus (gradient, curl, divergence) into one elegant theory.</li>
<li>They are the foundation of Stokes’ theorem, which generalizes all the fundamental theorems of calculus.</li>
<li>Physics: electromagnetism, fluid dynamics, and general relativity all use differential forms.</li>
</ul>
</section>
<section id="exercises-31" class="level4">
<h4 class="anchored" data-anchor-id="exercises-31">Exercises</h4>
<ol type="1">
<li><p>1-Form Action: Let <span class="math inline">\(\omega = 3dx + 2dy\)</span>. Evaluate <span class="math inline">\(\omega(v)\)</span> for <span class="math inline">\(v = (1,4)\)</span>.</p></li>
<li><p>Wedge of 1-Forms: Compute <span class="math inline">\(dx \wedge dy(v,w)\)</span> for <span class="math inline">\(v=(1,0), w=(0,2)\)</span>.</p></li>
<li><p>Exterior Derivative: For <span class="math inline">\(f(x,y) = x^2 y\)</span>, compute <span class="math inline">\(df\)</span>.</p></li>
<li><p>Integration Example: Interpret <span class="math inline">\(\int_\gamma (y \, dx)\)</span> along a path <span class="math inline">\(\gamma\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. What does it represent physically?</p></li>
<li><p>Thought Experiment: How does the wedge product in linear algebra naturally lead into differential forms in calculus?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-9.-hodge-dual-and-metrics-optional-but-useful" class="level2">
<h2 class="anchored" data-anchor-id="chapter-9.-hodge-dual-and-metrics-optional-but-useful">Chapter 9. Hodge Dual and Metrics (Optional but Useful)</h2>
<section id="inner-products-on-exterior-powers" class="level3">
<h3 class="anchored" data-anchor-id="inner-products-on-exterior-powers">9.1 Inner Products on Exterior Powers</h3>
<p>To use wedge products effectively, we need a way to measure their size and compare them. This is where inner products on exterior powers come in. They extend the familiar dot product on vectors to areas, volumes, and higher-dimensional elements.</p>
<section id="induced-inner-product" class="level4">
<h4 class="anchored" data-anchor-id="induced-inner-product">Induced Inner Product</h4>
<p>Suppose <span class="math inline">\(V\)</span> has an inner product <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span>. We can extend it to <span class="math inline">\(\Lambda^k(V)\)</span> by declaring wedge products of basis vectors to be orthonormal:</p>
<p><span class="math display">\[
\langle e_{i_1} \wedge \cdots \wedge e_{i_k}, \, e_{j_1} \wedge \cdots \wedge e_{j_k} \rangle =
\begin{cases}
\det(\delta_{i_a j_b}) &amp; \text{if sets are equal}, \\
0 &amp; \text{otherwise}.
\end{cases}
\]</span></p>
<p>In simpler words:</p>
<ul>
<li>Take all ordered <span class="math inline">\(k\)</span>-tuples of basis vectors.</li>
<li>The wedge products of these are orthonormal if the underlying sets match.</li>
</ul>
</section>
<section id="example-in-mathbbr2" class="level4">
<h4 class="anchored" data-anchor-id="example-in-mathbbr2">Example in <span class="math inline">\(\mathbb{R}^2\)</span></h4>
<p>Let <span class="math inline">\(e_1, e_2\)</span> be an orthonormal basis. Then</p>
<p><span class="math display">\[
\langle e_1 \wedge e_2, e_1 \wedge e_2 \rangle = 1.
\]</span></p>
<p>So <span class="math inline">\(e_1 \wedge e_2\)</span> has unit length, representing a unit square area.</p>
</section>
<section id="example-in-mathbbr3" class="level4">
<h4 class="anchored" data-anchor-id="example-in-mathbbr3">Example in <span class="math inline">\(\mathbb{R}^3\)</span></h4>
<p>Let <span class="math inline">\(e_1, e_2, e_3\)</span> be orthonormal. Then</p>
<ul>
<li><span class="math inline">\(e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3\)</span> form an orthonormal basis of <span class="math inline">\(\Lambda^2(\mathbb{R}^3)\)</span>.</li>
<li><span class="math inline">\(e_1 \wedge e_2 \wedge e_3\)</span> is the unit element of <span class="math inline">\(\Lambda^3(\mathbb{R}^3)\)</span>, representing a unit cube volume.</li>
</ul>
</section>
<section id="norms-of-wedge-products" class="level4">
<h4 class="anchored" data-anchor-id="norms-of-wedge-products">Norms of Wedge Products</h4>
<p>For <span class="math inline">\(u,v \in V\)</span>:</p>
<p><span class="math display">\[
\|u \wedge v\|^2 = \|u\|^2 \|v\|^2 - \langle u,v \rangle^2.
\]</span></p>
<p>This is the formula for the area of a parallelogram spanned by <span class="math inline">\(u,v\)</span>.</p>
<ul>
<li>If <span class="math inline">\(u,v\)</span> are orthogonal: area = product of lengths.</li>
<li>If <span class="math inline">\(u,v\)</span> are parallel: area = 0.</li>
</ul>
<p>For higher <span class="math inline">\(k\)</span>, the squared norm of <span class="math inline">\(u_1 \wedge \cdots \wedge u_k\)</span> equals the determinant of the Gram matrix <span class="math inline">\([\langle u_i,u_j \rangle]\)</span>.</p>
</section>
<section id="why-this-matters-20" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-20">Why This Matters</h4>
<ul>
<li>Inner products on exterior powers connect wedge products to geometry quantitatively (areas, volumes).</li>
<li>They unify length, area, and volume measurement into one consistent framework.</li>
<li>This lays the groundwork for the Hodge star operator, which depends on having an inner product.</li>
</ul>
</section>
<section id="exercises-32" class="level4">
<h4 class="anchored" data-anchor-id="exercises-32">Exercises</h4>
<ol type="1">
<li><p>Area Formula: Compute <span class="math inline">\(\|u \wedge v\|\)</span> for <span class="math inline">\(u=(1,0), v=(1,1)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>.</p></li>
<li><p>Gram Determinant: Show that <span class="math inline">\(\|u \wedge v\|^2 = \det \begin{bmatrix} \langle u,u \rangle &amp; \langle u,v \rangle \\ \langle v,u \rangle &amp; \langle v,v \rangle \end{bmatrix}\)</span>.</p></li>
<li><p>3D Basis Check: Verify that <span class="math inline">\(e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3\)</span> are orthonormal in <span class="math inline">\(\Lambda^2(\mathbb{R}^3)\)</span>.</p></li>
<li><p>Volume Norm: Compute the norm of <span class="math inline">\(e_1 \wedge e_2 \wedge (e_1+e_3)\)</span>. Interpret geometrically.</p></li>
<li><p>Thought Experiment: Why does the Gram determinant naturally appear in wedge product norms? How does this connect to the idea of measuring volume via linear independence?</p></li>
</ol>
</section>
</section>
<section id="hodge-star-pseudo-vectors-cross-products" class="level3">
<h3 class="anchored" data-anchor-id="hodge-star-pseudo-vectors-cross-products">9.2 Hodge Star, Pseudo-Vectors, Cross Products</h3>
<p>With an inner product defined on exterior powers, we can now introduce the Hodge star operator (<span class="math inline">\(\star\)</span>), which creates a bridge between <span class="math inline">\(k\)</span>-forms and <span class="math inline">\((n-k)\)</span>-forms. This operator encodes geometric duality, and in 3D it gives rise to the familiar cross product.</p>
<section id="the-hodge-star-operator" class="level4">
<h4 class="anchored" data-anchor-id="the-hodge-star-operator">The Hodge Star Operator</h4>
<p>Let <span class="math inline">\(V\)</span> be an <span class="math inline">\(n\)</span>-dimensional inner product space with an orientation. The Hodge star is a linear map:</p>
<p><span class="math display">\[
\star : \Lambda^k(V) \to \Lambda^{n-k}(V).
\]</span></p>
<p>It is defined so that for all <span class="math inline">\(\alpha, \beta \in \Lambda^k(V)\)</span>:</p>
<p><span class="math display">\[
\alpha \wedge \star \beta = \langle \alpha, \beta \rangle \, \mathrm{vol},
\]</span></p>
<p>where <span class="math inline">\(\mathrm{vol}\)</span> is the unit volume element in <span class="math inline">\(\Lambda^n(V)\)</span>.</p>
</section>
<section id="examples-in-mathbbr3" class="level4">
<h4 class="anchored" data-anchor-id="examples-in-mathbbr3">Examples in <span class="math inline">\(\mathbb{R}^3\)</span></h4>
<p>With orthonormal basis <span class="math inline">\(e_1, e_2, e_3\)</span>:</p>
<ul>
<li><span class="math inline">\(\star e_1 = e_2 \wedge e_3\)</span>.</li>
<li><span class="math inline">\(\star (e_1 \wedge e_2) = e_3\)</span>.</li>
<li><span class="math inline">\(\star (e_1 \wedge e_2 \wedge e_3) = 1\)</span>.</li>
</ul>
<p>So the star turns 1-vectors into 2-forms, 2-forms into 1-vectors, and the 3-form into a scalar.</p>
</section>
<section id="cross-product-as-hodge-dual" class="level4">
<h4 class="anchored" data-anchor-id="cross-product-as-hodge-dual">Cross Product as Hodge Dual</h4>
<p>In <span class="math inline">\(\mathbb{R}^3\)</span>, the cross product <span class="math inline">\(u \times v\)</span> can be expressed using the Hodge star:</p>
<p><span class="math display">\[
u \times v = \star (u \wedge v).
\]</span></p>
<ul>
<li><span class="math inline">\(u \wedge v\)</span> represents the oriented area spanned by <span class="math inline">\(u,v\)</span>.</li>
<li>The Hodge star converts that 2-form into the unique vector perpendicular to the plane, with magnitude equal to the area.</li>
</ul>
<p>Thus, the cross product is not a primitive concept - it is the Hodge star of a wedge product.</p>
</section>
<section id="pseudo-vectors" class="level4">
<h4 class="anchored" data-anchor-id="pseudo-vectors">Pseudo-Vectors</h4>
<ul>
<li>Objects like angular momentum or magnetic field transform like vectors, but with an orientation twist.</li>
<li>These are pseudo-vectors, naturally arising from antisymmetric 2-forms in <span class="math inline">\(\mathbb{R}^3\)</span>.</li>
<li>The Hodge star provides the link between antisymmetric 2-forms and pseudo-vectors.</li>
</ul>
</section>
<section id="why-this-matters-21" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-21">Why This Matters</h4>
<ul>
<li>The Hodge star encodes geometric duality between subspaces and their complements.</li>
<li>It explains the origin of the cross product in 3D and why it does not generalize to higher dimensions.</li>
<li>It sets the stage for physics applications: electromagnetism, fluid dynamics, and general relativity all use the Hodge dual.</li>
</ul>
</section>
<section id="exercises-33" class="level4">
<h4 class="anchored" data-anchor-id="exercises-33">Exercises</h4>
<ol type="1">
<li><p>Basic Star: In <span class="math inline">\(\mathbb{R}^3\)</span>, compute <span class="math inline">\(\star (e_1)\)</span>, <span class="math inline">\(\star (e_2)\)</span>, and <span class="math inline">\(\star (e_3)\)</span>.</p></li>
<li><p>Star on 2-Forms: Verify that <span class="math inline">\(\star (e_1 \wedge e_2) = e_3\)</span> in <span class="math inline">\(\mathbb{R}^3\)</span>.</p></li>
<li><p>Cross Product via Star: Compute <span class="math inline">\(u \times v\)</span> using <span class="math inline">\(u \times v = \star (u \wedge v)\)</span> for <span class="math inline">\(u=(1,0,0), v=(0,1,0)\)</span>.</p></li>
<li><p>Star Squared: Show that in <span class="math inline">\(\mathbb{R}^3\)</span>, applying the star twice gives <span class="math inline">\(\star \star \alpha = \alpha\)</span> for 1-forms.</p></li>
<li><p>Thought Experiment: Why does the cross product only exist in <span class="math inline">\(\mathbb{R}^3\)</span> (and in a special sense in <span class="math inline">\(\mathbb{R}^7\)</span>)? How does the Hodge star viewpoint clarify this?</p></li>
</ol>
</section>
</section>
<section id="volume-forms-and-integration-glimpses" class="level3">
<h3 class="anchored" data-anchor-id="volume-forms-and-integration-glimpses">9.3 Volume Forms and Integration Glimpses</h3>
<p>We now tie together the Hodge star, wedge products, and inner products to introduce the concept of volume forms - the mathematical tools that let us integrate over spaces of any dimension.</p>
<section id="volume-form" class="level4">
<h4 class="anchored" data-anchor-id="volume-form">Volume Form</h4>
<p>In an <span class="math inline">\(n\)</span>-dimensional oriented inner product space <span class="math inline">\(V\)</span>, the volume form is the unit <span class="math inline">\(n\)</span>-form:</p>
<p><span class="math display">\[
\mathrm{vol} = e_1 \wedge e_2 \wedge \cdots \wedge e_n,
\]</span></p>
<p>where <span class="math inline">\(\{e_i\}\)</span> is an orthonormal positively oriented basis.</p>
<ul>
<li><p>It represents the “unit <span class="math inline">\(n\)</span>-dimensional volume element.”</p></li>
<li><p>Any <span class="math inline">\(n\)</span> vectors <span class="math inline">\(v_1,\dots,v_n\)</span> give</p>
<p><span class="math display">\[
\mathrm{vol}(v_1,\dots,v_n) = \det[v_1 \ \cdots \ v_n],
\]</span></p>
<p>i.e.&nbsp;their oriented volume.</p></li>
</ul>
</section>
<section id="volume-via-hodge-star" class="level4">
<h4 class="anchored" data-anchor-id="volume-via-hodge-star">Volume via Hodge Star</h4>
<p>For a <span class="math inline">\(k\)</span>-form <span class="math inline">\(\alpha\)</span>, its dual <span class="math inline">\(\star \alpha\)</span> is an <span class="math inline">\((n-k)\)</span>-form.</p>
<ul>
<li>The wedge <span class="math inline">\(\alpha \wedge \star \alpha\)</span> is proportional to <span class="math inline">\(\mathrm{vol}\)</span>.</li>
<li>This expresses how much “volume” <span class="math inline">\(\alpha\)</span> contributes in <span class="math inline">\(n\)</span>-dimensions.</li>
</ul>
<p>Example in <span class="math inline">\(\mathbb{R}^3\)</span>:</p>
<ul>
<li><p>If <span class="math inline">\(\alpha = u \wedge v\)</span>, then</p>
<p><span class="math display">\[
\alpha \wedge \star \alpha = \|u \wedge v\|^2 \, \mathrm{vol},
\]</span></p>
<p>encoding the squared area of the parallelogram spanned by <span class="math inline">\(u,v\)</span>.</p></li>
</ul>
</section>
<section id="integration-of-differential-forms-glimpse" class="level4">
<h4 class="anchored" data-anchor-id="integration-of-differential-forms-glimpse">Integration of Differential Forms (Glimpse)</h4>
<p>The volume form allows us to integrate over manifolds:</p>
<ul>
<li><p>In <span class="math inline">\(\mathbb{R}^n\)</span>, integrating <span class="math inline">\(\mathrm{vol}\)</span> over a region gives its volume.</p></li>
<li><p>A <span class="math inline">\(k\)</span>-form can be integrated over a <span class="math inline">\(k\)</span>-dimensional surface (curve, area, etc.).</p></li>
<li><p>Example:</p>
<ul>
<li>In <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math inline">\(\mathrm{vol} = dx \wedge dy\)</span>.</li>
<li>The integral <span class="math inline">\(\int_\Omega dx \wedge dy\)</span> is the area of region <span class="math inline">\(\Omega\)</span>.</li>
</ul></li>
</ul>
<p>This is the seed of Stokes’ theorem, which unifies the fundamental theorem of calculus, Green’s theorem, and the divergence theorem.</p>
</section>
<section id="why-this-matters-22" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-22">Why This Matters</h4>
<ul>
<li>Volume forms make precise the link between algebra (determinants) and geometry (volume).</li>
<li>They are the foundation for integration on curved spaces (manifolds).</li>
<li>In physics, volume forms appear in conservation laws, flux integrals, and field theories.</li>
</ul>
</section>
<section id="exercises-34" class="level4">
<h4 class="anchored" data-anchor-id="exercises-34">Exercises</h4>
<ol type="1">
<li><p>2D Volume Form: Show that for <span class="math inline">\(u=(1,0), v=(0,2)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>: <span class="math inline">\(\mathrm{vol}(u,v) = \det \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 2\end{bmatrix}\)</span>.</p></li>
<li><p>3D Example: Compute <span class="math inline">\(\mathrm{vol}(u,v,w)\)</span> for <span class="math inline">\(u=(1,0,0), v=(0,1,0), w=(0,0,2)\)</span>.</p></li>
<li><p>Star Relation: In <span class="math inline">\(\mathbb{R}^3\)</span>, verify that <span class="math inline">\((u \wedge v) \wedge \star (u \wedge v) = \|u \wedge v\|^2 \, \mathrm{vol}\)</span>.</p></li>
<li><p>Integration Preview: Evaluate <span class="math inline">\(\int_{[0,1]\times [0,1]} dx \wedge dy\)</span>. Interpret the result.</p></li>
<li><p>Thought Experiment: Why is orientation essential when defining a volume form? What would go wrong in integration if we ignored orientation?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-v.-symmetric-tensors-and-polynomial-view" class="level1">
<h1>Part V. Symmetric Tensors and Polynomial View</h1>
<section id="chapter-10.-symmetric-powers-and-homogeneous-polynomials" class="level2">
<h2 class="anchored" data-anchor-id="chapter-10.-symmetric-powers-and-homogeneous-polynomials">Chapter 10. Symmetric Powers and Homogeneous Polynomials</h2>
<section id="symmetric-tensors-as-polynomial-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="symmetric-tensors-as-polynomial-coefficients">10.1 Symmetric Tensors as Polynomial Coefficients</h3>
<p>So far, we have studied antisymmetric tensors and exterior algebra. Now we turn to the opposite case: symmetric tensors. These are equally important, and they connect directly to polynomials.</p>
<section id="symmetric-tensors-1" class="level4">
<h4 class="anchored" data-anchor-id="symmetric-tensors-1">Symmetric Tensors</h4>
<p>A symmetric tensor of order <span class="math inline">\(k\)</span> satisfies:</p>
<p><span class="math display">\[
T_{i_1 i_2 \dots i_k} = T_{i_{\pi(1)} i_{\pi(2)} \dots i_{\pi(k)}}
\]</span></p>
<p>for any permutation <span class="math inline">\(\pi\)</span>.</p>
<p>Example:</p>
<ul>
<li>A quadratic form <span class="math inline">\(Q(x) = x^\top A x\)</span> comes from a symmetric 2-tensor.</li>
<li>Higher-order symmetric tensors generalize this to cubic, quartic, etc.</li>
</ul>
</section>
<section id="from-symmetric-tensors-to-polynomials" class="level4">
<h4 class="anchored" data-anchor-id="from-symmetric-tensors-to-polynomials">From Symmetric Tensors to Polynomials</h4>
<p>Every symmetric <span class="math inline">\(k\)</span>-tensor corresponds to a homogeneous polynomial of degree <span class="math inline">\(k\)</span>.</p>
<ul>
<li><p>Given a symmetric tensor <span class="math inline">\(T_{i_1 \dots i_k}\)</span>:</p>
<p><span class="math display">\[
p(x) = T_{i_1 \dots i_k} x^{i_1} \cdots x^{i_k}.
\]</span></p></li>
<li><p>Example: In <span class="math inline">\(\mathbb{R}^2\)</span>, let <span class="math inline">\(T_{11}=1, T_{12}=T_{21}=2, T_{22}=3\)</span>. Then</p>
<p><span class="math display">\[
p(x,y) = x^2 + 4xy + 3y^2.
\]</span></p></li>
</ul>
<p>This is a homogeneous polynomial of degree 2.</p>
</section>
<section id="from-polynomials-to-symmetric-tensors" class="level4">
<h4 class="anchored" data-anchor-id="from-polynomials-to-symmetric-tensors">From Polynomials to Symmetric Tensors</h4>
<p>Conversely, every homogeneous polynomial defines a symmetric tensor:</p>
<ul>
<li>Example: <span class="math inline">\(p(x,y,z) = 2x^2z + 3yz^2\)</span>.</li>
<li>Its coefficients directly correspond to entries of a symmetric 3-tensor.</li>
</ul>
<p>Thus, symmetric tensors and homogeneous polynomials are two sides of the same coin.</p>
</section>
<section id="why-this-matters-23" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-23">Why This Matters</h4>
<ul>
<li>This correspondence makes symmetric tensors central in algebraic geometry and statistics.</li>
<li>Moments of probability distributions are symmetric tensors.</li>
<li>Polynomial optimization problems can be rephrased in terms of symmetric tensors.</li>
</ul>
</section>
<section id="exercises-35" class="level4">
<h4 class="anchored" data-anchor-id="exercises-35">Exercises</h4>
<ol type="1">
<li><p>Quadratic Form: Show that the polynomial <span class="math inline">\(p(x,y) = 3x^2 + 2xy + y^2\)</span> corresponds to a symmetric 2-tensor. Write its matrix.</p></li>
<li><p>Cubic Example: Write the symmetric 3-tensor for <span class="math inline">\(p(x,y) = x^3 + 3x^2y + 2y^3\)</span>.</p></li>
<li><p>Backwards: Given the symmetric tensor with entries <span class="math inline">\(T_{111}=1, T_{112}=2, T_{122}=3, T_{222}=4\)</span>, write the polynomial in two variables.</p></li>
<li><p>Homogeneity Check: Explain why the polynomial <span class="math inline">\(x^2 + xy + y^2\)</span> is homogeneous of degree 2, but <span class="math inline">\(x^2 + y + 1\)</span> is not.</p></li>
<li><p>Thought Experiment: Why might it be useful to switch between tensor and polynomial viewpoints in machine learning or physics? ### 10.2 Polarization Identities</p></li>
</ol>
<p>We saw that symmetric tensors and homogeneous polynomials are two sides of the same coin. But how exactly do we go from one to the other in a precise way? The tool for this is the polarization identity, which allows us to reconstruct a symmetric multilinear map (or tensor) from its associated polynomial.</p>
</section>
<section id="from-symmetric-tensor-to-polynomial" class="level4">
<h4 class="anchored" data-anchor-id="from-symmetric-tensor-to-polynomial">From Symmetric Tensor to Polynomial</h4>
<p>If <span class="math inline">\(T\)</span> is a symmetric <span class="math inline">\(k\)</span>-linear form on <span class="math inline">\(V\)</span>, we define the polynomial</p>
<p><span class="math display">\[
p(x) = T(x,x,\dots,x).
\]</span></p>
<p>This is a homogeneous polynomial of degree <span class="math inline">\(k\)</span>.</p>
<p>Example:</p>
<ul>
<li>If <span class="math inline">\(T(x,y) = \langle x,y \rangle\)</span>, then <span class="math inline">\(p(x) = \langle x,x \rangle = \|x\|^2\)</span>.</li>
</ul>
</section>
<section id="from-polynomial-to-symmetric-tensor" class="level4">
<h4 class="anchored" data-anchor-id="from-polynomial-to-symmetric-tensor">From Polynomial to Symmetric Tensor</h4>
<p>Given a homogeneous polynomial <span class="math inline">\(p(x)\)</span> of degree <span class="math inline">\(k\)</span>, we want to recover the symmetric tensor <span class="math inline">\(T\)</span>.</p>
<p>The polarization identity says:</p>
<p><span class="math display">\[
T(x_1, \dots, x_k) = \frac{1}{k! \, 2^k} \sum_{\epsilon_1,\dots,\epsilon_k = \pm 1} \epsilon_1 \cdots \epsilon_k \, p(\epsilon_1 x_1 + \cdots + \epsilon_k x_k).
\]</span></p>
<p>This formula extracts the multilinear coefficients hidden inside the polynomial.</p>
</section>
<section id="example-quadratic-forms" class="level4">
<h4 class="anchored" data-anchor-id="example-quadratic-forms">Example: Quadratic Forms</h4>
<p>For <span class="math inline">\(p(x) = \|x\|^2\)</span>:</p>
<ul>
<li><p>Using polarization, we recover the bilinear form:</p>
<p><span class="math display">\[
T(x,y) = \tfrac{1}{2} \big( \|x+y\|^2 - \|x\|^2 - \|y\|^2 \big) = \langle x,y \rangle.
\]</span></p></li>
</ul>
<p>This is the classical polarization identity for inner products.</p>
</section>
<section id="why-this-matters-24" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-24">Why This Matters</h4>
<ul>
<li><p>The polarization identity ensures that the correspondence between symmetric tensors and homogeneous polynomials is exact and invertible.</p></li>
<li><p>It provides a way to compute multilinear structure from polynomial data.</p></li>
<li><p>In applications:</p>
<ul>
<li>In physics, energy polynomials give rise to symmetric stress tensors.</li>
<li>In statistics, cumulants and moments correspond to symmetric tensors recovered from generating functions.</li>
</ul></li>
</ul>
</section>
<section id="exercises-36" class="level4">
<h4 class="anchored" data-anchor-id="exercises-36">Exercises</h4>
<ol type="1">
<li><p>Inner Product Recovery: Use the quadratic polarization identity to show that</p>
<p><span class="math display">\[
\langle x,y \rangle = \tfrac{1}{4} \big( \|x+y\|^2 - \|x-y\|^2 \big).
\]</span></p></li>
<li><p>Cubic Case: Let <span class="math inline">\(p(x) = x_1^3\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. Use the polarization formula to compute <span class="math inline">\(T(e_1,e_1,e_1)\)</span> and <span class="math inline">\(T(e_1,e_1,e_2)\)</span>.</p></li>
<li><p>Verification: For <span class="math inline">\(p(x,y) = x^2 + y^2\)</span>, verify that the polarization identity recovers the symmetric bilinear form <span class="math inline">\(T(x,y) = x_1y_1 + x_2y_2\)</span>.</p></li>
<li><p>Homogeneity Check: Show why polarization fails if <span class="math inline">\(p(x)\)</span> is not homogeneous.</p></li>
<li><p>Thought Experiment: Why is it important that the tensor be symmetric in order for the polynomial ↔︎ tensor correspondence to work smoothly?</p></li>
</ol>
</section>
</section>
<section id="polarization-identities" class="level3">
<h3 class="anchored" data-anchor-id="polarization-identities">10.2 Polarization Identities</h3>
<p>We saw that symmetric tensors and homogeneous polynomials are two sides of the same coin. But how exactly do we go from one to the other in a precise way? The tool for this is the polarization identity, which allows us to reconstruct a symmetric multilinear map (or tensor) from its associated polynomial.</p>
<section id="from-symmetric-tensor-to-polynomial-1" class="level4">
<h4 class="anchored" data-anchor-id="from-symmetric-tensor-to-polynomial-1">From Symmetric Tensor to Polynomial</h4>
<p>If <span class="math inline">\(T\)</span> is a symmetric <span class="math inline">\(k\)</span>-linear form on <span class="math inline">\(V\)</span>, we define the polynomial</p>
<p><span class="math display">\[
p(x) = T(x,x,\dots,x).
\]</span></p>
<p>This is a homogeneous polynomial of degree <span class="math inline">\(k\)</span>.</p>
<p>Example:</p>
<ul>
<li>If <span class="math inline">\(T(x,y) = \langle x,y \rangle\)</span>, then <span class="math inline">\(p(x) = \langle x,x \rangle = \|x\|^2\)</span>.</li>
</ul>
</section>
<section id="from-polynomial-to-symmetric-tensor-1" class="level4">
<h4 class="anchored" data-anchor-id="from-polynomial-to-symmetric-tensor-1">From Polynomial to Symmetric Tensor</h4>
<p>Given a homogeneous polynomial <span class="math inline">\(p(x)\)</span> of degree <span class="math inline">\(k\)</span>, we want to recover the symmetric tensor <span class="math inline">\(T\)</span>.</p>
<p>The polarization identity says:</p>
<p><span class="math display">\[
T(x_1, \dots, x_k) = \frac{1}{k! \, 2^k} \sum_{\epsilon_1,\dots,\epsilon_k = \pm 1} \epsilon_1 \cdots \epsilon_k \, p(\epsilon_1 x_1 + \cdots + \epsilon_k x_k).
\]</span></p>
<p>This formula extracts the multilinear coefficients hidden inside the polynomial.</p>
</section>
<section id="example-quadratic-forms-1" class="level4">
<h4 class="anchored" data-anchor-id="example-quadratic-forms-1">Example: Quadratic Forms</h4>
<p>For <span class="math inline">\(p(x) = \|x\|^2\)</span>:</p>
<ul>
<li><p>Using polarization, we recover the bilinear form:</p>
<p><span class="math display">\[
T(x,y) = \tfrac{1}{2} \big( \|x+y\|^2 - \|x\|^2 - \|y\|^2 \big) = \langle x,y \rangle.
\]</span></p></li>
</ul>
<p>This is the classical polarization identity for inner products.</p>
</section>
<section id="why-this-matters-25" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-25">Why This Matters</h4>
<ul>
<li><p>The polarization identity ensures that the correspondence between symmetric tensors and homogeneous polynomials is exact and invertible.</p></li>
<li><p>It provides a way to compute multilinear structure from polynomial data.</p></li>
<li><p>In applications:</p>
<ul>
<li>In physics, energy polynomials give rise to symmetric stress tensors.</li>
<li>In statistics, cumulants and moments correspond to symmetric tensors recovered from generating functions.</li>
</ul></li>
</ul>
</section>
<section id="exercises-37" class="level4">
<h4 class="anchored" data-anchor-id="exercises-37">Exercises</h4>
<ol type="1">
<li><p>Inner Product Recovery: Use the quadratic polarization identity to show that</p>
<p><span class="math display">\[
\langle x,y \rangle = \tfrac{1}{4} \big( \|x+y\|^2 - \|x-y\|^2 \big).
\]</span></p></li>
<li><p>Cubic Case: Let <span class="math inline">\(p(x) = x_1^3\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. Use the polarization formula to compute <span class="math inline">\(T(e_1,e_1,e_1)\)</span> and <span class="math inline">\(T(e_1,e_1,e_2)\)</span>.</p></li>
<li><p>Verification: For <span class="math inline">\(p(x,y) = x^2 + y^2\)</span>, verify that the polarization identity recovers the symmetric bilinear form <span class="math inline">\(T(x,y) = x_1y_1 + x_2y_2\)</span>.</p></li>
<li><p>Homogeneity Check: Show why polarization fails if <span class="math inline">\(p(x)\)</span> is not homogeneous.</p></li>
<li><p>Thought Experiment: Why is it important that the tensor be symmetric in order for the polynomial ↔︎ tensor correspondence to work smoothly?</p></li>
</ol>
</section>
</section>
<section id="moments-and-cumulants" class="level3">
<h3 class="anchored" data-anchor-id="moments-and-cumulants">10.3 Moments and Cumulants</h3>
<p>Symmetric tensors are not just abstract objects - they naturally appear in probability and statistics. Two key concepts, moments and cumulants, can be represented as symmetric tensors, providing a multilinear view of random variables and their dependencies.</p>
<section id="moments-as-symmetric-tensors" class="level4">
<h4 class="anchored" data-anchor-id="moments-as-symmetric-tensors">Moments as Symmetric Tensors</h4>
<p>For a random vector <span class="math inline">\(X \in \mathbb{R}^n\)</span>, the <span class="math inline">\(k\)</span>-th moment tensor is:</p>
<p><span class="math display">\[
M^{(k)} = \mathbb{E}[X^{\otimes k}],
\]</span></p>
<p>where</p>
<p><span class="math display">\[
X^{\otimes k} = X \otimes X \otimes \cdots \otimes X \quad (k \text{ times}).
\]</span></p>
<ul>
<li><p>Entries:</p>
<p><span class="math display">\[
(M^{(k)})_{i_1 i_2 \dots i_k} = \mathbb{E}[X_{i_1} X_{i_2} \cdots X_{i_k}].
\]</span></p></li>
<li><p>Since multiplication is commutative, <span class="math inline">\(M^{(k)}\)</span> is a symmetric tensor.</p></li>
</ul>
<p>Examples:</p>
<ul>
<li><span class="math inline">\(M^{(1)}\)</span> = mean vector.</li>
<li><span class="math inline">\(M^{(2)}\)</span> = second moment matrix (related to covariance).</li>
<li>Higher-order moments capture skewness, kurtosis, etc.</li>
</ul>
</section>
<section id="cumulants-as-symmetric-tensors" class="level4">
<h4 class="anchored" data-anchor-id="cumulants-as-symmetric-tensors">Cumulants as Symmetric Tensors</h4>
<p>Cumulants refine moments by removing redundancy:</p>
<ul>
<li><p>Defined via the log of the moment-generating function.</p></li>
<li><p>The <span class="math inline">\(k\)</span>-th cumulant tensor <span class="math inline">\(C^{(k)}\)</span> is also symmetric.</p></li>
<li><p>Example:</p>
<ul>
<li><span class="math inline">\(C^{(1)}\)</span> = mean.</li>
<li><span class="math inline">\(C^{(2)}\)</span> = covariance matrix.</li>
<li><span class="math inline">\(C^{(3)}\)</span> = skewness tensor.</li>
<li><span class="math inline">\(C^{(4)}\)</span> = kurtosis tensor.</li>
</ul></li>
</ul>
</section>
<section id="why-tensors" class="level4">
<h4 class="anchored" data-anchor-id="why-tensors">Why Tensors?</h4>
<ul>
<li><p>Moments and cumulants encode multi-way dependencies between variables.</p></li>
<li><p>As tensors, they can be studied with linear algebra and decompositions.</p></li>
<li><p>Applications:</p>
<ul>
<li>Signal processing: blind source separation via cumulant tensors.</li>
<li>Statistics: identifying distributions via moment tensors.</li>
<li>Machine learning: feature extraction from higher-order statistics.</li>
</ul></li>
</ul>
</section>
<section id="connection-to-symmetric-polynomials" class="level4">
<h4 class="anchored" data-anchor-id="connection-to-symmetric-polynomials">Connection to Symmetric Polynomials</h4>
<ul>
<li>Moments correspond to coefficients of homogeneous polynomials (moment generating functions).</li>
<li>Polarization identities let us move between the polynomial representation and the tensor form.</li>
</ul>
</section>
<section id="exercises-38" class="level4">
<h4 class="anchored" data-anchor-id="exercises-38">Exercises</h4>
<ol type="1">
<li><p>Second Moment: Let <span class="math inline">\(X = (X_1,X_2)\)</span> with <span class="math inline">\(\mathbb{E}[X_1^2]=2, \mathbb{E}[X_1X_2]=1, \mathbb{E}[X_2^2]=3\)</span>. Write the 2nd moment tensor <span class="math inline">\(M^{(2)}\)</span>.</p></li>
<li><p>Symmetry Check: Show that <span class="math inline">\((M^{(3)})_{ijk} = \mathbb{E}[X_i X_j X_k]\)</span> is symmetric in all indices.</p></li>
<li><p>Covariance as Cumulant: Explain why the covariance matrix is the 2nd cumulant tensor <span class="math inline">\(C^{(2)}\)</span>.</p></li>
<li><p>Higher Cumulants: What does <span class="math inline">\(C^{(3)}\)</span> measure that is not captured by <span class="math inline">\(C^{(2)}\)</span>?</p></li>
<li><p>Thought Experiment: Why might cumulants be more useful than moments in separating independent signals or detecting non-Gaussianity?</p></li>
</ol>
</section>
</section>
<section id="low-rank-symmetric-decompositions" class="level3">
<h3 class="anchored" data-anchor-id="low-rank-symmetric-decompositions">10.4 Low-Rank Symmetric Decompositions</h3>
<p>Symmetric tensors, like general tensors, can be decomposed into simpler parts. In the symmetric case, this often means writing a symmetric tensor as a sum of outer products of vectors with themselves. These low-rank symmetric decompositions play a central role in data science, signal processing, and machine learning.</p>
<section id="symmetric-rank" class="level4">
<h4 class="anchored" data-anchor-id="symmetric-rank">Symmetric Rank</h4>
<p>For a symmetric tensor <span class="math inline">\(T \in \mathrm{Sym}^k(V)\)</span>, the symmetric rank is the smallest <span class="math inline">\(r\)</span> such that</p>
<p><span class="math display">\[
T = \sum_{i=1}^r \lambda_i \, v_i^{\otimes k},
\]</span></p>
<p>where <span class="math inline">\(v_i \in V\)</span> and <span class="math inline">\(\lambda_i \in \mathbb{R}\)</span>.</p>
<ul>
<li>Each term <span class="math inline">\(v_i^{\otimes k} = v_i \otimes v_i \otimes \cdots \otimes v_i\)</span> (k times) is a simple symmetric tensor.</li>
<li>This is the symmetric analogue of the rank-one decomposition for matrices.</li>
</ul>
</section>
<section id="example-quadratic-forms-2" class="level4">
<h4 class="anchored" data-anchor-id="example-quadratic-forms-2">Example: Quadratic Forms</h4>
<p>A quadratic form (symmetric 2-tensor) can be decomposed as:</p>
<p><span class="math display">\[
Q(x) = \sum_{i=1}^r \lambda_i (v_i^\top x)^2.
\]</span></p>
<p>This is the spectral decomposition of a symmetric matrix.</p>
</section>
<section id="higher-order-case" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-case">Higher-Order Case</h4>
<p>For cubic or quartic symmetric tensors:</p>
<ul>
<li><p>Example in 3rd order:</p>
<p><span class="math display">\[
T(x,y,z) = \sum_{i=1}^r \lambda_i (v_i^\top x)(v_i^\top y)(v_i^\top z).
\]</span></p></li>
<li><p>This is the CP decomposition restricted to the symmetric case.</p></li>
</ul>
</section>
<section id="applications" class="level4">
<h4 class="anchored" data-anchor-id="applications">Applications</h4>
<ul>
<li>Statistics: cumulant tensors often admit low-rank symmetric decompositions, useful in blind source separation.</li>
<li>Machine learning: compressing polynomial kernels, tensor regression, deep learning model compression.</li>
<li>Algebraic geometry: the study of Waring decompositions of polynomials.</li>
</ul>
</section>
<section id="why-this-matters-26" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-26">Why This Matters</h4>
<ul>
<li>Low-rank symmetric decompositions reduce complexity, making huge tensors manageable.</li>
<li>They connect tensor algebra with classical spectral theory and polynomial factorization.</li>
<li>They explain why symmetric tensors are a natural language for representing data with latent low-dimensional structure.</li>
</ul>
</section>
<section id="exercises-39" class="level4">
<h4 class="anchored" data-anchor-id="exercises-39">Exercises</h4>
<ol type="1">
<li><p>Matrix Case: Decompose</p>
<p><span class="math display">\[
A = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix}
\]</span></p>
<p>as a sum of symmetric rank-one matrices.</p></li>
<li><p>Simple Symmetric Tensor: Write explicitly <span class="math inline">\(v^{\otimes 3}\)</span> for <span class="math inline">\(v=(1,2)\)</span>.</p></li>
<li><p>Symmetric Decomposition: Express the quadratic form <span class="math inline">\(Q(x,y) = 4x^2 + 2xy + y^2\)</span> as a sum of rank-one terms <span class="math inline">\(\lambda_i (a_i x + b_i y)^2\)</span>.</p></li>
<li><p>Interpretation: For a 3rd-order symmetric tensor in <span class="math inline">\(\mathbb{R}^2\)</span>, how many coefficients does it have? Compare this with how many are needed to specify a rank-1 symmetric decomposition.</p></li>
<li><p>Thought Experiment: Why might low-rank symmetric decompositions be especially valuable in machine learning (hint: think compression and interpretability)?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-vi.-linear-maps-between-tensor-spaces" class="level1">
<h1>Part VI. Linear Maps Between Tensor Spaces</h1>
<section id="chapter-11.-reshaping-vectorization-and-commutation" class="level2">
<h2 class="anchored" data-anchor-id="chapter-11.-reshaping-vectorization-and-commutation">Chapter 11. Reshaping, Vectorization, and Commutation</h2>
<section id="mode-n-unfolding-and-matricization" class="level3">
<h3 class="anchored" data-anchor-id="mode-n-unfolding-and-matricization">11.1 Mode-n Unfolding and Matricization</h3>
<p>When working with higher-order tensors, it is often useful to “flatten” them into matrices so that standard linear algebra tools can be applied. This process is called unfolding or matricization.</p>
<section id="definition-mode-n-unfolding" class="level4">
<h4 class="anchored" data-anchor-id="definition-mode-n-unfolding">Definition: Mode-n Unfolding</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, the mode-n unfolding arranges all entries into a matrix where:</p>
<ul>
<li>The row index corresponds to the <span class="math inline">\(n\)</span>-th mode.</li>
<li>The column index corresponds to all other modes combined.</li>
</ul>
<p>Formally:</p>
<p><span class="math display">\[
T_{i_1 i_2 \dots i_N} \quad \mapsto \quad T^{(n)}_{i_n, j},
\]</span></p>
<p>where <span class="math inline">\(j\)</span> encodes the multi-index <span class="math inline">\((i_1,\dots,i_{n-1},i_{n+1},\dots,i_N)\)</span>.</p>
</section>
<section id="example-3rd-order-tensor" class="level4">
<h4 class="anchored" data-anchor-id="example-3rd-order-tensor">Example (3rd-Order Tensor)</h4>
<p>If <span class="math inline">\(T\)</span> has shape <span class="math inline">\((I_1,I_2,I_3)\)</span>:</p>
<ul>
<li>Mode-1 unfolding: size <span class="math inline">\(I_1 \times (I_2 I_3)\)</span>.</li>
<li>Mode-2 unfolding: size <span class="math inline">\(I_2 \times (I_1 I_3)\)</span>.</li>
<li>Mode-3 unfolding: size <span class="math inline">\(I_3 \times (I_1 I_2)\)</span>.</li>
</ul>
<p>This reshaping preserves all entries, only changing their layout.</p>
</section>
<section id="why-unfold" class="level4">
<h4 class="anchored" data-anchor-id="why-unfold">Why Unfold?</h4>
<ul>
<li>Brings tensors into a familiar matrix framework.</li>
<li>Enables the use of matrix decompositions (SVD, QR, eigenvalue methods).</li>
<li>Used in algorithms for tensor decompositions (Tucker, CP, HOSVD).</li>
<li>Useful in machine learning for feature extraction, low-rank approximations, and compression.</li>
</ul>
</section>
<section id="connection-to-linear-maps" class="level4">
<h4 class="anchored" data-anchor-id="connection-to-linear-maps">Connection to Linear Maps</h4>
<p>Mode-<span class="math inline">\(n\)</span> unfolding allows us to represent how a linear operator acts on the <span class="math inline">\(n\)</span>-th mode of a tensor.</p>
<ul>
<li><p>For example, applying a matrix <span class="math inline">\(A \in \mathbb{R}^{J \times I_n}\)</span> to mode-<span class="math inline">\(n\)</span> corresponds to left-multiplying the unfolded tensor:</p>
<p><span class="math display">\[
(T \times_n A)^{(n)} = A \, T^{(n)}.
\]</span></p></li>
</ul>
<p>This makes tensor-matrix products consistent with matrix multiplication.</p>
</section>
<section id="why-this-matters-27" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-27">Why This Matters</h4>
<ul>
<li>Mode-n unfolding bridges the gap between multilinear algebra and classical linear algebra.</li>
<li>It provides the foundation for defining multilinear rank and tensor decompositions.</li>
<li>Without unfolding, many computational algorithms for tensors would be infeasible.</li>
</ul>
</section>
<section id="exercises-40" class="level4">
<h4 class="anchored" data-anchor-id="exercises-40">Exercises</h4>
<ol type="1">
<li><p>Shape Check: If <span class="math inline">\(T\)</span> has dimensions <span class="math inline">\((2,3,4)\)</span>, what are the shapes of its mode-1, mode-2, and mode-3 unfoldings?</p></li>
<li><p>Matrix Form: Write out explicitly the mode-1 unfolding of a tensor <span class="math inline">\(T \in \mathbb{R}^{2 \times 2 \times 2}\)</span>.</p></li>
<li><p>Operator Action: Let <span class="math inline">\(T \in \mathbb{R}^{2 \times 3 \times 4}\)</span>. If <span class="math inline">\(A \in \mathbb{R}^{5 \times 2}\)</span>, what is the shape of <span class="math inline">\((T \times_1 A)\)</span>?</p></li>
<li><p>Reversibility: Explain why unfolding is reversible (i.e., why no information is lost).</p></li>
<li><p>Thought Experiment: Why is it advantageous to analyze tensors via unfoldings rather than directly in their multi-index form?</p></li>
</ol>
</section>
</section>
<section id="vec-operator-and-kronecker-identities" class="level3">
<h3 class="anchored" data-anchor-id="vec-operator-and-kronecker-identities">11.2 Vec Operator and Kronecker Identities</h3>
<p>Once a tensor has been unfolded into a matrix, a common next step is to “flatten” that matrix into a vector. This operation is called vectorization, or the vec operator. It provides a bridge between multilinear algebra and standard linear algebra identities involving Kronecker products.</p>
<section id="the-vec-operator" class="level4">
<h4 class="anchored" data-anchor-id="the-vec-operator">The Vec Operator</h4>
<p>For a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>:</p>
<p><span class="math display">\[
\mathrm{vec}(A) \in \mathbb{R}^{mn}
\]</span></p>
<p>is obtained by stacking the columns of <span class="math inline">\(A\)</span> into a single vector.</p>
<p>Example:</p>
<p><span class="math display">\[
A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}, \quad
\mathrm{vec}(A) = \begin{bmatrix} a \\ c \\ b \\ d \end{bmatrix}.
\]</span></p>
<p>For higher-order tensors, we apply vec after unfolding into a matrix.</p>
</section>
<section id="key-identity-matrix-multiplication" class="level4">
<h4 class="anchored" data-anchor-id="key-identity-matrix-multiplication">Key Identity (Matrix Multiplication)</h4>
<p>For matrices <span class="math inline">\(A, X, B\)</span> of compatible sizes:</p>
<p><span class="math display">\[
\mathrm{vec}(AXB^\top) = (B \otimes A)\, \mathrm{vec}(X).
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(\otimes\)</span> is the Kronecker product.</li>
<li>This identity rewrites a two-sided matrix multiplication as a single linear transformation.</li>
</ul>
</section>
<section id="tensor-extension" class="level4">
<h4 class="anchored" data-anchor-id="tensor-extension">Tensor Extension</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, linear maps applied along each mode can be expressed in vec form using Kronecker products.</p>
<p>If <span class="math inline">\(A_n \in \mathbb{R}^{J_n \times I_n}\)</span>, then</p>
<p><span class="math display">\[
(T \times_1 A_1 \times_2 A_2 \cdots \times_N A_N)^{\mathrm{vec}} = (A_N \otimes \cdots \otimes A_2 \otimes A_1) \, \mathrm{vec}(T).
\]</span></p>
<p>This is the multilinear analogue of the matrix vec identity.</p>
</section>
<section id="applications-1" class="level4">
<h4 class="anchored" data-anchor-id="applications-1">Applications</h4>
<ul>
<li>Efficient coding of multilinear transformations.</li>
<li>Proofs of tensor decomposition algorithms.</li>
<li>Numerical linear algebra (solving tensor equations).</li>
<li>Machine learning: efficient representation of Kronecker-structured layers.</li>
</ul>
</section>
<section id="why-this-matters-28" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-28">Why This Matters</h4>
<ul>
<li>The vec operator turns multilinear problems into linear ones.</li>
<li>Kronecker products capture structure in large transformations compactly.</li>
<li>These tools make computation with tensors more systematic and efficient.</li>
</ul>
</section>
<section id="exercises-41" class="level4">
<h4 class="anchored" data-anchor-id="exercises-41">Exercises</h4>
<ol type="1">
<li><p>Vec of a Matrix: Compute <span class="math inline">\(\mathrm{vec}\left(\begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}\right)\)</span>.</p></li>
<li><p>Matrix Identity Check: Let <span class="math inline">\(A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 2\end{bmatrix}, X = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}, B = \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>. Verify that <span class="math inline">\(\mathrm{vec}(AXB^\top) = (B \otimes A) \, \mathrm{vec}(X)\)</span>.</p></li>
<li><p>Tensor Shape: If <span class="math inline">\(T \in \mathbb{R}^{2 \times 3 \times 4}\)</span>, what is the dimension of <span class="math inline">\(\mathrm{vec}(T)\)</span>?</p></li>
<li><p>Kronecker Ordering: Explain why the order of factors in <span class="math inline">\((A_N \otimes \cdots \otimes A_1)\)</span> matters.</p></li>
<li><p>Thought Experiment: Why does vectorization make multilinear algebra easier to connect with existing linear algebra tools?</p></li>
</ol>
</section>
</section>
<section id="linear-operators-acting-on-tensors" class="level3">
<h3 class="anchored" data-anchor-id="linear-operators-acting-on-tensors">11.3 Linear Operators Acting on Tensors</h3>
<p>So far, we have unfolded tensors and vectorized them to connect with matrix operations. Now we look at tensors as natural domains and codomains for linear operators. This viewpoint is powerful because many tensor operations are simply linear maps on tensor product spaces.</p>
<section id="operators-on-tensor-product-spaces" class="level4">
<h4 class="anchored" data-anchor-id="operators-on-tensor-product-spaces">Operators on Tensor Product Spaces</h4>
<p>If <span class="math inline">\(A: V \to V'\)</span> and <span class="math inline">\(B: W \to W'\)</span> are linear maps, then their tensor product operator</p>
<p><span class="math display">\[
A \otimes B : V \otimes W \to V' \otimes W'
\]</span></p>
<p>is defined by</p>
<p><span class="math display">\[
(A \otimes B)(v \otimes w) = (Av) \otimes (Bw).
\]</span></p>
<p>This definition extends linearly to all of <span class="math inline">\(V \otimes W\)</span>.</p>
</section>
<section id="example-with-matrices" class="level4">
<h4 class="anchored" data-anchor-id="example-with-matrices">Example with Matrices</h4>
<p>If <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(B \in \mathbb{R}^{p \times q}\)</span>, and <span class="math inline">\(X \in \mathbb{R}^{n \times q}\)</span>:</p>
<p><span class="math display">\[
(A \otimes B)\,\mathrm{vec}(X) = \mathrm{vec}(BXA^\top).
\]</span></p>
<p>This links tensor product operators to Kronecker product identities.</p>
</section>
<section id="mode-n-multiplication-as-operator-action" class="level4">
<h4 class="anchored" data-anchor-id="mode-n-multiplication-as-operator-action">Mode-n Multiplication as Operator Action</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times \cdots \times I_N}\)</span>, multiplying along the <span class="math inline">\(n\)</span>-th mode by a matrix <span class="math inline">\(A \in \mathbb{R}^{J \times I_n}\)</span>:</p>
<p><span class="math display">\[
T' = T \times_n A,
\]</span></p>
<p>is equivalent to applying the operator <span class="math inline">\(I \otimes \cdots \otimes A \otimes \cdots \otimes I\)</span> to <span class="math inline">\(\mathrm{vec}(T)\)</span>.</p>
<p>Thus, tensor–matrix products are just specialized cases of linear operators on tensor spaces.</p>
</section>
<section id="higher-order-operators" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-operators">Higher-Order Operators</h4>
<p>More generally, a linear operator can act on multiple modes simultaneously:</p>
<p><span class="math display">\[
T' = T \times_1 A_1 \times_2 A_2 \cdots \times_N A_N.
\]</span></p>
<p>This corresponds to the operator</p>
<p><span class="math display">\[
A_N \otimes \cdots \otimes A_2 \otimes A_1
\]</span></p>
<p>on the vectorized tensor.</p>
</section>
<section id="why-this-matters-29" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-29">Why This Matters</h4>
<ul>
<li>Many “complicated” tensor operations are just linear maps in disguise.</li>
<li>Operator language clarifies the connection between abstract multilinear algebra and concrete matrix computations.</li>
<li>This sets the stage for defining tensor rank, decompositions, and algorithms.</li>
</ul>
</section>
<section id="exercises-42" class="level4">
<h4 class="anchored" data-anchor-id="exercises-42">Exercises</h4>
<ol type="1">
<li><p>Operator on Product: Let <span class="math inline">\(A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 2\end{bmatrix}\)</span>, <span class="math inline">\(B = \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>. Compute <span class="math inline">\((A \otimes B)(e_1 \otimes e_2)\)</span>.</p></li>
<li><p>Mode-1 Action: For <span class="math inline">\(T \in \mathbb{R}^{2 \times 3}\)</span>, let <span class="math inline">\(A = \begin{bmatrix}1 &amp; 1 \\ 0 &amp; 1\end{bmatrix}\)</span>. Write explicitly how <span class="math inline">\(T \times_1 A\)</span> transforms rows.</p></li>
<li><p>Vec Form: Verify that <span class="math inline">\((A \otimes B)\,\mathrm{vec}(X) = \mathrm{vec}(BXA^\top)\)</span> for <span class="math inline">\(A = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 4\end{bmatrix}, B = I\)</span>.</p></li>
<li><p>Composed Operators: Show that <span class="math inline">\((A_1 \otimes B_1)(A_2 \otimes B_2) = (A_1A_2) \otimes (B_1B_2)\)</span>.</p></li>
<li><p>Thought Experiment: Why is it useful to think of tensor–matrix multiplication as applying a linear operator on a tensor space, instead of just as array reshaping?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-12.-metrics-forms-and-raisinglowering-indices" class="level2">
<h2 class="anchored" data-anchor-id="chapter-12.-metrics-forms-and-raisinglowering-indices">Chapter 12. Metrics, Forms, and Raising/Lowering Indices</h2>
<section id="using-inner-products-to-move-indices" class="level3">
<h3 class="anchored" data-anchor-id="using-inner-products-to-move-indices">12.1 Using Inner Products to Move Indices</h3>
<p>In tensor calculus, indices can appear as upper (contravariant) or lower (covariant). Inner products give us a way to convert between them. This process is called raising and lowering indices.</p>
<section id="covariant-vs.-contravariant-indices-1" class="level4">
<h4 class="anchored" data-anchor-id="covariant-vs.-contravariant-indices-1">Covariant vs.&nbsp;Contravariant Indices</h4>
<ul>
<li>A vector <span class="math inline">\(v\)</span> has components <span class="math inline">\(v^i\)</span> (upper index).</li>
<li>A covector (linear functional) <span class="math inline">\(\omega\)</span> has components <span class="math inline">\(\omega_i\)</span> (lower index).</li>
<li>A tensor may mix both kinds, e.g.&nbsp;<span class="math inline">\(T^{i}{}_{j}\)</span>.</li>
</ul>
<p>Without an inner product, upper and lower indices live in different spaces (<span class="math inline">\(V\)</span> and <span class="math inline">\(V^*\)</span>).</p>
</section>
<section id="using-the-metric-tensor" class="level4">
<h4 class="anchored" data-anchor-id="using-the-metric-tensor">Using the Metric Tensor</h4>
<p>An inner product (or metric) <span class="math inline">\(g\)</span> provides a natural way to link vectors and covectors.</p>
<ul>
<li>Metric tensor: <span class="math inline">\(g_{ij} = \langle e_i, e_j \rangle\)</span>.</li>
<li>Inverse metric: <span class="math inline">\(g^{ij}\)</span> satisfies <span class="math inline">\(g^{ij} g_{jk} = \delta^i_k\)</span>.</li>
</ul>
</section>
<section id="lowering-an-index" class="level4">
<h4 class="anchored" data-anchor-id="lowering-an-index">Lowering an Index</h4>
<p>Given a vector <span class="math inline">\(v^i\)</span>, define its covector by:</p>
<p><span class="math display">\[
v_i = g_{ij} v^j.
\]</span></p>
<p>This maps <span class="math inline">\(V \to V^*\)</span>.</p>
</section>
<section id="raising-an-index" class="level4">
<h4 class="anchored" data-anchor-id="raising-an-index">Raising an Index</h4>
<p>Given a covector <span class="math inline">\(\omega_i\)</span>, define its vector by:</p>
<p><span class="math display">\[
\omega^i = g^{ij} \omega_j.
\]</span></p>
<p>This maps <span class="math inline">\(V^- \to V\)</span>.</p>
</section>
<section id="example-in-mathbbr2-with-euclidean-metric" class="level4">
<h4 class="anchored" data-anchor-id="example-in-mathbbr2-with-euclidean-metric">Example in <span class="math inline">\(\mathbb{R}^2\)</span> with Euclidean Metric</h4>
<p>Let <span class="math inline">\(g = I\)</span> (the identity matrix). Then raising and lowering indices does nothing:</p>
<ul>
<li><span class="math inline">\(v_i = v^i\)</span>.</li>
<li><span class="math inline">\(\omega^i = \omega_i\)</span>.</li>
</ul>
<p>But in a non-Euclidean metric (e.g., relativity), raising/lowering changes components significantly.</p>
</section>
<section id="why-this-matters-30" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-30">Why This Matters</h4>
<ul>
<li>Index manipulation allows us to move seamlessly between vector and covector viewpoints.</li>
<li>In geometry and physics, metrics define lengths, angles, and duality.</li>
<li>In relativity, raising and lowering indices with the Minkowski metric distinguishes between time and space components.</li>
</ul>
</section>
<section id="exercises-43" class="level4">
<h4 class="anchored" data-anchor-id="exercises-43">Exercises</h4>
<ol type="1">
<li><p>Lowering Indices: In <span class="math inline">\(\mathbb{R}^2\)</span> with metric</p>
<p><span class="math display">\[
g = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix},
\]</span></p>
<p>compute the covector <span class="math inline">\(v_i\)</span> for <span class="math inline">\(v^j = (1,4)\)</span>.</p></li>
<li><p>Raising Indices: Using the same metric, compute <span class="math inline">\(\omega^i\)</span> from <span class="math inline">\(\omega_j = (2,6)\)</span>.</p></li>
<li><p>Check Consistency: Verify that raising and then lowering an index returns the original vector.</p></li>
<li><p>Physics Example: In special relativity, the Minkowski metric is</p>
<p><span class="math display">\[
g = \mathrm{diag}(-1,1,1,1).
\]</span></p>
<p>Show how lowering the time component of a 4-vector changes its sign.</p></li>
<li><p>Thought Experiment: Why is an inner product (metric) essential for connecting vectors and covectors? What would break if we tried to raise/lower indices without one? ### 12.2 Dualizations and Adjoint Operations</p></li>
</ol>
<p>Raising and lowering indices with a metric doesn’t just apply to vectors and covectors - it also extends naturally to linear maps and tensors. This leads to the notions of duals and adjoints, which are central in multilinear algebra, physics, and functional analysis.</p>
</section>
<section id="dual-of-a-linear-map" class="level4">
<h4 class="anchored" data-anchor-id="dual-of-a-linear-map">Dual of a Linear Map</h4>
<p>Given a linear map <span class="math inline">\(A: V \to W\)</span>, its dual map is</p>
<p><span class="math display">\[
A^*: W^- \to V^*
\]</span></p>
<p>defined by</p>
<p><span class="math display">\[
(A^- \omega)(v) = \omega(Av),
\]</span></p>
<p>for all <span class="math inline">\(v \in V, \, \omega \in W^*\)</span>.</p>
<ul>
<li>In matrix form: If <span class="math inline">\(A\)</span> has matrix <span class="math inline">\(M\)</span>, then <span class="math inline">\(A^*\)</span> has matrix <span class="math inline">\(M^\top\)</span>.</li>
</ul>
</section>
<section id="adjoint-of-a-linear-map-1" class="level4">
<h4 class="anchored" data-anchor-id="adjoint-of-a-linear-map-1">Adjoint of a Linear Map</h4>
<p>If <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> have inner products, we can define the adjoint <span class="math inline">\(A^\dagger: W \to V\)</span> by the property:</p>
<p><span class="math display">\[
\langle Av, w \rangle_W = \langle v, A^\dagger w \rangle_V.
\]</span></p>
<ul>
<li><p>In Euclidean space with the standard dot product, <span class="math inline">\(A^\dagger = A^\top\)</span>.</p></li>
<li><p>With a general metric tensor <span class="math inline">\(g\)</span>, the adjoint depends on raising/lowering indices:</p>
<p><span class="math display">\[
(A^\dagger)^i{}_j = g^{ik} A^l{}_k g_{lj}.
\]</span></p></li>
</ul>
</section>
<section id="extension-to-tensors" class="level4">
<h4 class="anchored" data-anchor-id="extension-to-tensors">Extension to Tensors</h4>
<ul>
<li><p>For tensors with mixed indices, dualization flips upper ↔︎ lower indices.</p></li>
<li><p>Example: For <span class="math inline">\(T^i{}_j\)</span>, the dual acts as</p>
<p><span class="math display">\[
(T^*)_i{}^j = g_{ik} g^{jl} T^k{}_l.
\]</span></p></li>
</ul>
<p>This systematically moves indices while preserving linear relationships.</p>
</section>
<section id="why-this-matters-31" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-31">Why This Matters</h4>
<ul>
<li>Dualization is purely algebraic (map between spaces and their duals).</li>
<li>Adjoint operations involve the metric and carry geometric meaning (orthogonality, length).</li>
<li>In physics, adjoints appear in energy conservation laws, quantum mechanics (Hermitian operators), and relativity.</li>
</ul>
</section>
<section id="exercises-44" class="level4">
<h4 class="anchored" data-anchor-id="exercises-44">Exercises</h4>
<ol type="1">
<li><p>Dual Map: Let <span class="math inline">\(A = \begin{bmatrix}1 &amp; 2 \\ 0 &amp; 3\end{bmatrix}\)</span>. Compute its dual <span class="math inline">\(A^*\)</span>.</p></li>
<li><p>Adjoint in Euclidean Space: Show that in <span class="math inline">\(\mathbb{R}^2\)</span> with the standard dot product, the adjoint of <span class="math inline">\(A\)</span> is just <span class="math inline">\(A^\top\)</span>.</p></li>
<li><p>Adjoint with Metric: In <span class="math inline">\(\mathbb{R}^2\)</span> with metric</p>
<p><span class="math display">\[
g = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix},
\]</span></p>
<p>compute the adjoint of <span class="math inline">\(A = \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>.</p></li>
<li><p>Tensor Dualization: For a tensor <span class="math inline">\(T^i{}_j = \delta^i_j\)</span> (the identity), compute its dual under the Euclidean metric.</p></li>
<li><p>Thought Experiment: Why does the adjoint depend on the choice of inner product, while the dual does not? What does this tell us about algebra vs.&nbsp;geometry? ### 12.3 Coordinate Rules Made Simple</p></li>
</ol>
<p>So far, we’ve seen raising/lowering of indices, dual maps, and adjoints. These can look abstract, but in practice they reduce to straightforward coordinate rules once a basis and metric are fixed. This section summarizes the “index gymnastics” in a beginner-friendly way.</p>
</section>
<section id="raising-and-lowering" class="level4">
<h4 class="anchored" data-anchor-id="raising-and-lowering">Raising and Lowering</h4>
<ul>
<li><p>Lowering:</p>
<p><span class="math display">\[
v_i = g_{ij} v^j
\]</span></p></li>
<li><p>Raising:</p>
<p><span class="math display">\[
\omega^i = g^{ij} \omega_j
\]</span></p></li>
</ul>
<p>Here <span class="math inline">\(g_{ij}\)</span> is the metric, <span class="math inline">\(g^{ij}\)</span> its inverse.</p>
<p>Tip: Think of lowering as “applying <span class="math inline">\(g\)</span>” and raising as “applying <span class="math inline">\(g^{-1}\)</span>.”</p>
</section>
<section id="dual-maps" class="level4">
<h4 class="anchored" data-anchor-id="dual-maps">Dual Maps</h4>
<p>For a matrix <span class="math inline">\(A\)</span> representing a linear map:</p>
<p><span class="math display">\[
(A^*)_{ij} = A_{ji}.
\]</span></p>
<p>So the dual corresponds to a transpose in coordinates.</p>
</section>
<section id="adjoint-maps" class="level4">
<h4 class="anchored" data-anchor-id="adjoint-maps">Adjoint Maps</h4>
<p>With a general metric <span class="math inline">\(g\)</span>, the adjoint of a matrix <span class="math inline">\(A\)</span> is:</p>
<p><span class="math display">\[
A^\dagger = g^{-1} A^\top g.
\]</span></p>
<ul>
<li>If <span class="math inline">\(g = I\)</span> (Euclidean metric), then <span class="math inline">\(A^\dagger = A^\top\)</span>.</li>
<li>This formula generalizes to any inner product space.</li>
</ul>
</section>
<section id="tensors-with-mixed-indices" class="level4">
<h4 class="anchored" data-anchor-id="tensors-with-mixed-indices">Tensors with Mixed Indices</h4>
<p>For a tensor <span class="math inline">\(T^{i}{}_j\)</span>, indices can be moved by contracting with <span class="math inline">\(g_{ij}\)</span> or <span class="math inline">\(g^{ij}\)</span>:</p>
<ul>
<li><p>To lower an upper index:</p>
<p><span class="math display">\[
T_{ij} = g_{ik} T^k{}_j.
\]</span></p></li>
<li><p>To raise a lower index:</p>
<p><span class="math display">\[
T^{ij} = g^{jk} T^i{}_k.
\]</span></p></li>
</ul>
<p>This bookkeeping ensures correct transformations under basis changes.</p>
</section>
<section id="why-this-matters-32" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-32">Why This Matters</h4>
<ul>
<li>These coordinate rules are the practical toolkit for working with metrics and adjoints.</li>
<li>In relativity, this is exactly how time and space components are manipulated.</li>
<li>In machine learning and physics, these rules underlie how gradients, covariances, and bilinear forms are expressed.</li>
</ul>
</section>
<section id="exercises-45" class="level4">
<h4 class="anchored" data-anchor-id="exercises-45">Exercises</h4>
<ol type="1">
<li><p>Lower an Index: In <span class="math inline">\(\mathbb{R}^2\)</span> with metric <span class="math inline">\(g = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 3\end{bmatrix}\)</span>, compute <span class="math inline">\(v_i\)</span> for <span class="math inline">\(v^j = (1,2)\)</span>.</p></li>
<li><p>Raise an Index: With the same metric, compute <span class="math inline">\(\omega^i\)</span> for <span class="math inline">\(\omega_j = (4,6)\)</span>.</p></li>
<li><p>Adjoint Check: For <span class="math inline">\(A = \begin{bmatrix}1 &amp; 2 \\ 0 &amp; 3\end{bmatrix}\)</span>, compute <span class="math inline">\(A^\dagger\)</span> under <span class="math inline">\(g = I\)</span>.</p></li>
<li><p>Mixed Tensor: Given <span class="math inline">\(T^i{}_j = \begin{bmatrix}1 &amp; 0 \\ 2 &amp; 3\end{bmatrix}\)</span>, compute <span class="math inline">\(T_{ij}\)</span> using <span class="math inline">\(g = I\)</span>.</p></li>
<li><p>Thought Experiment: Why do physicists insist on keeping track of upper vs.&nbsp;lower indices, while engineers often ignore the distinction in Euclidean spaces?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-vii.-tensor-ranks-and-decompositions" class="level1">
<h1>Part VII. Tensor Ranks and Decompositions</h1>
<section id="chapter-13.-ranks-for-tensors" class="level2">
<h2 class="anchored" data-anchor-id="chapter-13.-ranks-for-tensors">Chapter 13. Ranks for Tensors</h2>
<section id="matrix-rank-vs.-tensor-rank" class="level3">
<h3 class="anchored" data-anchor-id="matrix-rank-vs.-tensor-rank">13.1 Matrix Rank vs.&nbsp;Tensor Rank</h3>
<p>Before diving into advanced tensor decompositions, we need to understand what rank means for tensors. Unlike matrices, where rank is simple and unique, tensors have several different notions of rank. This section starts with the familiar matrix rank and then introduces tensor rank.</p>
<section id="matrix-rank-review" class="level4">
<h4 class="anchored" data-anchor-id="matrix-rank-review">Matrix Rank (Review)</h4>
<p>For a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>:</p>
<ul>
<li><p>The rank is the dimension of its column space (or row space).</p></li>
<li><p>Equivalently, the smallest <span class="math inline">\(r\)</span> such that</p>
<p><span class="math display">\[
A = \sum_{i=1}^r u_i v_i^\top,
\]</span></p>
<p>where <span class="math inline">\(u_i \in \mathbb{R}^m, v_i \in \mathbb{R}^n\)</span>.</p></li>
<li><p>Each term is a rank-one matrix.</p></li>
</ul>
<p>Example:</p>
<p><span class="math display">\[
\begin{bmatrix}1 &amp; 2 \\ 2 &amp; 4\end{bmatrix}
= \begin{bmatrix}1 \\ 2\end{bmatrix} \begin{bmatrix}1 &amp; 2\end{bmatrix},
\]</span></p>
<p>so its rank is 1.</p>
</section>
<section id="tensor-rank" class="level4">
<h4 class="anchored" data-anchor-id="tensor-rank">Tensor Rank</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, the tensor rank (sometimes called CP rank) is the smallest <span class="math inline">\(r\)</span> such that</p>
<p><span class="math display">\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]</span></p>
<p>where each <span class="math inline">\(u^{(k)}_i \in \mathbb{R}^{I_k}\)</span>.</p>
<ul>
<li>Each term is a rank-one tensor (outer product of vectors).</li>
<li>Rank measures how many simple pieces are needed to build the tensor.</li>
</ul>
</section>
<section id="key-differences-matrix-vs.-tensor" class="level4">
<h4 class="anchored" data-anchor-id="key-differences-matrix-vs.-tensor">Key Differences: Matrix vs.&nbsp;Tensor</h4>
<ul>
<li>Uniqueness: Matrix rank is uniquely defined; tensor rank can vary depending on the notion (CP rank, Tucker rank, etc.).</li>
<li>Computation: Matrix rank is easy to compute (via SVD); tensor rank is NP-hard to compute in general.</li>
<li>Upper Bound: For a matrix <span class="math inline">\(m \times n\)</span>, rank ≤ min(m,n). For a tensor, the maximum possible rank is often not obvious.</li>
</ul>
</section>
<section id="example-3-way-tensor" class="level4">
<h4 class="anchored" data-anchor-id="example-3-way-tensor">Example: 3-Way Tensor</h4>
<p>Let</p>
<p><span class="math display">\[
T_{ijk} = 1 \quad \text{if } i=j=k, \quad 0 \text{ otherwise}.
\]</span></p>
<p>This “diagonal” tensor has tensor rank = dimension <span class="math inline">\(n\)</span>.</p>
</section>
<section id="why-this-matters-33" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-33">Why This Matters</h4>
<ul>
<li>Tensor rank generalizes the concept of complexity from matrices to higher-order data.</li>
<li>Low-rank structure is crucial in applications: compression, latent factor models, signal separation.</li>
<li>Understanding tensor rank lays the foundation for CP, Tucker, and TT decompositions.</li>
</ul>
</section>
<section id="exercises-46" class="level4">
<h4 class="anchored" data-anchor-id="exercises-46">Exercises</h4>
<ol type="1">
<li><p>Matrix Rank: Compute the rank of</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 1 \\ 2 &amp; 2\end{bmatrix}.
\]</span></p></li>
<li><p>Tensor Rank-1: Show that the tensor <span class="math inline">\(T_{ijk} = a_i b_j c_k\)</span> has rank 1.</p></li>
<li><p>Decomposition Practice: Express</p>
<p><span class="math display">\[
T_{ij} = \begin{bmatrix}1 &amp; 2 \\ 3 &amp; 6\end{bmatrix}
\]</span></p>
<p>as a sum of rank-one matrices.</p></li>
<li><p>Diagonal Tensor: For <span class="math inline">\(T_{ijk}\)</span> with <span class="math inline">\(T_{111}=1, T_{222}=1\)</span>, all others 0, determine its rank.</p></li>
<li><p>Thought Experiment: Why might tensor rank be harder to compute and less well-behaved than matrix rank?</p></li>
</ol>
</section>
</section>
<section id="multilinear-tucker-rank-and-mode-ranks" class="level3">
<h3 class="anchored" data-anchor-id="multilinear-tucker-rank-and-mode-ranks">13.2 Multilinear (Tucker) Rank and Mode Ranks</h3>
<p>Besides CP rank, tensors have another important notion of rank: the multilinear rank, also called the Tucker rank. This definition is based on the ranks of mode-<span class="math inline">\(n\)</span> unfoldings and gives a more stable and computable measure of complexity.</p>
<section id="mode-n-rank" class="level4">
<h4 class="anchored" data-anchor-id="mode-n-rank">Mode-n Rank</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>:</p>
<ul>
<li>The mode-n rank is the matrix rank of its mode-<span class="math inline">\(n\)</span> unfolding <span class="math inline">\(T^{(n)}\)</span>.</li>
<li>Denote it by <span class="math inline">\(\text{rank}_n(T)\)</span>.</li>
</ul>
</section>
<section id="multilinear-tucker-rank" class="level4">
<h4 class="anchored" data-anchor-id="multilinear-tucker-rank">Multilinear (Tucker) Rank</h4>
<p>The multilinear rank of <span class="math inline">\(T\)</span> is the tuple</p>
<p><span class="math display">\[
\big( \text{rank}_1(T), \text{rank}_2(T), \dots, \text{rank}_N(T) \big).
\]</span></p>
<p>This captures how much independent variation the tensor has along each mode.</p>
</section>
<section id="example-3rd-order-tensor-1" class="level4">
<h4 class="anchored" data-anchor-id="example-3rd-order-tensor-1">Example (3rd-Order Tensor)</h4>
<p>Suppose <span class="math inline">\(T \in \mathbb{R}^{3 \times 4 \times 5}\)</span>.</p>
<ul>
<li>Mode-1 unfolding rank = 2.</li>
<li>Mode-2 unfolding rank = 3.</li>
<li>Mode-3 unfolding rank = 4.</li>
</ul>
<p>Then the multilinear rank of <span class="math inline">\(T\)</span> is <span class="math inline">\((2,3,4)\)</span>.</p>
</section>
<section id="comparison-with-cp-rank" class="level4">
<h4 class="anchored" data-anchor-id="comparison-with-cp-rank">Comparison with CP Rank</h4>
<ul>
<li>CP rank: minimal number of rank-1 outer products.</li>
<li>Tucker rank: ranks of unfoldings (tuple of integers).</li>
<li>CP rank is a single number, often hard to compute.</li>
<li>Tucker rank is a multi-dimensional profile, easier to compute via SVD of unfoldings.</li>
</ul>
</section>
<section id="tucker-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="tucker-decomposition">Tucker Decomposition</h4>
<p>The multilinear rank is closely tied to the Tucker decomposition:</p>
<p><span class="math display">\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(G\)</span> = core tensor,</li>
<li><span class="math inline">\(U^{(n)}\)</span> = basis matrices capturing each mode’s subspace.</li>
</ul>
<p>The dimensions of <span class="math inline">\(G\)</span> are exactly the multilinear rank.</p>
</section>
<section id="why-this-matters-34" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-34">Why This Matters</h4>
<ul>
<li>Multilinear rank provides a practical, computable measure of tensor complexity.</li>
<li>It is robust under noise and approximations, unlike CP rank.</li>
<li>Widely used in tensor compression (Tucker, HOSVD) and machine learning.</li>
</ul>
</section>
<section id="exercises-47" class="level4">
<h4 class="anchored" data-anchor-id="exercises-47">Exercises</h4>
<ol type="1">
<li><p>Matrix Case: Show that for a matrix (2nd-order tensor), the Tucker rank reduces to the usual matrix rank.</p></li>
<li><p>Rank Profile: Suppose <span class="math inline">\(T \in \mathbb{R}^{2 \times 3 \times 4}\)</span> has mode ranks (2,2,1). What does this tell you about its structure?</p></li>
<li><p>Unfolding Rank: For a tensor <span class="math inline">\(T_{ijk} = u_i v_j\)</span> (independent of <span class="math inline">\(k\)</span>), compute its Tucker rank.</p></li>
<li><p>Comparison: Give an example of a tensor with low Tucker rank but high CP rank.</p></li>
<li><p>Thought Experiment: Why might Tucker rank be preferred over CP rank in applications like compression or noise-robust signal processing? ### 13.3 Identifiability and Uniqueness</p></li>
</ol>
<p>When we decompose a tensor into simpler components, a natural question arises: is the decomposition unique? For matrices, the SVD gives a stable, essentially unique factorization. For tensors, things are more subtle. This section explores identifiability - the conditions under which tensor decompositions are unique.</p>
</section>
<section id="identifiability-in-cp-rank" class="level4">
<h4 class="anchored" data-anchor-id="identifiability-in-cp-rank">Identifiability in CP Rank</h4>
<p>For a CP (CANDECOMP/PARAFAC) decomposition</p>
<p><span class="math display">\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]</span></p>
<p>the decomposition is said to be identifiable if no other decomposition with the same rank <span class="math inline">\(r\)</span> exists (up to trivial scaling and permutation).</p>
<ul>
<li><p>Trivial indeterminacies:</p>
<ul>
<li>Scaling: multiplying one factor by <span class="math inline">\(\alpha\)</span> and another by <span class="math inline">\(1/\alpha\)</span>.</li>
<li>Permutation: reordering the components.</li>
</ul></li>
</ul>
</section>
<section id="kruskals-condition-for-uniqueness" class="level4">
<h4 class="anchored" data-anchor-id="kruskals-condition-for-uniqueness">Kruskal’s Condition (for Uniqueness)</h4>
<p>A famous result: If each factor matrix <span class="math inline">\(U^{(n)}\)</span> has Kruskal rank <span class="math inline">\(k_n\)</span> (maximum number of columns that are linearly independent), and</p>
<p><span class="math display">\[
k_1 + k_2 + \cdots + k_N \geq 2r + (N-1),
\]</span></p>
<p>then the CP decomposition is unique (up to scaling and permutation).</p>
</section>
<section id="identifiability-in-tucker-rank" class="level4">
<h4 class="anchored" data-anchor-id="identifiability-in-tucker-rank">Identifiability in Tucker Rank</h4>
<p>For Tucker decomposition</p>
<p><span class="math display">\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]</span></p>
<p>uniqueness is generally weaker:</p>
<ul>
<li>The core tensor <span class="math inline">\(G\)</span> is not unique.</li>
<li>However, the subspaces spanned by the factor matrices <span class="math inline">\(U^{(n)}\)</span> are unique (this is the essence of HOSVD).</li>
</ul>
</section>
<section id="why-uniqueness-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-uniqueness-matters">Why Uniqueness Matters</h4>
<ul>
<li>In applications like chemometrics, signal separation, and latent factor models, unique decomposition means interpretable components.</li>
<li>Without identifiability, decompositions may still compress data but lose meaning.</li>
<li>CP rank uniqueness is one reason tensors can capture latent structure more faithfully than matrices.</li>
</ul>
</section>
<section id="challenges" class="level4">
<h4 class="anchored" data-anchor-id="challenges">Challenges</h4>
<ul>
<li>Checking uniqueness conditions is not always easy.</li>
<li>In practice, algorithms may converge to non-unique solutions.</li>
<li>Regularization and domain knowledge are often used to guide towards interpretable decompositions.</li>
</ul>
</section>
<section id="exercises-48" class="level4">
<h4 class="anchored" data-anchor-id="exercises-48">Exercises</h4>
<ol type="1">
<li><p>Matrix vs.&nbsp;Tensor: Why is the SVD of a matrix unique (up to signs), but CP decomposition of a tensor not always unique?</p></li>
<li><p>Scaling Ambiguity: Show explicitly how scaling one factor vector and compensating in another keeps the CP decomposition unchanged.</p></li>
<li><p>Permutation Ambiguity: Given two equivalent CP decompositions, illustrate how permuting rank-1 components yields the same tensor.</p></li>
<li><p>Kruskal’s Condition: Suppose a 3rd-order tensor has factor matrices with Kruskal ranks (3,3,3). For rank <span class="math inline">\(r=3\)</span>, does Kruskal’s condition guarantee uniqueness?</p></li>
<li><p>Thought Experiment: Why might uniqueness of tensor decompositions be more valuable in data analysis than uniqueness of matrix decompositions?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-14.-cannonical-decompositions" class="level2">
<h2 class="anchored" data-anchor-id="chapter-14.-cannonical-decompositions">Chapter 14. Cannonical Decompositions</h2>
<section id="cp-candecompparafac" class="level3">
<h3 class="anchored" data-anchor-id="cp-candecompparafac">14.1 CP (CANDECOMP/PARAFAC)</h3>
<p>The Canonical Polyadic (CP) decomposition - also known as CANDECOMP/PARAFAC - is one of the most fundamental ways to factorize a tensor. It generalizes the idea of expressing a matrix as a sum of rank-one outer products to higher-order tensors.</p>
<section id="definition-2" class="level4">
<h4 class="anchored" data-anchor-id="definition-2">Definition</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, a CP decomposition of rank <span class="math inline">\(r\)</span> is:</p>
<p><span class="math display">\[
T = \sum_{i=1}^r u^{(1)}_i \otimes u^{(2)}_i \otimes \cdots \otimes u^{(N)}_i,
\]</span></p>
<p>where each <span class="math inline">\(u^{(n)}_i \in \mathbb{R}^{I_n}\)</span>.</p>
<ul>
<li>Each term is a rank-one tensor.</li>
<li>The smallest such <span class="math inline">\(r\)</span> is the CP rank of <span class="math inline">\(T\)</span>.</li>
</ul>
</section>
<section id="example-matrix-case" class="level4">
<h4 class="anchored" data-anchor-id="example-matrix-case">Example (Matrix Case)</h4>
<p>For a matrix <span class="math inline">\(A\)</span>, the CP decomposition reduces to the familiar rank factorization:</p>
<p><span class="math display">\[
A = \sum_{i=1}^r u_i v_i^\top.
\]</span></p>
</section>
<section id="example-3rd-order-tensor-2" class="level4">
<h4 class="anchored" data-anchor-id="example-3rd-order-tensor-2">Example (3rd-Order Tensor)</h4>
<p>For <span class="math inline">\(T \in \mathbb{R}^{I \times J \times K}\)</span>, the CP decomposition is:</p>
<p><span class="math display">\[
T_{ijk} = \sum_{r=1}^R a_{ir} b_{jr} c_{kr},
\]</span></p>
<p>where <span class="math inline">\(A = [a_{ir}], B = [b_{jr}], C = [c_{kr}]\)</span> are called factor matrices.</p>
</section>
<section id="properties-3" class="level4">
<h4 class="anchored" data-anchor-id="properties-3">Properties</h4>
<ul>
<li>CP decomposition is unique under mild conditions (contrast with matrix factorization).</li>
<li>Provides a compact representation: instead of storing <span class="math inline">\(IJK\)</span> entries, we store only factor matrices of size <span class="math inline">\((I+J+K)R\)</span>.</li>
<li>Often interpretable in applications (each component corresponds to a latent factor).</li>
</ul>
</section>
<section id="applications-2" class="level4">
<h4 class="anchored" data-anchor-id="applications-2">Applications</h4>
<ul>
<li>Psychometrics: original use of PARAFAC for analyzing survey data.</li>
<li>Chemometrics: spectral analysis of chemical mixtures.</li>
<li>Signal processing: blind source separation.</li>
<li>Machine learning: latent factor models, recommender systems, neural net compression.</li>
</ul>
</section>
<section id="why-this-matters-35" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-35">Why This Matters</h4>
<ul>
<li>CP is the most direct generalization of matrix rank factorization.</li>
<li>Unlike SVD, CP works without orthogonality constraints, making it more flexible.</li>
<li>Its uniqueness is a key reason for its wide use in data analysis.</li>
</ul>
</section>
<section id="exercises-49" class="level4">
<h4 class="anchored" data-anchor-id="exercises-49">Exercises</h4>
<ol type="1">
<li><p>Matrix Analogy: Show that the CP decomposition for a <span class="math inline">\(2 \times 2\)</span> matrix is the same as its rank decomposition.</p></li>
<li><p>3rd-Order Example: Suppose <span class="math inline">\(T_{ijk} = u_i v_j w_k\)</span>. Show that <span class="math inline">\(T\)</span> has CP rank 1.</p></li>
<li><p>Factor Matrices: Write explicitly the factor matrices <span class="math inline">\(A,B,C\)</span> for</p>
<p><span class="math display">\[
T_{ijk} = \delta_{ij}\delta_{jk}.
\]</span></p></li>
<li><p>Compression: Estimate storage cost for a tensor of size <span class="math inline">\(50 \times 40 \times 30\)</span> if represented directly vs.&nbsp;CP decomposition with rank <span class="math inline">\(r=10\)</span>.</p></li>
<li><p>Thought Experiment: Why might CP decomposition be more interpretable than Tucker decomposition in some applications?</p></li>
</ol>
</section>
</section>
<section id="tucker-and-hosvd" class="level3">
<h3 class="anchored" data-anchor-id="tucker-and-hosvd">14.2 Tucker and HOSVD</h3>
<p>The Tucker decomposition is another foundational way to factorize tensors. It generalizes the matrix SVD to higher dimensions and is closely tied to the concept of multilinear rank. When computed with orthogonal factors, it is often called the Higher-Order SVD (HOSVD).</p>
<section id="tucker-decomposition-1" class="level4">
<h4 class="anchored" data-anchor-id="tucker-decomposition-1">Tucker Decomposition</h4>
<p>For a tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, the Tucker decomposition is:</p>
<p><span class="math display">\[
T = G \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_N U^{(N)},
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(G \in \mathbb{R}^{R_1 \times R_2 \times \cdots \times R_N}\)</span> is the core tensor,</li>
<li><span class="math inline">\(U^{(n)} \in \mathbb{R}^{I_n \times R_n}\)</span> are the factor matrices,</li>
<li><span class="math inline">\((R_1, R_2, \dots, R_N)\)</span> is the multilinear rank of <span class="math inline">\(T\)</span>.</li>
</ul>
</section>
<section id="comparison-with-cp" class="level4">
<h4 class="anchored" data-anchor-id="comparison-with-cp">Comparison with CP</h4>
<ul>
<li>CP: sum of rank-one components, no core tensor.</li>
<li>Tucker: includes a core tensor that mixes components across modes.</li>
<li>CP rank: single number, often hard to compute.</li>
<li>Tucker rank: tuple <span class="math inline">\((R_1,\dots,R_N)\)</span>, easier to compute (via SVD of unfoldings).</li>
</ul>
</section>
<section id="higher-order-svd-hosvd" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-svd-hosvd">Higher-Order SVD (HOSVD)</h4>
<p>The HOSVD is a special Tucker decomposition where:</p>
<ul>
<li>Each factor matrix <span class="math inline">\(U^{(n)}\)</span> has orthonormal columns (from the SVD of the mode-<span class="math inline">\(n\)</span> unfolding).</li>
<li>The core tensor <span class="math inline">\(G\)</span> has certain orthogonality properties.</li>
</ul>
<p>HOSVD is not unique but provides a stable, interpretable decomposition.</p>
</section>
<section id="example-3rd-order-tensor-3" class="level4">
<h4 class="anchored" data-anchor-id="example-3rd-order-tensor-3">Example (3rd-Order Tensor)</h4>
<p>For <span class="math inline">\(T \in \mathbb{R}^{I \times J \times K}\)</span>:</p>
<p><span class="math display">\[
T = \sum_{p=1}^{R_1}\sum_{q=1}^{R_2}\sum_{r=1}^{R_3} g_{pqr}\, u^{(1)}_p \otimes u^{(2)}_q \otimes u^{(3)}_r.
\]</span></p>
<p>Here, the core entries <span class="math inline">\(g_{pqr}\)</span> show how basis vectors across different modes interact.</p>
</section>
<section id="applications-3" class="level4">
<h4 class="anchored" data-anchor-id="applications-3">Applications</h4>
<ul>
<li>Compression: reduce each dimension by truncating factor matrices.</li>
<li>Signal processing: spatiotemporal data analysis.</li>
<li>Machine learning: feature extraction and dimensionality reduction.</li>
<li>Neuroscience: multi-subject brain activity analysis.</li>
</ul>
</section>
<section id="why-this-matters-36" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-36">Why This Matters</h4>
<ul>
<li>Tucker/HOSVD generalize the SVD to tensors, making them intuitive for those familiar with matrices.</li>
<li>Tucker rank is easier to compute than CP rank, and truncation gives practical low-rank approximations.</li>
<li>Provides a balance between interpretability (factor matrices) and flexibility (core tensor).</li>
</ul>
</section>
<section id="exercises-50" class="level4">
<h4 class="anchored" data-anchor-id="exercises-50">Exercises</h4>
<ol type="1">
<li><p>Matrix Analogy: Show that Tucker decomposition reduces to the usual SVD when <span class="math inline">\(N=2\)</span>.</p></li>
<li><p>Mode Ranks: For <span class="math inline">\(T \in \mathbb{R}^{3 \times 4 \times 5}\)</span>, suppose its multilinear rank is <span class="math inline">\((2,3,2)\)</span>. What is the size of the core tensor?</p></li>
<li><p>Factor Matrix Construction: Explain how to construct the mode-1 factor matrix <span class="math inline">\(U^{(1)}\)</span> from the SVD of the mode-1 unfolding.</p></li>
<li><p>Compression: Estimate storage cost for a tensor of size <span class="math inline">\(30 \times 40 \times 50\)</span> with Tucker rank <span class="math inline">\((5,5,5)\)</span>. Compare to full storage.</p></li>
<li><p>Thought Experiment: Why might Tucker/HOSVD be better than CP for approximation, even if CP is more interpretable?</p></li>
</ol>
</section>
</section>
<section id="tensor-trains-tt-and-hierarchical-formats" class="level3">
<h3 class="anchored" data-anchor-id="tensor-trains-tt-and-hierarchical-formats">14.3 Tensor Trains (TT) and Hierarchical Formats</h3>
<p>When tensors become very large (high dimensions or many modes), storing and computing with them directly becomes impossible. Tensor Train (TT) decomposition and related hierarchical formats provide scalable ways to represent such tensors with drastically reduced storage.</p>
<section id="tensor-train-tt-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="tensor-train-tt-decomposition">Tensor Train (TT) Decomposition</h4>
<p>A tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span> is represented as a product of smaller 3-way tensors (called cores):</p>
<p><span class="math display">\[
T_{i_1 i_2 \cdots i_N} = G^{(1)}_{i_1} G^{(2)}_{i_2} \cdots G^{(N)}_{i_N},
\]</span></p>
<p>where:</p>
<ul>
<li>Each <span class="math inline">\(G^{(k)}_{i_k}\)</span> is an <span class="math inline">\(r_{k-1} \times r_k\)</span> matrix,</li>
<li><span class="math inline">\((r_0, r_1, \dots, r_N)\)</span> are the TT ranks, with <span class="math inline">\(r_0 = r_N = 1\)</span>.</li>
</ul>
<p>Thus, the tensor entry is computed by multiplying a chain of matrices.</p>
</section>
<section id="example-order-3-tensor" class="level4">
<h4 class="anchored" data-anchor-id="example-order-3-tensor">Example (Order-3 Tensor)</h4>
<p>For <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times I_3}\)</span>:</p>
<p><span class="math display">\[
T_{i_1 i_2 i_3} = \sum_{a=1}^{r_1}\sum_{b=1}^{r_2} G^{(1)}_{i_1, a} \, G^{(2)}_{a, i_2, b} \, G^{(3)}_{b, i_3}.
\]</span></p>
</section>
<section id="storage-cost" class="level4">
<h4 class="anchored" data-anchor-id="storage-cost">Storage Cost</h4>
<ul>
<li>Full tensor: <span class="math inline">\(I_1 I_2 \cdots I_N\)</span> entries.</li>
<li>TT decomposition: about <span class="math inline">\(\sum_{k=1}^N I_k r_{k-1} r_k\)</span>.</li>
<li>For moderate TT ranks, this is exponentially smaller.</li>
</ul>
</section>
<section id="hierarchical-formats-ht-h-tucker" class="level4">
<h4 class="anchored" data-anchor-id="hierarchical-formats-ht-h-tucker">Hierarchical Formats (HT, H-Tucker)</h4>
<ul>
<li>Hierarchical Tucker (HT) generalizes TT with a tree structure.</li>
<li>Both TT and HT exploit low-rank structure in different unfolding schemes.</li>
<li>Widely used in scientific computing, quantum physics, and deep learning.</li>
</ul>
</section>
<section id="applications-4" class="level4">
<h4 class="anchored" data-anchor-id="applications-4">Applications</h4>
<ul>
<li>Scientific computing: solving PDEs with high-dimensional discretizations.</li>
<li>Quantum physics: matrix product states (MPS) in quantum many-body systems.</li>
<li>Machine learning: compressing large models, representing structured kernels.</li>
</ul>
</section>
<section id="why-this-matters-37" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-37">Why This Matters</h4>
<ul>
<li>TT and HT make computations feasible in dimensions where naive methods fail (“curse of dimensionality”).</li>
<li>They connect tensor methods with physics (MPS) and numerical mathematics (low-rank solvers).</li>
<li>Provide scalable building blocks for high-dimensional learning and simulation.</li>
</ul>
</section>
<section id="exercises-51" class="level4">
<h4 class="anchored" data-anchor-id="exercises-51">Exercises</h4>
<ol type="1">
<li><p>Storage Comparison: Compute storage size for a tensor of size <span class="math inline">\(10 \times 10 \times 10 \times 10\)</span> vs.&nbsp;TT decomposition with TT ranks all equal to 5.</p></li>
<li><p>Chain Structure: For <span class="math inline">\(T \in \mathbb{R}^{4 \times 4 \times 4}\)</span>, sketch the TT structure (cores and ranks).</p></li>
<li><p>Rank-1 Case: Show that if all TT ranks are 1, the TT decomposition reduces to a rank-one tensor.</p></li>
<li><p>Quantum Link: Explain how TT decomposition corresponds to Matrix Product States (MPS) in physics.</p></li>
<li><p>Thought Experiment: Why does the chain-like structure of TT decomposition scale better than CP or Tucker decompositions in very high dimensions?</p></li>
</ol>
</section>
</section>
<section id="connections-to-svd-and-pca" class="level3">
<h3 class="anchored" data-anchor-id="connections-to-svd-and-pca">14.4 Connections to SVD and PCA</h3>
<p>Tensor decompositions are natural generalizations of matrix factorizations such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA). Understanding these connections helps link classical linear algebra intuition with modern multilinear methods.</p>
<section id="svd-recap-matrix-case" class="level4">
<h4 class="anchored" data-anchor-id="svd-recap-matrix-case">SVD Recap (Matrix Case)</h4>
<p>For a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>:</p>
<p><span class="math display">\[
A = U \Sigma V^\top,
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal,</li>
<li><span class="math inline">\(\Sigma\)</span> is diagonal with singular values.</li>
</ul>
<p>This factorization is unique (up to signs) and provides:</p>
<ul>
<li>Rank,</li>
<li>Best low-rank approximation,</li>
<li>Geometric interpretation (rotations and scalings).</li>
</ul>
</section>
<section id="pca-recap" class="level4">
<h4 class="anchored" data-anchor-id="pca-recap">PCA Recap</h4>
<p>PCA applies SVD to a data matrix:</p>
<ul>
<li>Rows = observations, columns = features.</li>
<li>Extracts orthogonal directions (principal components) that maximize variance.</li>
</ul>
</section>
<section id="cp-and-svd" class="level4">
<h4 class="anchored" data-anchor-id="cp-and-svd">CP and SVD</h4>
<ul>
<li>CP decomposition generalizes matrix rank factorization to higher orders.</li>
<li>A matrix is just a 2-way tensor: CP = rank decomposition = SVD without orthogonality.</li>
<li>For <span class="math inline">\(N \geq 3\)</span>, CP decomposition does not correspond to orthogonal factors, but it still reveals latent components.</li>
</ul>
</section>
<section id="tuckerhosvd-and-svd" class="level4">
<h4 class="anchored" data-anchor-id="tuckerhosvd-and-svd">Tucker/HOSVD and SVD</h4>
<ul>
<li><p>Tucker decomposition generalizes SVD to higher-order tensors.</p></li>
<li><p>HOSVD is literally the higher-order SVD:</p>
<ul>
<li>Compute SVD of each mode unfolding.</li>
<li>Factor matrices = left singular vectors.</li>
<li>Core tensor = interaction of components across modes.</li>
</ul></li>
<li><p>Truncating singular vectors gives best low-rank approximations in a multilinear sense.</p></li>
</ul>
</section>
<section id="pca-vs.-multilinear-pca" class="level4">
<h4 class="anchored" data-anchor-id="pca-vs.-multilinear-pca">PCA vs.&nbsp;Multilinear PCA</h4>
<ul>
<li>PCA = best low-rank approximation of a data matrix.</li>
<li>Tucker decomposition = multilinear PCA, reducing dimensionality along each mode simultaneously.</li>
<li>Applications: face recognition, video compression, multi-way data analysis.</li>
</ul>
</section>
<section id="why-this-matters-38" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-38">Why This Matters</h4>
<ul>
<li>CP ↔︎ matrix rank factorization.</li>
<li>Tucker/HOSVD ↔︎ SVD/PCA.</li>
<li>These links provide intuition: tensor decompositions are not arbitrary; they extend familiar tools to multi-way data.</li>
</ul>
</section>
<section id="exercises-52" class="level4">
<h4 class="anchored" data-anchor-id="exercises-52">Exercises</h4>
<ol type="1">
<li><p>SVD Analogy: Show how SVD of a <span class="math inline">\(3 \times 3\)</span> matrix can be viewed as a Tucker decomposition with a diagonal core.</p></li>
<li><p>CP vs.&nbsp;SVD: Explain why CP decomposition of a matrix reduces to its rank factorization, but not to SVD.</p></li>
<li><p>HOSVD Steps: Outline the steps of HOSVD for a 3rd-order tensor <span class="math inline">\(T \in \mathbb{R}^{4 \times 5 \times 6}\)</span>.</p></li>
<li><p>PCA Analogy: Suppose we have a video dataset stored as a tensor <span class="math inline">\((\text{frames} \times \text{height} \times \text{width})\)</span>. Explain how Tucker decomposition acts as a multilinear PCA.</p></li>
<li><p>Thought Experiment: Why is orthogonality central in SVD/PCA, but not in CP decomposition? What are the trade-offs?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-viii.-computation-and-numerical-practice" class="level1">
<h1>Part VIII. Computation and Numerical Practice</h1>
<section id="chapter-15.-working-with-tensors-in-code" class="level2">
<h2 class="anchored" data-anchor-id="chapter-15.-working-with-tensors-in-code">Chapter 15. Working with Tensors in Code</h2>
<section id="efficient-indexing-and-memory-layout" class="level3">
<h3 class="anchored" data-anchor-id="efficient-indexing-and-memory-layout">15.1 Efficient Indexing and Memory Layout</h3>
<p>Working with tensors in practice requires careful attention to how they are stored and accessed in memory. Poor indexing can make even simple operations slow. This section explains how indexing works under the hood and how to optimize memory layout for performance.</p>
<section id="linearization-of-multi-indices" class="level4">
<h4 class="anchored" data-anchor-id="linearization-of-multi-indices">Linearization of Multi-Indices</h4>
<p>A tensor <span class="math inline">\(T \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span> is stored in linear memory.</p>
<ul>
<li><p>Each entry <span class="math inline">\(T_{i_1,i_2,\dots,i_N}\)</span> is mapped to a single offset in memory.</p></li>
<li><p>Formula (row-major / C-style layout):</p>
<p><span class="math display">\[
\text{offset}(i_1,\dots,i_N) = i_1 (I_2 I_3 \cdots I_N) + i_2 (I_3 \cdots I_N) + \cdots + i_N.
\]</span></p></li>
<li><p>Formula (column-major / Fortran, MATLAB):</p>
<p><span class="math display">\[
\text{offset}(i_1,\dots,i_N) = i_1 + I_1(i_2 + I_2(i_3 + \cdots + I_{N-1} i_N)).
\]</span></p></li>
</ul>
</section>
<section id="strides" class="level4">
<h4 class="anchored" data-anchor-id="strides">Strides</h4>
<ul>
<li>A stride tells how far (in memory) you move when an index increases by 1.</li>
<li>Example: in row-major order, stride along the last index = 1.</li>
<li>Efficient access requires stepping through memory with small, contiguous strides.</li>
</ul>
</section>
<section id="cache-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="cache-efficiency">Cache Efficiency</h4>
<ul>
<li>CPUs fetch memory in blocks (cache lines).</li>
<li>Accessing elements in a contiguous block is fast.</li>
<li>Jumping across large strides leads to cache misses and slow performance.</li>
</ul>
</section>
<section id="practical-implications" class="level4">
<h4 class="anchored" data-anchor-id="practical-implications">Practical Implications</h4>
<ul>
<li>Loop ordering matters: iterate over the innermost (stride-1) dimension in the inner loop.</li>
<li>Slicing tensors may produce views with non-contiguous strides (NumPy, PyTorch). Copying may be needed for efficiency.</li>
<li>Tensor libraries often allow explicit control of memory layout (row-major vs.&nbsp;column-major).</li>
</ul>
</section>
<section id="why-this-matters-39" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-39">Why This Matters</h4>
<ul>
<li>Many tensor operations are memory-bound, not compute-bound.</li>
<li>Efficient indexing and layout can make the difference between minutes and milliseconds.</li>
<li>A good understanding of strides helps when debugging tensor code in NumPy, PyTorch, JAX, etc.</li>
</ul>
</section>
</section>
<section id="exercises-53" class="level3">
<h3 class="anchored" data-anchor-id="exercises-53">Exercises</h3>
<ol type="1">
<li><p>Offset Calculation (Row-Major): For <span class="math inline">\(T \in \mathbb{R}^{2 \times 3 \times 4}\)</span>, what is the memory offset of <span class="math inline">\(T_{1,2,3}\)</span> (0-based indexing)?</p></li>
<li><p>Offset Calculation (Column-Major): For the same tensor, compute the offset of <span class="math inline">\(T_{1,2,3}\)</span> in column-major order.</p></li>
<li><p>Stride Check: In row-major order for <span class="math inline">\(T \in \mathbb{R}^{3 \times 4 \times 5}\)</span>, what are the strides for each dimension?</p></li>
<li><p>Cache-Efficient Loop: Write pseudocode for iterating over all entries of a 3D tensor in row-major order.</p></li>
<li><p>Thought Experiment: Why might a deep learning library internally reorder tensor layouts depending on the hardware (CPU vs.&nbsp;GPU)?</p></li>
</ol>
</section>
<section id="blas-einsum-performance-patterns" class="level3">
<h3 class="anchored" data-anchor-id="blas-einsum-performance-patterns">15.2 BLAS, Einsum, Performance Patterns</h3>
<p>After understanding memory layout, the next step in efficient tensor computation is using optimized libraries and abstractions. This section covers BLAS, the einsum notation, and common performance patterns that make tensor operations practical at scale.</p>
<section id="blas-basic-linear-algebra-subprograms" class="level4">
<h4 class="anchored" data-anchor-id="blas-basic-linear-algebra-subprograms">BLAS: Basic Linear Algebra Subprograms</h4>
<ul>
<li><p>BLAS is the standard library for high-performance vector and matrix operations.</p></li>
<li><p>Many tensor operations reduce to BLAS calls:</p>
<ul>
<li>Matrix multiplication (GEMM) is the backbone of most tensor contractions.</li>
<li>Level-1 BLAS: vector ops (<span class="math inline">\(y \leftarrow ax + y\)</span>).</li>
<li>Level-2 BLAS: matrix-vector ops.</li>
<li>Level-3 BLAS: matrix-matrix ops (highest efficiency).</li>
</ul></li>
</ul>
<p>Why this matters: Efficient tensor libraries (NumPy, PyTorch, TensorFlow, JAX) rely heavily on BLAS under the hood.</p>
</section>
<section id="einsum-notation" class="level4">
<h4 class="anchored" data-anchor-id="einsum-notation">Einsum Notation</h4>
<p>The Einstein summation convention (“einsum”) expresses tensor contractions concisely.</p>
<p>Example:</p>
<ul>
<li><p>Matrix multiplication:</p>
<p><span class="math display">\[
C_{ij} = \sum_k A_{ik} B_{kj}
\]</span></p>
<p>In einsum: <code>einsum('ik,kj-&gt;ij', A, B)</code>.</p></li>
<li><p>Inner product:</p>
<p><span class="math display">\[
\langle u,v \rangle = \sum_i u_i v_i
\]</span></p>
<p>In einsum: <code>einsum('i,i-&gt;', u, v)</code>.</p></li>
<li><p>Outer product:</p>
<p><span class="math display">\[
T_{ij} = u_i v_j
\]</span></p>
<p>In einsum: <code>einsum('i,j-&gt;ij', u, v)</code>.</p></li>
</ul>
<p>Advantages:</p>
<ul>
<li>Expresses contractions clearly without reshaping.</li>
<li>Often compiles to highly efficient BLAS/GPU kernels.</li>
</ul>
</section>
<section id="performance-patterns" class="level4">
<h4 class="anchored" data-anchor-id="performance-patterns">Performance Patterns</h4>
<ol type="1">
<li><p>Batching: Group operations across multiple tensors (e.g., batched GEMM in GPUs).</p></li>
<li><p>Blocking / Tiling: Break large tensors into cache-sized blocks to improve locality.</p></li>
<li><p>Fusing Operations: Combine multiple small operations into one kernel (important in GPU computing).</p></li>
<li><p>Avoiding Copies: Use views/strides instead of reshaping whenever possible.</p></li>
<li><p>Automatic Differentiation: Frameworks like PyTorch and JAX integrate einsum with backpropagation efficiently.</p></li>
</ol>
</section>
<section id="why-this-matters-40" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-40">Why This Matters</h4>
<ul>
<li>BLAS-level performance is critical for large-scale tensor applications.</li>
<li>Einsum notation unifies different tensor operations under one compact framework.</li>
<li>Recognizing performance patterns makes code scale across CPUs, GPUs, and accelerators.</li>
</ul>
</section>
<section id="exercises-54" class="level4">
<h4 class="anchored" data-anchor-id="exercises-54">Exercises</h4>
<ol type="1">
<li><p>Einsum Practice: Write the einsum expression for computing</p>
<p><span class="math display">\[
C_{ij} = \sum_{k,l} A_{ik} B_{kl} D_{lj}.
\]</span></p></li>
<li><p>Outer Product: Using einsum, compute the 3rd-order tensor <span class="math inline">\(T_{ijk} = u_i v_j w_k\)</span>.</p></li>
<li><p>BLAS Levels: Classify the following into BLAS level-1, 2, or 3:</p>
<ul>
<li>Dot product,</li>
<li>Matrix-vector product,</li>
<li>Matrix-matrix product.</li>
</ul></li>
<li><p>Batching Example: If you have 100 matrices of size <span class="math inline">\(50 \times 50\)</span>, why is a batched GEMM faster than 100 separate GEMM calls?</p></li>
<li><p>Thought Experiment: Why might einsum be more maintainable than manually reshaping and transposing arrays for contractions?</p></li>
</ol>
</section>
</section>
<section id="stability-conditioning-scaling-tricks" class="level3">
<h3 class="anchored" data-anchor-id="stability-conditioning-scaling-tricks">15.3 Stability, Conditioning, Scaling Tricks</h3>
<p>Efficient computation is not enough - tensor operations must also be numerically stable. Large-scale problems often involve ill-conditioned matrices or tensors, and small floating-point errors can accumulate dramatically. This section introduces stability concerns and practical tricks for keeping computations reliable.</p>
<section id="conditioning-and-stability" class="level4">
<h4 class="anchored" data-anchor-id="conditioning-and-stability">Conditioning and Stability</h4>
<ul>
<li><p>Condition number: For a matrix <span class="math inline">\(A\)</span>,</p>
<p><span class="math display">\[
\kappa(A) = \|A\| \cdot \|A^{-1}\|
\]</span></p>
<p>measures sensitivity of solutions to perturbations.</p></li>
<li><p>In tensors, contractions can amplify errors, especially when factors are nearly linearly dependent.</p></li>
<li><p>High tensor ranks often worsen conditioning.</p></li>
</ul>
<p>Rule of thumb: Poorly conditioned problems cannot be solved accurately, no matter the algorithm.</p>
</section>
<section id="common-stability-issues-in-tensors" class="level4">
<h4 class="anchored" data-anchor-id="common-stability-issues-in-tensors">Common Stability Issues in Tensors</h4>
<ol type="1">
<li>Overflows/underflows: multiplying many large/small entries.</li>
<li>Loss of orthogonality: iterative algorithms drift from true subspaces.</li>
<li>Cancellation errors: subtracting nearly equal numbers.</li>
<li>Exploding/vanishing gradients: in automatic differentiation with deep tensor networks.</li>
</ol>
</section>
<section id="scaling-tricks" class="level4">
<h4 class="anchored" data-anchor-id="scaling-tricks">Scaling Tricks</h4>
<ul>
<li>Normalization: Rescale vectors and factor matrices to keep entries in a safe range.</li>
<li>Orthogonalization: Regularly re-orthogonalize factor matrices in decompositions (QR or SVD steps).</li>
<li>Log-domain computations: Replace products with sums of logarithms to prevent overflow (e.g., in probabilistic models).</li>
<li>Balanced scaling: In CP decompositions, distribute scale evenly across modes to avoid extreme values.</li>
</ul>
</section>
<section id="regularization" class="level4">
<h4 class="anchored" data-anchor-id="regularization">Regularization</h4>
<ul>
<li>Add small perturbations (like <span class="math inline">\(\lambda I\)</span>) to stabilize inversions (“Tikhonov regularization”).</li>
<li>In optimization, add penalties to discourage ill-conditioned solutions.</li>
<li>Helps avoid overfitting in statistical tensor models.</li>
</ul>
</section>
<section id="why-this-matters-41" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-41">Why This Matters</h4>
<ul>
<li>Numerical stability is essential for trustworthy tensor computations.</li>
<li>Scaling and orthogonalization tricks are used in nearly every practical algorithm.</li>
<li>Without them, decompositions may diverge, optimizations may fail, and results may become meaningless.</li>
</ul>
</section>
<section id="exercises-55" class="level4">
<h4 class="anchored" data-anchor-id="exercises-55">Exercises</h4>
<ol type="1">
<li><p>Condition Number: Compute the condition number of</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 10^{-6}\end{bmatrix}.
\]</span></p>
<p>What does it tell you about stability?</p></li>
<li><p>Overflow Example: Suppose we compute the product of 100 numbers all equal to 1.01. Estimate the result. Why might floating-point overflow occur?</p></li>
<li><p>Scaling in CP: Explain why rescaling one factor by <span class="math inline">\(10^6\)</span> and another by <span class="math inline">\(10^{-6}\)</span> in a CP decomposition gives the same tensor but may cause instability.</p></li>
<li><p>Orthogonalization: Describe how QR factorization can help maintain numerical stability in HOSVD computations.</p></li>
<li><p>Thought Experiment: Why might log-domain computation be essential in probabilistic models with tensors (e.g., hidden Markov models, Bayesian networks)?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-16.-automatic-differentiation-and-gradients" class="level2">
<h2 class="anchored" data-anchor-id="chapter-16.-automatic-differentiation-and-gradients">Chapter 16. Automatic Differentiation and Gradients</h2>
<section id="jacobianshessians-as-tensors" class="level3">
<h3 class="anchored" data-anchor-id="jacobianshessians-as-tensors">16.1 Jacobians/Hessians as Tensors</h3>
<p>In calculus, derivatives of multivariable functions are naturally represented as tensors. Recognizing this viewpoint helps connect analysis with multilinear algebra, and explains why tensors appear in optimization, machine learning, and physics.</p>
<section id="jacobian-as-a-matrix-2nd-order-tensor" class="level4">
<h4 class="anchored" data-anchor-id="jacobian-as-a-matrix-2nd-order-tensor">Jacobian as a Matrix (2nd-Order Tensor)</h4>
<p>For a function <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian matrix is</p>
<p><span class="math display">\[
J_{ij} = \frac{\partial f_i}{\partial x_j}.
\]</span></p>
<ul>
<li>It describes how small changes in input <span class="math inline">\(x\)</span> produce changes in output <span class="math inline">\(f(x)\)</span>.</li>
<li>In tensor terms: <span class="math inline">\(J \in \mathbb{R}^{m \times n}\)</span>.</li>
</ul>
<p>Example:</p>
<p><span class="math display">\[
f(x,y) = (x^2, xy), \quad J = \begin{bmatrix} 2x &amp; 0 \\ y &amp; x \end{bmatrix}.
\]</span></p>
</section>
<section id="hessian-as-a-2nd-order-derivative-tensor" class="level4">
<h4 class="anchored" data-anchor-id="hessian-as-a-2nd-order-derivative-tensor">Hessian as a 2nd-Order Derivative Tensor</h4>
<p>For a scalar function <span class="math inline">\(g: \mathbb{R}^n \to \mathbb{R}\)</span>, the Hessian is</p>
<p><span class="math display">\[
H_{ij} = \frac{\partial^2 g}{\partial x_i \partial x_j}.
\]</span></p>
<ul>
<li>It is a symmetric matrix (<span class="math inline">\(H_{ij} = H_{ji}\)</span>).</li>
<li>Encodes curvature: quadratic approximation of <span class="math inline">\(g(x)\)</span>.</li>
<li>In optimization, eigenvalues of <span class="math inline">\(H\)</span> indicate convexity.</li>
</ul>
</section>
<section id="higher-order-derivatives-as-tensors" class="level4">
<h4 class="anchored" data-anchor-id="higher-order-derivatives-as-tensors">Higher-Order Derivatives as Tensors</h4>
<ul>
<li><p>Third derivatives form a 3rd-order tensor:</p>
<p><span class="math display">\[
T_{ijk} = \frac{\partial^3 g}{\partial x_i \partial x_j \partial x_k}.
\]</span></p></li>
<li><p>In general, the <span class="math inline">\(k\)</span>-th derivative of <span class="math inline">\(g\)</span> is a symmetric <span class="math inline">\(k\)</span>-tensor.</p></li>
<li><p>These appear in Taylor expansions, perturbation analysis, and physics (nonlinear elasticity, quantum chemistry).</p></li>
</ul>
</section>
<section id="automatic-differentiation-ad-perspective" class="level4">
<h4 class="anchored" data-anchor-id="automatic-differentiation-ad-perspective">Automatic Differentiation (AD) Perspective</h4>
<ul>
<li>AD frameworks (PyTorch, JAX, TensorFlow) compute Jacobians, Hessians, and higher derivatives automatically.</li>
<li>Under the hood, they construct tensors of partial derivatives and contract them efficiently.</li>
<li>Tensor viewpoint clarifies why gradients, Jacobians, and Hessians are different “levels” of the same structure.</li>
</ul>
</section>
<section id="why-this-matters-42" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-42">Why This Matters</h4>
<ul>
<li>Viewing derivatives as tensors unifies multivariable calculus and multilinear algebra.</li>
<li>Explains the role of Jacobians in transformations, Hessians in optimization, and higher derivatives in scientific modeling.</li>
<li>Essential foundation for backpropagation and deep learning.</li>
</ul>
</section>
</section>
<section id="exercises-56" class="level3">
<h3 class="anchored" data-anchor-id="exercises-56">Exercises</h3>
<ol type="1">
<li><p>Jacobian Practice: Compute the Jacobian of</p>
<p><span class="math display">\[
f(x,y,z) = (xy, yz, xz).
\]</span></p></li>
<li><p>Hessian Example: For <span class="math inline">\(g(x,y) = x^2y + y^3\)</span>, compute the Hessian matrix.</p></li>
<li><p>Symmetry Check: Show explicitly that mixed partials of <span class="math inline">\(g(x,y)\)</span> are equal: <span class="math inline">\(\frac{\partial^2 g}{\partial x \partial y} = \frac{\partial^2 g}{\partial y \partial x}\)</span>.</p></li>
<li><p>Third Derivative Tensor: Write down all nonzero entries of the 3rd derivative tensor of <span class="math inline">\(g(x) = x^4\)</span> (1D case).</p></li>
<li><p>Thought Experiment: Why is it natural that higher derivatives are symmetric tensors? What would break if they weren’t?</p></li>
</ol>
</section>
<section id="backprop-as-structured-contractions" class="level3">
<h3 class="anchored" data-anchor-id="backprop-as-structured-contractions">16.2 Backprop as Structured Contractions</h3>
<p>Backpropagation, the core algorithm behind training neural networks, is fundamentally a sequence of tensor contractions guided by the chain rule. Multilinear algebra provides a clean way to see why backprop works and why it is efficient.</p>
<section id="chain-rule-in-tensor-form" class="level4">
<h4 class="anchored" data-anchor-id="chain-rule-in-tensor-form">Chain Rule in Tensor Form</h4>
<p>For functions <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span> and <span class="math inline">\(g: \mathbb{R}^m \to \mathbb{R}^p\)</span>:</p>
<p><span class="math display">\[
J_{g \circ f}(x) = J_g(f(x)) \, J_f(x),
\]</span></p>
<p>where <span class="math inline">\(J\)</span> denotes the Jacobian.</p>
<ul>
<li>Composition of functions = multiplication (contraction) of Jacobians.</li>
<li>Backpropagation efficiently evaluates this chain without materializing huge Jacobians.</li>
</ul>
</section>
<section id="forward-vs.-reverse-mode" class="level4">
<h4 class="anchored" data-anchor-id="forward-vs.-reverse-mode">Forward vs.&nbsp;Reverse Mode</h4>
<ul>
<li>Forward mode AD: propagate derivatives forward (good when inputs are few).</li>
<li>Reverse mode AD (backprop): propagate sensitivities backward (good when outputs are few, e.g.&nbsp;scalar loss).</li>
</ul>
<p>In reverse mode:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial x} = \left( \frac{\partial y}{\partial x} \right)^\top \frac{\partial L}{\partial y}.
\]</span></p>
<p>This is a tensor contraction: contract gradient vector with a Jacobian.</p>
</section>
<section id="layer-by-layer-in-neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="layer-by-layer-in-neural-networks">Layer-by-Layer in Neural Networks</h4>
<p>Each layer is a function:</p>
<p><span class="math display">\[
h^{(l+1)} = \sigma(W^{(l)} h^{(l)} + b^{(l)}).
\]</span></p>
<p>Backprop proceeds by:</p>
<ol type="1">
<li><p>Compute forward activations.</p></li>
<li><p>For loss <span class="math inline">\(L\)</span>, compute gradient wrt output: <span class="math inline">\(\frac{\partial L}{\partial h^{(L)}}\)</span>.</p></li>
<li><p>Contract backwards through each layer:</p>
<ul>
<li>Jacobian of linear part: <span class="math inline">\(W^{(l)}\)</span>.</li>
<li>Jacobian of nonlinearity: diagonal tensor of <span class="math inline">\(\sigma'(z)\)</span>.</li>
</ul></li>
</ol>
</section>
<section id="example-two-layers" class="level4">
<h4 class="anchored" data-anchor-id="example-two-layers">Example (Two Layers)</h4>
<p>For <span class="math inline">\(L(f(x))\)</span> with <span class="math inline">\(f(x) = \sigma(Wx)\)</span>:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial x} = W^\top \big( \sigma'(Wx) \odot \frac{\partial L}{\partial f} \big).
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(\odot\)</span> = elementwise product,</li>
<li>Contraction with <span class="math inline">\(W^\top\)</span> propagates gradient backward.</li>
</ul>
</section>
<section id="tensor-viewpoint" class="level4">
<h4 class="anchored" data-anchor-id="tensor-viewpoint">Tensor Viewpoint</h4>
<ul>
<li>Jacobians are tensors.</li>
<li>Backprop avoids forming full Jacobians (which would be huge) by contracting only along needed directions.</li>
<li>Each step is a structured contraction of the gradient with the local Jacobian.</li>
</ul>
</section>
<section id="why-this-matters-43" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-43">Why This Matters</h4>
<ul>
<li>Explains efficiency: backprop runs in time proportional to the forward pass.</li>
<li>Shows the unity of AD, calculus, and multilinear algebra.</li>
<li>Clarifies why backprop generalizes beyond neural nets (any differentiable computational graph).</li>
</ul>
</section>
<section id="exercises-57" class="level4">
<h4 class="anchored" data-anchor-id="exercises-57">Exercises</h4>
<ol type="1">
<li><p>Chain Rule Contraction: Let <span class="math inline">\(f(x,y) = (x+y, xy)\)</span>, <span class="math inline">\(g(u,v) = u^2+v\)</span>. Write the backprop step explicitly using contractions.</p></li>
<li><p>Linear Layer: For <span class="math inline">\(h = Wx\)</span>, show that <span class="math inline">\(\frac{\partial L}{\partial x} = W^\top \frac{\partial L}{\partial h}\)</span>.</p></li>
<li><p>Nonlinear Layer: For <span class="math inline">\(h = \tanh(z)\)</span>, derive the contraction rule for backpropagation.</p></li>
<li><p>Efficiency Check: Estimate the cost of explicitly forming the Jacobian of a fully connected layer with <span class="math inline">\(m\)</span> outputs and <span class="math inline">\(n\)</span> inputs, versus the cost of backprop.</p></li>
<li><p>Thought Experiment: Why is reverse-mode AD (backprop) much more efficient than forward-mode AD for training neural networks?</p></li>
</ol>
</section>
</section>
<section id="practical-tips-for-pytorchjaxnumpy" class="level3">
<h3 class="anchored" data-anchor-id="practical-tips-for-pytorchjaxnumpy">16.3 Practical Tips for PyTorch/JAX/NumPy</h3>
<p>Automatic differentiation (AD) frameworks like PyTorch, JAX, and NumPy (with autograd extensions) make tensor calculus practical. But efficiency and clarity depend on how you structure code. This section gives concrete tips to avoid pitfalls and exploit the strengths of these libraries.</p>
<section id="tip-1.-use-vectorization-not-loops" class="level4">
<h4 class="anchored" data-anchor-id="tip-1.-use-vectorization-not-loops">Tip 1. Use Vectorization, Not Loops</h4>
<ul>
<li><p>Replace Python loops with tensorized operations.</p></li>
<li><p>Example (inefficient):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.zeros(n)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    y[i] <span class="op">=</span> a[i] <span class="op">-</span> b[i]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Example (efficient):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> a <span class="op">-</span> b</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="tip-2.-exploit-broadcasting" class="level4">
<h4 class="anchored" data-anchor-id="tip-2.-exploit-broadcasting">Tip 2. Exploit Broadcasting</h4>
<ul>
<li><p>Broadcasting avoids unnecessary reshaping and repetition.</p></li>
<li><p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add bias vector to each row</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> X <span class="op">+</span> b   <span class="co"># automatic broadcasting</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Broadcasting keeps memory use low and code clean.</p></li>
</ul>
</section>
<section id="tip-3.-prefer-einsum-for-complex-contractions" class="level4">
<h4 class="anchored" data-anchor-id="tip-3.-prefer-einsum-for-complex-contractions">Tip 3. Prefer <code>einsum</code> for Complex Contractions</h4>
<ul>
<li><p><code>einsum</code> is expressive and optimized.</p></li>
<li><p>Example: matrix multiplication:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.einsum(<span class="st">'ik,kj-&gt;ij'</span>, A, B)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Works the same in NumPy and JAX.</p></li>
</ul>
</section>
<section id="tip-4.-control-gradient-flow" class="level4">
<h4 class="anchored" data-anchor-id="tip-4.-control-gradient-flow">Tip 4. Control Gradient Flow</h4>
<ul>
<li>In PyTorch: <code>x.detach()</code> to stop gradients.</li>
<li>In JAX: use <code>jax.lax.stop_gradient(x)</code>.</li>
<li>Important for stabilizing training and avoiding accidental memory blowups.</li>
</ul>
</section>
<section id="tip-5.-check-shapes-with-assertions" class="level4">
<h4 class="anchored" data-anchor-id="tip-5.-check-shapes-with-assertions">Tip 5. Check Shapes with Assertions</h4>
<ul>
<li><p>Many AD errors come from shape mismatches.</p></li>
<li><p>Insert sanity checks:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> X.shape <span class="op">==</span> (batch, features)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Shape discipline avoids subtle bugs in backprop.</p></li>
</ul>
</section>
<section id="tip-6.-monitor-numerical-stability" class="level4">
<h4 class="anchored" data-anchor-id="tip-6.-monitor-numerical-stability">Tip 6. Monitor Numerical Stability</h4>
<ul>
<li>Use functions like <code>torch.nn.functional.log_softmax</code> instead of naive <code>softmax</code> to avoid overflow.</li>
<li>Add small epsilons in denominators: <code>x / (y + 1e-8)</code>.</li>
<li>Use mixed precision cautiously (FP16 vs.&nbsp;FP32).</li>
</ul>
</section>
<section id="tip-7.-benchmark-with-profilers" class="level4">
<h4 class="anchored" data-anchor-id="tip-7.-benchmark-with-profilers">Tip 7. Benchmark with Profilers</h4>
<ul>
<li>PyTorch: <code>torch.profiler</code>.</li>
<li>JAX: <code>jax.profiler.trace</code>.</li>
<li>NumPy: <code>%timeit</code> (Jupyter).</li>
<li>Helps identify bottlenecks in contractions and data movement.</li>
</ul>
</section>
<section id="why-this-matters-44" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-44">Why This Matters</h4>
<ul>
<li>Writing tensor code is easy; writing fast, stable, and scalable tensor code requires discipline.</li>
<li>Following these practices prevents performance cliffs and silent gradient bugs.</li>
<li>Bridges theory (multilinear algebra) with implementation (real training pipelines).</li>
</ul>
</section>
<section id="exercises-58" class="level4">
<h4 class="anchored" data-anchor-id="exercises-58">Exercises</h4>
<ol type="1">
<li><p>Vectorization: Rewrite a loop-based dot product in PyTorch using vectorized syntax.</p></li>
<li><p>Broadcasting: Given <span class="math inline">\(X \in \mathbb{R}^{100 \times 50}\)</span> and <span class="math inline">\(b \in \mathbb{R}^{50}\)</span>, add <span class="math inline">\(b\)</span> to each row using broadcasting.</p></li>
<li><p>Einsum Practice: Write the einsum expression for batched matrix multiplication <span class="math inline">\(Y_b = A_b B_b\)</span>, with batch dimension <span class="math inline">\(b\)</span>.</p></li>
<li><p>Gradient Stop: In PyTorch, why might we use <code>x.detach()</code> inside a training loop? Give an example.</p></li>
<li><p>Thought Experiment: Why is <code>log_softmax</code> numerically safer than <code>exp(x)/sum(exp(x))</code>?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-ix.-applications-you-can-touch" class="level1">
<h1>Part IX. Applications you can touch</h1>
<section id="chapter-17.-data-science-and-signal-processing" class="level2">
<h2 class="anchored" data-anchor-id="chapter-17.-data-science-and-signal-processing">Chapter 17. Data Science and Signal Processing</h2>
<section id="multilinear-regression" class="level3">
<h3 class="anchored" data-anchor-id="multilinear-regression">17.1 Multilinear Regression</h3>
<p>Regression is one of the most basic tools in data science: fitting a model that predicts an output from input data. When the data is naturally multi-way (tensor-structured) instead of flat vectors or matrices, multilinear regression becomes a natural extension.</p>
<section id="ordinary-regression-review" class="level4">
<h4 class="anchored" data-anchor-id="ordinary-regression-review">Ordinary Regression (Review)</h4>
<p>For data pairs <span class="math inline">\((x_i, y_i)\)</span>:</p>
<p><span class="math display">\[
y \approx Wx + b.
\]</span></p>
<p>Here, <span class="math inline">\(x \in \mathbb{R}^n\)</span>, <span class="math inline">\(y \in \mathbb{R}^m\)</span>, and <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span>.</p>
<p>This assumes vector inputs.</p>
</section>
<section id="multilinear-regression-model" class="level4">
<h4 class="anchored" data-anchor-id="multilinear-regression-model">Multilinear Regression Model</h4>
<p>Suppose input data is a tensor <span class="math inline">\(X \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>. Instead of flattening <span class="math inline">\(X\)</span> into a vector, we preserve its structure by using a multilinear map:</p>
<p><span class="math display">\[
\hat{y} = X \times_1 W^{(1)} \times_2 W^{(2)} \cdots \times_N W^{(N)} + b,
\]</span></p>
<p>where each <span class="math inline">\(W^{(n)}\)</span> acts along mode-<span class="math inline">\(n\)</span>.</p>
<ul>
<li>This reduces parameter count dramatically compared to a full vectorized regression.</li>
<li>Preserves interpretability along each mode (e.g., time, space, frequency).</li>
</ul>
</section>
<section id="example-1" class="level4">
<h4 class="anchored" data-anchor-id="example-1">Example</h4>
<ul>
<li>Input: video clip <span class="math inline">\(X \in \mathbb{R}^{\text{frames} \times \text{height} \times \text{width}}\)</span>.</li>
<li>Flattened regression would need millions of parameters.</li>
<li>Multilinear regression uses three factor matrices <span class="math inline">\(W^{(\text{frames})}, W^{(\text{height})}, W^{(\text{width})}\)</span>, drastically reducing parameters.</li>
</ul>
</section>
<section id="training" class="level4">
<h4 class="anchored" data-anchor-id="training">Training</h4>
<ul>
<li>Solve by least squares or regularized optimization.</li>
<li>Often implemented via alternating minimization (update one <span class="math inline">\(W^{(n)}\)</span> at a time).</li>
<li>Regularization (e.g., low-rank constraints) prevents overfitting.</li>
</ul>
</section>
<section id="applications-5" class="level4">
<h4 class="anchored" data-anchor-id="applications-5">Applications</h4>
<ul>
<li>Neuroscience: predict brain activity from multi-way stimulus data.</li>
<li>Chemometrics: regression on spectral cubes.</li>
<li>Time-series analysis: structured prediction across modes (time, channels, features).</li>
</ul>
</section>
<section id="why-this-matters-45" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-45">Why This Matters</h4>
<ul>
<li>Exploits multi-way structure instead of destroying it by flattening.</li>
<li>Reduces model complexity while keeping interpretability.</li>
<li>Lays the groundwork for tensor methods in supervised learning.</li>
</ul>
</section>
<section id="exercises-59" class="level4">
<h4 class="anchored" data-anchor-id="exercises-59">Exercises</h4>
<ol type="1">
<li><p>Flattened vs.&nbsp;Multilinear: For input <span class="math inline">\(X \in \mathbb{R}^{10 \times 20}\)</span> and output scalar, how many parameters does flattened regression need? How many parameters if we use multilinear regression with <span class="math inline">\(W^{(1)} \in \mathbb{R}^{10 \times 3}, W^{(2)} \in \mathbb{R}^{20 \times 3}\)</span>?</p></li>
<li><p>Mode Multiplication: Write explicitly how <span class="math inline">\(X \times_1 W^{(1)} \times_2 W^{(2)}\)</span> works for a 2D input (matrix).</p></li>
<li><p>Interpretability: Explain why multilinear regression can separate effects of time and space in spatiotemporal data.</p></li>
<li><p>Optimization: Why is alternating minimization a natural algorithm for training multilinear regression models?</p></li>
<li><p>Thought Experiment: In what situations would flattening be acceptable, and when is multilinear regression clearly superior?</p></li>
</ol>
</section>
</section>
<section id="spatiotemporal-data-and-video-tensors" class="level3">
<h3 class="anchored" data-anchor-id="spatiotemporal-data-and-video-tensors">17.2 Spatiotemporal Data and Video Tensors</h3>
<p>Many real-world datasets are not flat vectors or simple matrices but inherently multi-way arrays. A prime example is spatiotemporal data - measurements varying across both space and time. Video is a natural case: each frame is a 2D image, and the sequence of frames adds a temporal dimension, giving a 3rd-order tensor.</p>
<section id="video-as-a-tensor" class="level4">
<h4 class="anchored" data-anchor-id="video-as-a-tensor">Video as a Tensor</h4>
<p>A grayscale video with <span class="math inline">\(F\)</span> frames, height <span class="math inline">\(H\)</span>, and width <span class="math inline">\(W\)</span> is naturally represented as:</p>
<p><span class="math display">\[
X \in \mathbb{R}^{F \times H \times W}.
\]</span></p>
<p>For color video, an additional channel dimension is added:</p>
<p><span class="math display">\[
X \in \mathbb{R}^{F \times H \times W \times 3}.
\]</span></p>
<p>Flattening this into a matrix or vector loses structure and explodes parameter count.</p>
</section>
<section id="tensor-decomposition-for-spatiotemporal-data" class="level4">
<h4 class="anchored" data-anchor-id="tensor-decomposition-for-spatiotemporal-data">Tensor Decomposition for Spatiotemporal Data</h4>
<ol type="1">
<li><p>Tucker decomposition:</p>
<ul>
<li>Separates time, spatial rows, and spatial columns into low-rank factors.</li>
<li>Compresses video efficiently while preserving essential dynamics.</li>
</ul></li>
<li><p>CP decomposition:</p>
<ul>
<li>Represents data as a sum of rank-one spatiotemporal components.</li>
<li>Each component factors into (time profile) × (spatial pattern).</li>
</ul></li>
<li><p>Tensor Train (TT):</p>
<ul>
<li>Handles very long video sequences by chaining local factors.</li>
</ul></li>
</ol>
</section>
<section id="applications-6" class="level4">
<h4 class="anchored" data-anchor-id="applications-6">Applications</h4>
<ul>
<li>Compression: reduce storage while keeping perceptual quality.</li>
<li>Background modeling: separate foreground objects from static background (via low-rank + sparse decomposition).</li>
<li>Forecasting: use multilinear regression on decomposed factors to predict future frames.</li>
<li>Pattern discovery: extract temporal modes (e.g., daily cycles) and spatial modes (e.g., recurring structures).</li>
</ul>
</section>
<section id="beyond-video-general-spatiotemporal-data" class="level4">
<h4 class="anchored" data-anchor-id="beyond-video-general-spatiotemporal-data">Beyond Video: General Spatiotemporal Data</h4>
<ul>
<li>Climate data: temperature, humidity, pressure (time × latitude × longitude × altitude).</li>
<li>Neuroscience: brain activity measured over time across sensors.</li>
<li>Traffic flows: time × location × type of vehicle.</li>
</ul>
<p>All benefit from multilinear analysis.</p>
</section>
<section id="why-this-matters-46" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-46">Why This Matters</h4>
<ul>
<li>Treating spatiotemporal data as tensors respects its inherent multi-way structure.</li>
<li>Leads to compact models, better interpretability, and efficient computation.</li>
<li>Bridges data science, machine learning, and physics-based modeling.</li>
</ul>
</section>
<section id="exercises-60" class="level4">
<h4 class="anchored" data-anchor-id="exercises-60">Exercises</h4>
<ol type="1">
<li><p>Video Dimensions: A color video with 100 frames of size <span class="math inline">\(64 \times 64\)</span>. What is its tensor shape?</p></li>
<li><p>Compression: Estimate the storage size (in entries) of this video vs.&nbsp;a Tucker decomposition with ranks <span class="math inline">\((10, 10, 10, 3)\)</span>.</p></li>
<li><p>Foreground/Background: Explain how a low-rank + sparse model might separate background (low-rank) from moving objects (sparse).</p></li>
<li><p>Temporal Modes: If CP decomposition yields components of the form (time × space), how would you interpret a component with strong daily periodicity in the time factor?</p></li>
<li><p>Thought Experiment: Why might tensor decompositions uncover hidden structure in spatiotemporal data that PCA on flattened vectors would miss?</p></li>
</ol>
</section>
</section>
<section id="blind-source-separation" class="level3">
<h3 class="anchored" data-anchor-id="blind-source-separation">17.3 Blind Source Separation</h3>
<p>Blind Source Separation (BSS) is the problem of extracting hidden signals (sources) from observed mixtures, without detailed knowledge of how they were mixed. Tensors provide powerful tools for solving BSS, often outperforming classical matrix-based methods.</p>
<section id="the-mixing-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-mixing-problem">The Mixing Problem</h4>
<p>Suppose we observe signals <span class="math inline">\(x(t) \in \mathbb{R}^m\)</span> that are mixtures of <span class="math inline">\(n\)</span> hidden sources <span class="math inline">\(s(t) \in \mathbb{R}^n\)</span>:</p>
<p><span class="math display">\[
x(t) = A s(t),
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is an unknown mixing matrix.</p>
<p>Goal: Recover <span class="math inline">\(s(t)\)</span> and <span class="math inline">\(A\)</span> from only the observations <span class="math inline">\(x(t)\)</span>.</p>
</section>
<section id="classical-approach-ica-independent-component-analysis" class="level4">
<h4 class="anchored" data-anchor-id="classical-approach-ica-independent-component-analysis">Classical Approach: ICA (Independent Component Analysis)</h4>
<ul>
<li>Assumes sources are statistically independent.</li>
<li>Uses second- and higher-order statistics to separate signals.</li>
<li>Works well for simple mixtures, but struggles with multi-way structure.</li>
</ul>
</section>
<section id="tensor-approach-to-bss" class="level4">
<h4 class="anchored" data-anchor-id="tensor-approach-to-bss">Tensor Approach to BSS</h4>
<p>Moments and cumulants of observed signals are naturally represented as symmetric tensors:</p>
<ul>
<li>2nd-order cumulant (covariance): matrix.</li>
<li>4th-order cumulant: 4th-order tensor.</li>
</ul>
<p>By analyzing these higher-order tensors:</p>
<ul>
<li>Sources can be separated even when covariance is insufficient.</li>
<li>CP decomposition of the cumulant tensor reveals source directions.</li>
</ul>
</section>
<section id="example-cocktail-party-problem" class="level4">
<h4 class="anchored" data-anchor-id="example-cocktail-party-problem">Example: Cocktail Party Problem</h4>
<ul>
<li>Microphones record overlapping voices in a room.</li>
<li>Covariance matrix cannot separate voices if they overlap in energy.</li>
<li>4th-order cumulant tensor factorization recovers independent voices.</li>
</ul>
</section>
<section id="applications-7" class="level4">
<h4 class="anchored" data-anchor-id="applications-7">Applications</h4>
<ul>
<li>Audio processing: separating voices, music, or environmental sounds.</li>
<li>Medical imaging: separating independent brain activity sources from EEG/fMRI data.</li>
<li>Telecommunications: extracting signals from mixed channels.</li>
<li>Finance: identifying independent factors driving market time series.</li>
</ul>
</section>
<section id="why-this-matters-47" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-47">Why This Matters</h4>
<ul>
<li>Tensor methods exploit multi-way statistical structure, not just pairwise correlations.</li>
<li>CP decomposition guarantees identifiability in cases where matrix factorizations fail.</li>
<li>This makes BSS one of the most successful real-world applications of multilinear algebra.</li>
</ul>
</section>
<section id="exercises-61" class="level4">
<h4 class="anchored" data-anchor-id="exercises-61">Exercises</h4>
<ol type="1">
<li><p>Covariance Limitation: Why might two voices with similar pitch have indistinguishable covariance, but separable 4th-order statistics?</p></li>
<li><p>Tensor Rank: Explain why the CP rank of the 4th-order cumulant tensor corresponds to the number of independent sources.</p></li>
<li><p>Practical Example: Given three observed mixtures of two signals, sketch how CP decomposition could be used to separate them.</p></li>
<li><p>ICA vs.&nbsp;Tensor: Compare ICA (matrix-based) and tensor-based approaches for BSS. Which one uses more information about the data?</p></li>
<li><p>Thought Experiment: Why might tensors be especially effective for BSS when the number of sensors is close to the number of sources?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-18.-machine-learning-and-deep-models" class="level2">
<h2 class="anchored" data-anchor-id="chapter-18.-machine-learning-and-deep-models">Chapter 18. Machine Learning and Deep Models</h2>
<section id="convolutions-as-multilinear-maps" class="level3">
<h3 class="anchored" data-anchor-id="convolutions-as-multilinear-maps">18.1 Convolutions as Multilinear Maps</h3>
<p>Convolutions, a cornerstone of modern deep learning, are fundamentally multilinear operations. While often introduced algorithmically (sliding filters over data), they can be expressed neatly within the tensor algebra framework.</p>
<section id="convolution-as-a-tensor-contraction" class="level4">
<h4 class="anchored" data-anchor-id="convolution-as-a-tensor-contraction">Convolution as a Tensor Contraction</h4>
<p>Consider a 1D convolution of an input signal <span class="math inline">\(x \in \mathbb{R}^n\)</span> with a kernel <span class="math inline">\(h \in \mathbb{R}^k\)</span>:</p>
<p><span class="math display">\[
y_i = \sum_{j=1}^k h_j \, x_{i-j}.
\]</span></p>
<p>This is a bilinear map: linear in both the input <span class="math inline">\(x\)</span> and kernel <span class="math inline">\(h\)</span>.</p>
<ul>
<li>In higher dimensions (2D, 3D), the same structure holds: convolution = tensor contraction between input data and kernel.</li>
</ul>
</section>
<section id="convolution-as-a-multilinear-operator" class="level4">
<h4 class="anchored" data-anchor-id="convolution-as-a-multilinear-operator">Convolution as a Multilinear Operator</h4>
<p>For 2D convolution (images):</p>
<ul>
<li><p>Input: <span class="math inline">\(X \in \mathbb{R}^{H \times W \times C}\)</span> (height × width × channels).</p></li>
<li><p>Kernel: <span class="math inline">\(K \in \mathbb{R}^{r \times s \times C \times M}\)</span> (filter height × filter width × channels × output channels).</p></li>
<li><p>Output:</p>
<p><span class="math display">\[
Y_{i,j,m} = \sum_{p,q,c} K_{p,q,c,m} \, X_{i+p, j+q, c}.
\]</span></p></li>
</ul>
<p>This is a 4-way contraction across indices <span class="math inline">\(p,q,c\)</span>.</p>
</section>
<section id="tensor-perspective-benefits" class="level4">
<h4 class="anchored" data-anchor-id="tensor-perspective-benefits">Tensor Perspective Benefits</h4>
<ol type="1">
<li>Unification: Convolution is just a structured multilinear map.</li>
<li>Efficiency: Frameworks optimize convolution via tensor contractions and reshaping into matrix multiplications (im2col trick).</li>
<li>Generalization: Other operations (cross-correlation, attention) are tensor contractions of similar form.</li>
</ol>
</section>
<section id="connection-to-low-rank-tensors" class="level4">
<h4 class="anchored" data-anchor-id="connection-to-low-rank-tensors">Connection to Low-Rank Tensors</h4>
<ul>
<li>Convolution kernels can be approximated by low-rank tensor decompositions (e.g., CP, Tucker).</li>
<li>This reduces parameters and speeds up training in deep neural networks.</li>
<li>Example: a 3D convolution kernel decomposed into separable 1D kernels.</li>
</ul>
</section>
<section id="applications-beyond-deep-nets" class="level4">
<h4 class="anchored" data-anchor-id="applications-beyond-deep-nets">Applications Beyond Deep Nets</h4>
<ul>
<li>Signal processing: filtering, denoising, feature extraction.</li>
<li>Physics: differential operators (Laplacian, wave equation) as convolutions.</li>
<li>Graphics: image blurring, sharpening, edge detection.</li>
</ul>
</section>
<section id="why-this-matters-48" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-48">Why This Matters</h4>
<ul>
<li>Shows that convolutions are not “magic,” but structured tensor contractions.</li>
<li>Provides a natural bridge between deep learning and multilinear algebra.</li>
<li>Explains why tensor decompositions are effective for convolutional networks.</li>
</ul>
</section>
<section id="exercises-62" class="level4">
<h4 class="anchored" data-anchor-id="exercises-62">Exercises</h4>
<ol type="1">
<li><p>1D Convolution: Write the convolution of <span class="math inline">\(x = (1,2,3,4)\)</span> with <span class="math inline">\(h = (1,-1)\)</span> explicitly.</p></li>
<li><p>Tensor Formulation: For a grayscale image <span class="math inline">\(X \in \mathbb{R}^{H \times W}\)</span> and filter <span class="math inline">\(K \in \mathbb{R}^{r \times s}\)</span>, express convolution as a tensor contraction.</p></li>
<li><p>Kernel Decomposition: Show how a separable 2D kernel (rank-1 matrix) can be written as outer product of two 1D kernels.</p></li>
<li><p>Low-Rank Compression: Estimate parameter savings if a <span class="math inline">\(7 \times 7 \times 64 \times 128\)</span> kernel is approximated by separable <span class="math inline">\(7 \times 1\)</span> and <span class="math inline">\(1 \times 7\)</span> filters.</p></li>
<li><p>Thought Experiment: Why might expressing convolutions as multilinear maps help in designing more efficient deep learning architectures?</p></li>
</ol>
</section>
</section>
<section id="low-rank-tensor-compression-of-nets" class="level3">
<h3 class="anchored" data-anchor-id="low-rank-tensor-compression-of-nets">18.2 Low-Rank Tensor Compression of Nets</h3>
<p>Modern neural networks, especially convolutional and transformer-based models, contain millions (or even billions) of parameters. Many of these parameters are highly redundant. Low-rank tensor decompositions provide a principled way to compress networks without losing much accuracy.</p>
<section id="redundancy-in-neural-nets" class="level4">
<h4 class="anchored" data-anchor-id="redundancy-in-neural-nets">Redundancy in Neural Nets</h4>
<ul>
<li>Convolution kernels: <span class="math inline">\(K \in \mathbb{R}^{r \times s \times C_{\text{in}} \times C_{\text{out}}}\)</span>.</li>
<li>Fully connected layers: weight matrices <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span>.</li>
<li>These often have effective rank much smaller than full dimensions.</li>
</ul>
</section>
<section id="cp-decomposition-for-compression" class="level4">
<h4 class="anchored" data-anchor-id="cp-decomposition-for-compression">CP Decomposition for Compression</h4>
<p>A convolutional kernel <span class="math inline">\(K\)</span> can be approximated as:</p>
<p><span class="math display">\[
K \approx \sum_{i=1}^R a^{(1)}_i \otimes a^{(2)}_i \otimes a^{(3)}_i \otimes a^{(4)}_i,
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is small.</p>
<ul>
<li>Reduces parameters from <span class="math inline">\(r \cdot s \cdot C_{\text{in}} \cdot C_{\text{out}}\)</span> to about <span class="math inline">\(R(r+s+C_{\text{in}}+C_{\text{out}})\)</span>.</li>
</ul>
</section>
<section id="tucker-decomposition-for-compression" class="level4">
<h4 class="anchored" data-anchor-id="tucker-decomposition-for-compression">Tucker Decomposition for Compression</h4>
<p>Factorize kernel as:</p>
<p><span class="math display">\[
K \approx G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)} \times_4 U^{(4)},
\]</span></p>
<p>where <span class="math inline">\(G\)</span> is a small core tensor.</p>
<ul>
<li>Allows flexible rank choices along each mode.</li>
<li>Often used for compressing fully connected layers.</li>
</ul>
</section>
<section id="tensor-train-tt-for-compression" class="level4">
<h4 class="anchored" data-anchor-id="tensor-train-tt-for-compression">Tensor Train (TT) for Compression</h4>
<p>Large fully connected layers <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> can be reshaped into a high-order tensor and approximated in TT format.</p>
<ul>
<li>Parameters scale as <span class="math inline">\(\mathcal{O}(d r^2 n)\)</span> instead of <span class="math inline">\(\mathcal{O}(mn)\)</span>.</li>
<li>Enables deployment of large models on resource-limited devices.</li>
</ul>
</section>
<section id="applications-in-deep-learning" class="level4">
<h4 class="anchored" data-anchor-id="applications-in-deep-learning">Applications in Deep Learning</h4>
<ul>
<li>CNNs: low-rank approximations of convolution filters.</li>
<li>Transformers: compress attention matrices with tensor decomposition.</li>
<li>Mobile AI: deploy compressed models on smartphones or edge devices.</li>
</ul>
</section>
<section id="trade-offs" class="level4">
<h4 class="anchored" data-anchor-id="trade-offs">Trade-offs</h4>
<ul>
<li>Pros: fewer parameters, lower memory, faster inference.</li>
<li>Cons: extra decomposition step, potential accuracy loss if ranks are too low, need for retraining/fine-tuning.</li>
</ul>
</section>
<section id="why-this-matters-49" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-49">Why This Matters</h4>
<ul>
<li>Low-rank tensor methods make deep learning more efficient and accessible.</li>
<li>They link classical multilinear algebra directly with modern AI engineering.</li>
<li>Provide theoretical tools to understand redundancy and overparameterization.</li>
</ul>
</section>
<section id="exercises-63" class="level4">
<h4 class="anchored" data-anchor-id="exercises-63">Exercises</h4>
<ol type="1">
<li><p>Parameter Counting: Compare parameter count of a <span class="math inline">\(7 \times 7 \times 64 \times 128\)</span> convolution kernel vs.&nbsp;CP decomposition with rank <span class="math inline">\(R=20\)</span>.</p></li>
<li><p>Tucker Compression: Suppose a kernel has shape <span class="math inline">\(10 \times 10 \times 32 \times 64\)</span>. If Tucker ranks are <span class="math inline">\((5,5,10,10)\)</span>, how many parameters are needed (core + factors)?</p></li>
<li><p>TT Format: Explain how reshaping a <span class="math inline">\(1024 \times 1024\)</span> weight matrix into a 4th-order tensor enables TT compression.</p></li>
<li><p>Accuracy vs.&nbsp;Efficiency: Why might too aggressive a low-rank approximation harm model accuracy?</p></li>
<li><p>Thought Experiment: Could a neural net be trained directly in compressed tensor form, instead of compressing after training? What might be the advantages? ### 18.3 Attention as Tensor Contractions</p></li>
</ol>
<p>The attention mechanism, central to transformer models, can be seen as a sequence of structured tensor contractions. Expressing attention in multilinear algebra terms clarifies both its efficiency and its flexibility.</p>
</section>
<section id="standard-attention-formula" class="level4">
<h4 class="anchored" data-anchor-id="standard-attention-formula">Standard Attention Formula</h4>
<p>Given queries <span class="math inline">\(Q \in \mathbb{R}^{n \times d}\)</span>, keys <span class="math inline">\(K \in \mathbb{R}^{m \times d}\)</span>, and values <span class="math inline">\(V \in \mathbb{R}^{m \times d_v}\)</span>:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right) V.
\]</span></p>
<ul>
<li><span class="math inline">\(QK^\top\)</span>: similarity scores between queries and keys.</li>
<li>Softmax: normalizes across keys.</li>
<li>Multiplication with <span class="math inline">\(V\)</span>: aggregates values.</li>
</ul>
</section>
<section id="tensor-contraction-view" class="level4">
<h4 class="anchored" data-anchor-id="tensor-contraction-view">Tensor Contraction View</h4>
<ol type="1">
<li><p>Similarity computation:</p>
<p><span class="math display">\[
S_{ij} = \sum_{k} Q_{ik} K_{jk},
\]</span></p>
<p>a contraction over feature index <span class="math inline">\(k\)</span>.</p></li>
<li><p>Weighted aggregation:</p>
<p><span class="math display">\[
O_{i\ell} = \sum_{j} \text{softmax}(S_{ij}) \, V_{j\ell}.
\]</span></p></li>
</ol>
<p>Thus, attention = two contractions:</p>
<ul>
<li>Contract <span class="math inline">\(Q\)</span> with <span class="math inline">\(K\)</span> (dot product).</li>
<li>Contract softmax weights with <span class="math inline">\(V\)</span>.</li>
</ul>
</section>
<section id="multi-head-attention-as-blocked-contractions" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-attention-as-blocked-contractions">Multi-Head Attention as Blocked Contractions</h4>
<ul>
<li>Split <span class="math inline">\(Q, K, V\)</span> into <span class="math inline">\(h\)</span> heads (smaller feature dimensions).</li>
<li>Perform attention contraction in parallel for each head.</li>
<li>Concatenate results.</li>
</ul>
<p>This is equivalent to block-structured tensor contractions.</p>
</section>
<section id="low-rank-and-tensorized-variants" class="level4">
<h4 class="anchored" data-anchor-id="low-rank-and-tensorized-variants">Low-Rank and Tensorized Variants</h4>
<ul>
<li>Low-rank attention: approximate <span class="math inline">\(QK^\top\)</span> with a low-rank factorization.</li>
<li>Tensorized attention: represent weights in CP/Tucker/TT form for efficiency.</li>
<li>Linear attention: replace full contraction with kernelized approximations.</li>
</ul>
</section>
<section id="applications-and-insights" class="level4">
<h4 class="anchored" data-anchor-id="applications-and-insights">Applications and Insights</h4>
<ul>
<li>Shows attention is not “black magic” but structured multilinear algebra.</li>
<li>Explains why tensor decompositions reduce attention cost.</li>
<li>Connects attention with classical bilinear forms and projections.</li>
</ul>
</section>
<section id="why-this-matters-50" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-50">Why This Matters</h4>
<ul>
<li>Brings transformers into the same framework as convolutions and regression.</li>
<li>Provides a language for designing efficient attention mechanisms.</li>
<li>Helps bridge deep learning architectures with tensor theory.</li>
</ul>
</section>
<section id="exercises-64" class="level4">
<h4 class="anchored" data-anchor-id="exercises-64">Exercises</h4>
<ol type="1">
<li><p>Dot-Product Attention: Express <span class="math inline">\(S = QK^\top\)</span> as an einsum contraction.</p></li>
<li><p>Aggregation Step: Show how multiplying softmax-normalized scores with <span class="math inline">\(V\)</span> is another einsum contraction.</p></li>
<li><p>Multi-Head Splitting: If <span class="math inline">\(d=64\)</span> and <span class="math inline">\(h=8\)</span>, what is the per-head dimension?</p></li>
<li><p>Low-Rank Trick: If <span class="math inline">\(QK^\top\)</span> is approximated by <span class="math inline">\(Q(UV^\top)K^\top\)</span> with rank <span class="math inline">\(r\)</span>, how does this reduce complexity?</p></li>
<li><p>Thought Experiment: Why might thinking of attention as a tensor contraction help design new transformer variants?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-19.-physics-graphics-and-beyond" class="level2">
<h2 class="anchored" data-anchor-id="chapter-19.-physics-graphics-and-beyond">Chapter 19. Physics, Graphics, and Beyond</h2>
<section id="stressstrain-tensors" class="level3">
<h3 class="anchored" data-anchor-id="stressstrain-tensors">19.1 Stress/Strain Tensors</h3>
<p>In physics and engineering, stress and strain are key concepts for understanding how materials deform under forces. Both are naturally expressed as second-order tensors, making them a classic application of multilinear algebra.</p>
<section id="strain-tensor-deformation" class="level4">
<h4 class="anchored" data-anchor-id="strain-tensor-deformation">Strain Tensor (Deformation)</h4>
<p>When a material is deformed, each point moves by a displacement vector <span class="math inline">\(u(x)\)</span>.</p>
<ul>
<li>The strain tensor measures local stretching, compression, and shear.</li>
</ul>
<p><span class="math display">\[
\varepsilon_{ij} = \tfrac{1}{2} \left( \frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i} \right).
\]</span></p>
<ul>
<li>Symmetric: <span class="math inline">\(\varepsilon_{ij} = \varepsilon_{ji}\)</span>.</li>
<li>Diagonal entries: stretching along axes.</li>
<li>Off-diagonal entries: shear distortions.</li>
</ul>
</section>
<section id="stress-tensor-internal-forces" class="level4">
<h4 class="anchored" data-anchor-id="stress-tensor-internal-forces">Stress Tensor (Internal Forces)</h4>
<p>The stress tensor <span class="math inline">\(\sigma_{ij}\)</span> describes internal forces per unit area inside a material.</p>
<ul>
<li>Defined so that force on a surface with normal <span class="math inline">\(n_j\)</span> is</li>
</ul>
<p><span class="math display">\[
f_i = \sigma_{ij} n_j.
\]</span></p>
<ul>
<li>Diagonal entries: normal stresses (compression/tension).</li>
<li>Off-diagonal entries: shear stresses.</li>
</ul>
</section>
<section id="hookes-law-linear-elasticity" class="level4">
<h4 class="anchored" data-anchor-id="hookes-law-linear-elasticity">Hooke’s Law (Linear Elasticity)</h4>
<p>Stress and strain are related by a 4th-order elasticity tensor <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
\sigma_{ij} = \sum_{k,l} C_{ijkl} \, \varepsilon_{kl}.
\]</span></p>
<ul>
<li>In isotropic materials, <span class="math inline">\(C\)</span> depends only on two constants (Young’s modulus and Poisson’s ratio).</li>
<li>This is a bilinear relation between strain and stress tensors.</li>
</ul>
</section>
<section id="eigenvalues-and-principal-axes" class="level4">
<h4 class="anchored" data-anchor-id="eigenvalues-and-principal-axes">Eigenvalues and Principal Axes</h4>
<ul>
<li><p>Stress tensor <span class="math inline">\(\sigma\)</span> can be diagonalized:</p>
<ul>
<li>Eigenvalues = principal stresses.</li>
<li>Eigenvectors = principal directions.</li>
</ul></li>
<li><p>Interpretation: directions along which stress is purely compressive or tensile.</p></li>
</ul>
</section>
<section id="applications-8" class="level4">
<h4 class="anchored" data-anchor-id="applications-8">Applications</h4>
<ul>
<li>Civil engineering: bridge and building safety.</li>
<li>Mechanical engineering: design of engines, aircraft, machines.</li>
<li>Geophysics: stress in Earth’s crust, earthquakes.</li>
<li>Medical imaging: elastography for tissue stiffness.</li>
</ul>
</section>
<section id="why-this-matters-51" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-51">Why This Matters</h4>
<ul>
<li>Stress and strain are everyday tensor applications in engineering.</li>
<li>They demonstrate how multilinear algebra naturally describes geometry and physics.</li>
<li>Provide a tangible connection between abstract tensors and real-world forces.</li>
</ul>
</section>
<section id="exercises-65" class="level4">
<h4 class="anchored" data-anchor-id="exercises-65">Exercises</h4>
<ol type="1">
<li><p>Strain Calculation: For displacement field <span class="math inline">\(u(x,y) = (x+y, y)\)</span>, compute the strain tensor <span class="math inline">\(\varepsilon\)</span>.</p></li>
<li><p>Stress on a Plane: If</p>
<p><span class="math display">\[
\sigma = \begin{bmatrix} 10 &amp; 2 \\ 2 &amp; 5 \end{bmatrix}, \quad n = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
\]</span></p>
<p>compute the force vector <span class="math inline">\(f\)</span>.</p></li>
<li><p>Symmetry: Show that both stress and strain tensors are symmetric.</p></li>
<li><p>Principal Stresses: Find the eigenvalues of <span class="math inline">\(\sigma = \begin{bmatrix} 3 &amp; 1 \\ 1 &amp; 3 \end{bmatrix}\)</span>. Interpret them.</p></li>
<li><p>Thought Experiment: Why is it natural that stress and strain are tensors instead of just vectors?</p></li>
</ol>
</section>
</section>
<section id="inertia-tensors-and-principal-axes" class="level3">
<h3 class="anchored" data-anchor-id="inertia-tensors-and-principal-axes">19.2 Inertia Tensors and Principal Axes</h3>
<p>In mechanics, the moment of inertia describes how mass distribution resists rotational motion. While for simple objects it is a scalar, in general it is a second-order tensor - the inertia tensor.</p>
<section id="definition-of-inertia-tensor" class="level4">
<h4 class="anchored" data-anchor-id="definition-of-inertia-tensor">Definition of Inertia Tensor</h4>
<p>For a rigid body with mass density <span class="math inline">\(\rho(\mathbf{r})\)</span>, the inertia tensor is:</p>
<p><span class="math display">\[
I_{ij} = \int \left( \|\mathbf{r}\|^2 \delta_{ij} - r_i r_j \right) \rho(\mathbf{r}) \, dV,
\]</span></p>
<p>where <span class="math inline">\(\mathbf{r} = (x,y,z)\)</span> is the position vector relative to the chosen origin.</p>
<ul>
<li><span class="math inline">\(I_{ij}\)</span> encodes how difficult it is to rotate the body around axis <span class="math inline">\(i\)</span>.</li>
<li>Symmetric: <span class="math inline">\(I_{ij} = I_{ji}\)</span>.</li>
</ul>
</section>
<section id="angular-momentum-and-kinetic-energy" class="level4">
<h4 class="anchored" data-anchor-id="angular-momentum-and-kinetic-energy">Angular Momentum and Kinetic Energy</h4>
<p>For angular velocity <span class="math inline">\(\omega\)</span>:</p>
<ul>
<li><p>Angular momentum:</p>
<p><span class="math display">\[
\mathbf{L} = I \, \boldsymbol{\omega}.
\]</span></p></li>
<li><p>Rotational kinetic energy:</p>
<p><span class="math display">\[
T = \tfrac{1}{2} \boldsymbol{\omega}^\top I \boldsymbol{\omega}.
\]</span></p></li>
</ul>
<p>Thus, <span class="math inline">\(I\)</span> acts as the matrix linking angular velocity to angular momentum.</p>
</section>
<section id="principal-axes" class="level4">
<h4 class="anchored" data-anchor-id="principal-axes">Principal Axes</h4>
<ul>
<li><p>Inertia tensor can be diagonalized:</p>
<p><span class="math display">\[
I = P \Lambda P^\top,
\]</span></p>
<p>where <span class="math inline">\(\Lambda\)</span> contains principal moments of inertia, and <span class="math inline">\(P\)</span> gives principal axes.</p></li>
<li><p>Rotations about principal axes are “decoupled” and simpler to analyze.</p></li>
</ul>
</section>
<section id="examples-1" class="level4">
<h4 class="anchored" data-anchor-id="examples-1">Examples</h4>
<ol type="1">
<li><p>Solid sphere (mass <span class="math inline">\(M\)</span>, radius <span class="math inline">\(R\)</span>):</p>
<p><span class="math display">\[
I = \tfrac{2}{5} M R^2 I_3.
\]</span></p>
<p>(isotropic: same inertia around all axes).</p></li>
<li><p>Thin rod (length <span class="math inline">\(L\)</span>, axis through center):</p>
<p><span class="math display">\[
I = \tfrac{1}{12} M L^2.
\]</span></p></li>
<li><p>Rectangular box: inertia tensor has different diagonal entries depending on edge lengths.</p></li>
</ol>
</section>
<section id="applications-9" class="level4">
<h4 class="anchored" data-anchor-id="applications-9">Applications</h4>
<ul>
<li>Mechanical engineering: robotics, aerospace, vehicle dynamics.</li>
<li>Astronomy: rotation of planets, stability of satellites.</li>
<li>Computer graphics: simulating rigid-body dynamics in physics engines.</li>
</ul>
</section>
<section id="why-this-matters-52" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-52">Why This Matters</h4>
<ul>
<li>Inertia tensors are a clear example of a physical system governed by symmetric tensors.</li>
<li>Principal axes give both mathematical elegance and practical insight (e.g., why objects tumble).</li>
<li>Connects linear algebra (eigenvalues) directly with physical motion.</li>
</ul>
</section>
<section id="exercises-66" class="level4">
<h4 class="anchored" data-anchor-id="exercises-66">Exercises</h4>
<ol type="1">
<li><p>Rod Example: Compute the inertia tensor of a thin rod of length <span class="math inline">\(L\)</span> and mass <span class="math inline">\(M\)</span> lying along the <span class="math inline">\(x\)</span>-axis.</p></li>
<li><p>Sphere Symmetry: Show that a solid sphere’s inertia tensor is isotropic (same in all directions).</p></li>
<li><p>Principal Axes: Diagonalize</p>
<p><span class="math display">\[
I = \begin{bmatrix} 5 &amp; 1 &amp; 0 \\ 1 &amp; 4 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{bmatrix}.
\]</span></p>
<p>Interpret the eigenvalues.</p></li>
<li><p>Angular Momentum: For <span class="math inline">\(\omega = (1,0,0)\)</span> and <span class="math inline">\(I = \mathrm{diag}(2,3,4)\)</span>, compute <span class="math inline">\(\mathbf{L}\)</span>.</p></li>
<li><p>Thought Experiment: Why are principal axes of inertia so useful in spacecraft design?</p></li>
</ol>
</section>
</section>
<section id="d-graphics-transforms-and-shading" class="level3">
<h3 class="anchored" data-anchor-id="d-graphics-transforms-and-shading">19.3 3D Graphics: Transforms and Shading</h3>
<p>Computer graphics relies heavily on linear and multilinear algebra. Behind every rendered image are tensor operations that handle transformations, lighting, and shading.</p>
<section id="homogeneous-coordinates-and-transforms" class="level4">
<h4 class="anchored" data-anchor-id="homogeneous-coordinates-and-transforms">Homogeneous Coordinates and Transforms</h4>
<ul>
<li><p>A 3D point <span class="math inline">\((x,y,z)\)</span> is represented as a 4D vector <span class="math inline">\((x,y,z,1)\)</span>.</p></li>
<li><p>Transformations are represented as <span class="math inline">\(4 \times 4\)</span> matrices:</p>
<ul>
<li>Translation, rotation, scaling, perspective projection.</li>
</ul></li>
<li><p>Composition of transformations = matrix multiplication (tensor contraction).</p></li>
</ul>
<p>Example:</p>
<p><span class="math display">\[
\begin{bmatrix}
x' \\ y' \\ z' \\ 1
\end{bmatrix}
= T R S
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}.
\]</span></p>
</section>
<section id="lighting-as-a-tensor-operation" class="level4">
<h4 class="anchored" data-anchor-id="lighting-as-a-tensor-operation">Lighting as a Tensor Operation</h4>
<p>The Phong reflection model:</p>
<p><span class="math display">\[
I = k_a I_a + k_d (\mathbf{L} \cdot \mathbf{N}) I_d + k_s (\mathbf{R} \cdot \mathbf{V})^n I_s,
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{L}\)</span> = light direction,</li>
<li><span class="math inline">\(\mathbf{N}\)</span> = surface normal,</li>
<li><span class="math inline">\(\mathbf{R}\)</span> = reflection direction,</li>
<li><span class="math inline">\(\mathbf{V}\)</span> = viewer direction.</li>
</ul>
<p>Each dot product is a tensor contraction between vectors.</p>
</section>
<section id="normals-and-transformations" class="level4">
<h4 class="anchored" data-anchor-id="normals-and-transformations">Normals and Transformations</h4>
<ul>
<li>Surface normals transform differently than points (using inverse transpose of transformation matrix).</li>
<li>Preserves correct shading under scaling/shearing.</li>
<li>Another case where raising/lowering indices (via metrics) appears in practice.</li>
</ul>
</section>
<section id="shading-as-multilinear-maps" class="level4">
<h4 class="anchored" data-anchor-id="shading-as-multilinear-maps">Shading as Multilinear Maps</h4>
<ul>
<li><p>Shading combines:</p>
<ul>
<li>Light properties (color, intensity).</li>
<li>Surface properties (material, texture).</li>
<li>Geometry (normals, tangents).</li>
</ul></li>
<li><p>The mapping from these multi-way inputs to final pixel intensity is a multilinear function.</p></li>
</ul>
</section>
<section id="applications-in-graphics-pipelines" class="level4">
<h4 class="anchored" data-anchor-id="applications-in-graphics-pipelines">Applications in Graphics Pipelines</h4>
<ul>
<li>Vertex shaders: apply transformations (matrix multiplications).</li>
<li>Fragment shaders: compute color via multilinear lighting models.</li>
<li>Physics-based rendering: more advanced tensor models (BRDFs, radiance fields).</li>
</ul>
</section>
<section id="why-this-matters-53" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-53">Why This Matters</h4>
<ul>
<li>Brings tensor ideas into a domain familiar to many learners: 3D graphics.</li>
<li>Shows that rendering engines are, at heart, optimized tensor pipelines.</li>
<li>Builds intuition that tensor algebra is not abstract - it powers everyday technology (games, movies, VR).</li>
</ul>
</section>
<section id="exercises-67" class="level4">
<h4 class="anchored" data-anchor-id="exercises-67">Exercises</h4>
<ol type="1">
<li><p>Homogeneous Transform: Write the homogeneous transformation matrix for rotating 90° about the <span class="math inline">\(z\)</span>-axis and then translating by (2,3,0).</p></li>
<li><p>Dot Product Lighting: Given <span class="math inline">\(\mathbf{L} = (0,0,1)\)</span>, <span class="math inline">\(\mathbf{N} = (0,0,1)\)</span>, compute the diffuse term in the Phong model.</p></li>
<li><p>Normal Transformation: Explain why a non-uniform scaling requires using the inverse transpose matrix to transform normals.</p></li>
<li><p>Matrix Composition: If <span class="math inline">\(M_1\)</span> is a scaling matrix and <span class="math inline">\(M_2\)</span> is a rotation, what is the composite transformation?</p></li>
<li><p>Thought Experiment: How might tensor decompositions (CP, Tucker) be used to compress lighting models or neural radiance fields (NeRFs)?</p></li>
</ol>
</section>
</section>
<section id="quantum-states-and-operators" class="level3">
<h3 class="anchored" data-anchor-id="quantum-states-and-operators">19.4 Quantum States and Operators</h3>
<p>Quantum mechanics is one of the most natural playgrounds for multilinear algebra: states, observables, and dynamics are all encoded as tensors.</p>
<section id="quantum-states-as-vectors" class="level4">
<h4 class="anchored" data-anchor-id="quantum-states-as-vectors">Quantum States as Vectors</h4>
<ul>
<li><p>A pure quantum state is a vector in a complex Hilbert space:</p>
<p><span class="math display">\[
|\psi\rangle \in \mathbb{C}^n.
\]</span></p></li>
<li><p>Example: a single qubit is a vector in <span class="math inline">\(\mathbb{C}^2\)</span>:</p>
<p><span class="math display">\[
|\psi\rangle = \alpha |0\rangle + \beta |1\rangle, \quad |\alpha|^2 + |\beta|^2 = 1.
\]</span></p></li>
</ul>
</section>
<section id="operators-as-matrices-2nd-order-tensors" class="level4">
<h4 class="anchored" data-anchor-id="operators-as-matrices-2nd-order-tensors">Operators as Matrices (2nd-Order Tensors)</h4>
<ul>
<li><p>Observables and dynamics are represented as linear operators (Hermitian or unitary matrices).</p></li>
<li><p>Measurement probabilities come from contractions:</p>
<p><span class="math display">\[
p = \langle \psi | A | \psi \rangle.
\]</span></p></li>
</ul>
</section>
<section id="composite-systems-and-tensor-products" class="level4">
<h4 class="anchored" data-anchor-id="composite-systems-and-tensor-products">Composite Systems and Tensor Products</h4>
<ul>
<li><p>Multi-particle systems live in the tensor product of state spaces.</p></li>
<li><p>Two qubits:</p>
<p><span class="math display">\[
\mathbb{C}^2 \otimes \mathbb{C}^2 = \mathbb{C}^4.
\]</span></p></li>
<li><p>Example entangled state (Bell state):</p>
<p><span class="math display">\[
|\Phi^+\rangle = \tfrac{1}{\sqrt{2}} (|00\rangle + |11\rangle).
\]</span></p></li>
</ul>
<p>Entanglement is simply non-separability in tensor terms.</p>
</section>
<section id="density-matrices-mixed-states" class="level4">
<h4 class="anchored" data-anchor-id="density-matrices-mixed-states">Density Matrices (Mixed States)</h4>
<ul>
<li><p>General states described by density operator <span class="math inline">\(\rho\)</span>, a positive semidefinite Hermitian matrix with trace 1.</p></li>
<li><p>Expectation values:</p>
<p><span class="math display">\[
\langle A \rangle = \mathrm{Tr}(\rho A).
\]</span></p></li>
</ul>
</section>
<section id="quantum-gates-as-tensor-maps" class="level4">
<h4 class="anchored" data-anchor-id="quantum-gates-as-tensor-maps">Quantum Gates as Tensor Maps</h4>
<ul>
<li>Single-qubit gates: <span class="math inline">\(2 \times 2\)</span> unitary matrices (e.g., Pauli matrices).</li>
<li>Multi-qubit gates: act via Kronecker (tensor) products.</li>
<li>Example: CNOT = a <span class="math inline">\(4 \times 4\)</span> matrix acting on two-qubit states.</li>
</ul>
</section>
<section id="applications-10" class="level4">
<h4 class="anchored" data-anchor-id="applications-10">Applications</h4>
<ul>
<li>Quantum computing: algorithms rely on tensor contractions for simulating circuits.</li>
<li>Quantum many-body physics: tensor networks (MPS, PEPS) compress exponential state spaces.</li>
<li>Chemistry: molecular states represented as high-order tensors.</li>
</ul>
</section>
<section id="why-this-matters-54" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-54">Why This Matters</h4>
<ul>
<li>Shows how tensors form the mathematical backbone of quantum mechanics.</li>
<li>Explains entanglement as a tensor phenomenon.</li>
<li>Connects multilinear algebra directly with one of the most exciting modern sciences.</li>
</ul>
</section>
<section id="exercises-68" class="level4">
<h4 class="anchored" data-anchor-id="exercises-68">Exercises</h4>
<ol type="1">
<li><p>Qubit State: Write the state vector for a qubit in superposition <span class="math inline">\(|\psi\rangle = \tfrac{1}{\sqrt{2}}(|0\rangle + |1\rangle)\)</span>.</p></li>
<li><p>Operator Expectation: For Pauli-<span class="math inline">\(Z\)</span> operator <span class="math inline">\(\sigma_z = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; -1\end{bmatrix}\)</span>, compute <span class="math inline">\(\langle \psi | \sigma_z | \psi \rangle\)</span> for <span class="math inline">\(|\psi\rangle\)</span> above.</p></li>
<li><p>Tensor Product: Compute <span class="math inline">\(|0\rangle \otimes |1\rangle\)</span> explicitly as a vector in <span class="math inline">\(\mathbb{C}^4\)</span>.</p></li>
<li><p>CNOT Gate: Write the <span class="math inline">\(4 \times 4\)</span> matrix representation of the CNOT gate.</p></li>
<li><p>Thought Experiment: Why are tensor decompositions (MPS, PEPS, TT) essential for simulating quantum many-body systems efficiently?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="part-ix.-glimpses-beyond-this-book" class="level1">
<h1>Part IX. Glimpses Beyond This Book</h1>
<section id="chapter-20.-manifolds-and-tensor-fields-preview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-20.-manifolds-and-tensor-fields-preview">Chapter 20. Manifolds and Tensor Fields (Preview)</h2>
<section id="tangent-and-cotangent-bundles" class="level3">
<h3 class="anchored" data-anchor-id="tangent-and-cotangent-bundles">20.1 Tangent and Cotangent Bundles</h3>
<p>So far, we treated tensors on vector spaces with fixed bases. In geometry and physics, tensors often live on manifolds, where each point has its own tangent space. The tangent and cotangent bundles provide the foundation for tensor fields.</p>
<section id="tangent-space-at-a-point" class="level4">
<h4 class="anchored" data-anchor-id="tangent-space-at-a-point">Tangent Space at a Point</h4>
<p>For a smooth manifold <span class="math inline">\(M\)</span> and point <span class="math inline">\(p \in M\)</span>:</p>
<ul>
<li>The tangent space <span class="math inline">\(T_p M\)</span> is the vector space of all possible velocity vectors of curves through <span class="math inline">\(p\)</span>.</li>
<li>Dimension of <span class="math inline">\(T_p M\)</span> = dimension of <span class="math inline">\(M\)</span>.</li>
<li>Example: On a sphere <span class="math inline">\(S^2\)</span>, <span class="math inline">\(T_p S^2\)</span> is the plane tangent to the sphere at <span class="math inline">\(p\)</span>.</li>
</ul>
</section>
<section id="tangent-bundle" class="level4">
<h4 class="anchored" data-anchor-id="tangent-bundle">Tangent Bundle</h4>
<p>The tangent bundle is the union of all tangent spaces:</p>
<p><span class="math display">\[
TM = \bigsqcup_{p \in M} T_p M.
\]</span></p>
<ul>
<li>A smooth manifold itself (of dimension <span class="math inline">\(2n\)</span> if <span class="math inline">\(\dim M = n\)</span>).</li>
<li>A point in <span class="math inline">\(TM\)</span> is a pair <span class="math inline">\((p, v)\)</span> with <span class="math inline">\(v \in T_p M\)</span>.</li>
</ul>
</section>
<section id="cotangent-space-and-bundle" class="level4">
<h4 class="anchored" data-anchor-id="cotangent-space-and-bundle">Cotangent Space and Bundle</h4>
<ul>
<li>The cotangent space <span class="math inline">\(T^*_p M\)</span> is the dual space of <span class="math inline">\(T_p M\)</span>: linear functionals on tangent vectors.</li>
<li>Elements are called covectors (or differential 1-forms).</li>
<li>The cotangent bundle <span class="math inline">\(T^*M = \bigsqcup_{p \in M} T^*_p M\)</span>.</li>
</ul>
</section>
<section id="local-coordinates" class="level4">
<h4 class="anchored" data-anchor-id="local-coordinates">Local Coordinates</h4>
<p>If <span class="math inline">\(M\)</span> has coordinates <span class="math inline">\((x^1, \dots, x^n)\)</span>:</p>
<ul>
<li><p>Basis of tangent space: <span class="math inline">\(\{\partial/\partial x^i\}\)</span>.</p></li>
<li><p>Basis of cotangent space: <span class="math inline">\(\{dx^i\}\)</span>.</p></li>
<li><p>Any tangent vector <span class="math inline">\(v \in T_p M\)</span>:</p>
<p><span class="math display">\[
v = \sum_i v^i \frac{\partial}{\partial x^i}.
\]</span></p></li>
<li><p>Any covector <span class="math inline">\(\omega \in T^*_p M\)</span>:</p>
<p><span class="math display">\[
\omega = \sum_i \omega_i dx^i.
\]</span></p></li>
</ul>
</section>
<section id="why-this-matters-55" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-55">Why This Matters</h4>
<ul>
<li>Tangent and cotangent bundles generalize the vector/covector distinction from linear algebra to curved spaces.</li>
<li>They form the stage on which tensor fields (next sections) live.</li>
<li>Central in physics: tangent = velocities, cotangent = momenta.</li>
</ul>
</section>
<section id="exercises-69" class="level4">
<h4 class="anchored" data-anchor-id="exercises-69">Exercises</h4>
<ol type="1">
<li><p>Tangent Space: On the circle <span class="math inline">\(S^1\)</span>, describe the tangent space at the point <span class="math inline">\((1,0)\)</span>.</p></li>
<li><p>Cotangent Basis: In <span class="math inline">\(\mathbb{R}^2\)</span> with coordinates <span class="math inline">\((x,y)\)</span>, what are the basis vectors of the tangent and cotangent spaces?</p></li>
<li><p>Pairing: For <span class="math inline">\(v = v^1 \partial/\partial x + v^2 \partial/\partial y\)</span> and <span class="math inline">\(\omega = \omega_1 dx + \omega_2 dy\)</span>, compute <span class="math inline">\(\omega(v)\)</span>.</p></li>
<li><p>Bundle Structure: Explain why the tangent bundle of a 2D manifold has dimension 4.</p></li>
<li><p>Thought Experiment: Why is momentum naturally a covector (in <span class="math inline">\(T^*_p M\)</span>) instead of a vector?</p></li>
</ol>
</section>
</section>
<section id="tensor-fields-and-coordinate-changes" class="level3">
<h3 class="anchored" data-anchor-id="tensor-fields-and-coordinate-changes">20.2 Tensor Fields and Coordinate Changes</h3>
<p>So far we have defined tangent and cotangent spaces at a single point. To do geometry and physics, we need tensors that vary smoothly across a manifold. These are called tensor fields.</p>
<section id="tensor-fields" class="level4">
<h4 class="anchored" data-anchor-id="tensor-fields">Tensor Fields</h4>
<ul>
<li><p>A tensor field of type (r,s) assigns to each point <span class="math inline">\(p \in M\)</span> a tensor</p>
<p><span class="math display">\[
T(p) \in (T_pM)^{\otimes r} \otimes (T^*_pM)^{\otimes s}.
\]</span></p></li>
<li><p>Examples:</p>
<ul>
<li>Vector field = (1,0) tensor field (assigns a tangent vector at each point).</li>
<li>Covector field (1-form): (0,1) tensor field.</li>
<li>Metric: (0,2) symmetric tensor field.</li>
</ul></li>
</ul>
</section>
<section id="coordinate-expressions" class="level4">
<h4 class="anchored" data-anchor-id="coordinate-expressions">Coordinate Expressions</h4>
<p>If <span class="math inline">\((x^1,\dots,x^n)\)</span> are local coordinates, then:</p>
<ul>
<li><p>A vector field:</p>
<p><span class="math display">\[
X = \sum_i X^i(x) \frac{\partial}{\partial x^i}.
\]</span></p></li>
<li><p>A covector field:</p>
<p><span class="math display">\[
\omega = \sum_i \omega_i(x) dx^i.
\]</span></p></li>
<li><p>A general (r,s) tensor field:</p>
<p><span class="math display">\[
T = \sum T^{i_1 \cdots i_r}{}_{j_1 \cdots j_s}(x)
\frac{\partial}{\partial x^{i_1}} \otimes \cdots \otimes \frac{\partial}{\partial x^{i_r}}
\otimes dx^{j_1} \otimes \cdots \otimes dx^{j_s}.
\]</span></p></li>
</ul>
</section>
<section id="coordinate-transformations" class="level4">
<h4 class="anchored" data-anchor-id="coordinate-transformations">Coordinate Transformations</h4>
<p>If we change coordinates from <span class="math inline">\(x^i\)</span> to <span class="math inline">\(\tilde{x}^j\)</span>:</p>
<ul>
<li><p>Basis vectors transform as</p>
<p><span class="math display">\[
\frac{\partial}{\partial \tilde{x}^j} = \sum_i \frac{\partial x^i}{\partial \tilde{x}^j} \frac{\partial}{\partial x^i}.
\]</span></p></li>
<li><p>Dual basis transforms oppositely:</p>
<p><span class="math display">\[
d\tilde{x}^j = \sum_i \frac{\partial \tilde{x}^j}{\partial x^i} dx^i.
\]</span></p></li>
<li><p>Tensor components transform with a mix of both rules (contravariant and covariant).</p></li>
</ul>
<p>Example: For a (1,1) tensor field <span class="math inline">\(A^i{}_j\)</span>:</p>
<p><span class="math display">\[
\tilde{A}^i{}_j =
\frac{\partial \tilde{x}^i}{\partial x^p}
\frac{\partial x^q}{\partial \tilde{x}^j}
A^p{}_q.
\]</span></p>
</section>
<section id="why-this-matters-56" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-56">Why This Matters</h4>
<ul>
<li>Tensor fields generalize the coordinate-free viewpoint: the object is intrinsic, components adapt to coordinates.</li>
<li>Physics laws are tensorial: their form is preserved under coordinate transformations.</li>
<li>Explains why tensors are the “language of nature” in relativity and continuum mechanics.</li>
</ul>
</section>
<section id="exercises-70" class="level4">
<h4 class="anchored" data-anchor-id="exercises-70">Exercises</h4>
<ol type="1">
<li><p>Vector Field: Write the vector field <span class="math inline">\(X = x \frac{\partial}{\partial x} + y \frac{\partial}{\partial y}\)</span> on <span class="math inline">\(\mathbb{R}^2\)</span>.</p></li>
<li><p>1-Form Field: Write the 1-form <span class="math inline">\(\omega = x \, dx + y \, dy\)</span>. Evaluate <span class="math inline">\(\omega(X)\)</span> for the vector field above.</p></li>
<li><p>Transformation Rule: Show how a vector field <span class="math inline">\(X^i\)</span> transforms under coordinate change <span class="math inline">\(x^i \mapsto \tilde{x}^j\)</span>.</p></li>
<li><p>Mixed Tensor: Verify the transformation law for a (1,1) tensor <span class="math inline">\(A^i{}_j\)</span>.</p></li>
<li><p>Thought Experiment: Why is it essential in physics that tensorial equations look the same in any coordinate system?</p></li>
</ol>
</section>
</section>
<section id="covariant-derivatives-and-curvature" class="level3">
<h3 class="anchored" data-anchor-id="covariant-derivatives-and-curvature">20.3 Covariant Derivatives and Curvature</h3>
<p>On flat spaces like <span class="math inline">\(\mathbb{R}^n\)</span>, derivatives of vector fields are straightforward. On curved manifolds, however, we cannot subtract vectors at different points directly because they belong to different tangent spaces. The covariant derivative solves this problem and leads naturally to curvature.</p>
<section id="covariant-derivative" class="level4">
<h4 class="anchored" data-anchor-id="covariant-derivative">Covariant Derivative</h4>
<ul>
<li>For a vector field <span class="math inline">\(X\)</span> and another vector field <span class="math inline">\(Y\)</span>, the covariant derivative <span class="math inline">\(\nabla_X Y\)</span> measures how <span class="math inline">\(Y\)</span> changes along <span class="math inline">\(X\)</span>.</li>
<li>Unlike the usual derivative, <span class="math inline">\(\nabla_X Y \in T_p M\)</span>, so it lives in the tangent space at the same point.</li>
</ul>
<p>In coordinates <span class="math inline">\((x^i)\)</span>:</p>
<p><span class="math display">\[
\nabla_i Y^j = \frac{\partial Y^j}{\partial x^i} + \Gamma^j_{ik} Y^k,
\]</span></p>
<p>where <span class="math inline">\(\Gamma^j_{ik}\)</span> are the Christoffel symbols of the connection.</p>
</section>
<section id="parallel-transport" class="level4">
<h4 class="anchored" data-anchor-id="parallel-transport">Parallel Transport</h4>
<ul>
<li>Parallel transport moves a vector along a curve while keeping it “as constant as possible.”</li>
<li>Depends on the connection.</li>
<li>In Euclidean space, this agrees with ordinary translation; on curved manifolds (like spheres), the result depends on the path.</li>
</ul>
</section>
<section id="curvature-tensor" class="level4">
<h4 class="anchored" data-anchor-id="curvature-tensor">Curvature Tensor</h4>
<p>The failure of parallel transport to be path-independent is measured by the Riemann curvature tensor:</p>
<p><span class="math display">\[
R^i{}_{jkl} = \partial_k \Gamma^i_{jl} - \partial_l \Gamma^i_{jk}
+ \Gamma^i_{km} \Gamma^m_{jl} - \Gamma^i_{lm} \Gamma^m_{jk}.
\]</span></p>
<ul>
<li>If <span class="math inline">\(R=0\)</span>, the manifold is flat (locally Euclidean).</li>
<li>Nonzero <span class="math inline">\(R\)</span> encodes intrinsic curvature.</li>
</ul>
</section>
<section id="ricci-tensor-and-scalar-curvature" class="level4">
<h4 class="anchored" data-anchor-id="ricci-tensor-and-scalar-curvature">Ricci Tensor and Scalar Curvature</h4>
<ul>
<li><p>Contracting indices of the Riemann tensor gives the Ricci tensor <span class="math inline">\(R_{ij}\)</span>.</p></li>
<li><p>Further contraction gives the scalar curvature <span class="math inline">\(R\)</span>.</p></li>
<li><p>Central in Einstein’s field equations of general relativity:</p>
<p><span class="math display">\[
G_{ij} = R_{ij} - \tfrac{1}{2} g_{ij} R.
\]</span></p></li>
</ul>
</section>
<section id="why-this-matters-57" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-57">Why This Matters</h4>
<ul>
<li>Covariant derivative generalizes differentiation to curved spaces.</li>
<li>Curvature is intrinsic: no embedding is needed to detect it.</li>
<li>These ideas connect multilinear algebra to geometry, relativity, and modern physics.</li>
</ul>
</section>
<section id="exercises-71" class="level4">
<h4 class="anchored" data-anchor-id="exercises-71">Exercises</h4>
<ol type="1">
<li><p>Flat Space: Show that in Euclidean coordinates, Christoffel symbols vanish and the covariant derivative reduces to the usual derivative.</p></li>
<li><p>Sphere Example: Explain why parallel transport around a closed loop on a sphere rotates a vector.</p></li>
<li><p>Riemann Tensor Symmetries: Verify that <span class="math inline">\(R^i{}_{jkl} = -R^i{}_{jlk}\)</span>.</p></li>
<li><p>Ricci Contraction: Show how to obtain <span class="math inline">\(R_{ij}\)</span> from <span class="math inline">\(R^k{}_{ikj}\)</span>.</p></li>
<li><p>Thought Experiment: Why does general relativity require curvature, while Newtonian gravity does not?</p></li>
</ol>
</section>
</section>
</section>
<section id="chapter-21.-representation-theory-and-invariants-preview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-21.-representation-theory-and-invariants-preview">Chapter 21. Representation Theory and Invariants (Preview)</h2>
<section id="group-actions-on-tensor-spaces" class="level3">
<h3 class="anchored" data-anchor-id="group-actions-on-tensor-spaces">21.1 Group Actions on Tensor Spaces</h3>
<p>Symmetry plays a central role in mathematics and physics. Groups capture symmetry, and their actions on tensor spaces reveal invariants and structure. This section introduces how groups act on tensors and why this matters.</p>
<section id="group-actions" class="level4">
<h4 class="anchored" data-anchor-id="group-actions">Group Actions</h4>
<ul>
<li><p>A group action of <span class="math inline">\(G\)</span> on a vector space <span class="math inline">\(V\)</span> is a map</p>
<p><span class="math display">\[
G \times V \to V, \quad (g,v) \mapsto g \cdot v,
\]</span></p>
<p>such that <span class="math inline">\(e \cdot v = v\)</span> (identity acts trivially) and <span class="math inline">\((gh)\cdot v = g\cdot(h\cdot v)\)</span>.</p></li>
<li><p>Example: The rotation group <span class="math inline">\(SO(3)\)</span> acts on <span class="math inline">\(\mathbb{R}^3\)</span> by matrix multiplication.</p></li>
</ul>
</section>
<section id="group-actions-on-tensor-products" class="level4">
<h4 class="anchored" data-anchor-id="group-actions-on-tensor-products">Group Actions on Tensor Products</h4>
<p>If a group <span class="math inline">\(G\)</span> acts on <span class="math inline">\(V\)</span>, then it acts naturally on tensor powers:</p>
<p><span class="math display">\[
g \cdot (v_1 \otimes v_2 \otimes \cdots \otimes v_k)
= (g \cdot v_1) \otimes (g \cdot v_2) \otimes \cdots \otimes (g \cdot v_k).
\]</span></p>
<p>Thus, tensors inherit group actions from their underlying vector spaces.</p>
</section>
<section id="examples-2" class="level4">
<h4 class="anchored" data-anchor-id="examples-2">Examples</h4>
<ol type="1">
<li><p>Rotations on Vectors: In physics, tensors transform under coordinate rotations. Stress and strain tensors are invariant laws expressed under such actions.</p></li>
<li><p>Permutation Group: Acts on tensor indices by permuting them. Leads to symmetric and antisymmetric tensors.</p></li>
<li><p>General Linear Group <span class="math inline">\(GL(n)\)</span>: Natural action on <span class="math inline">\(\mathbb{R}^n\)</span>. Extends to higher-order tensors, explaining transformation rules under basis changes.</p></li>
</ol>
</section>
<section id="why-group-actions-matter" class="level4">
<h4 class="anchored" data-anchor-id="why-group-actions-matter">Why Group Actions Matter</h4>
<ul>
<li>Provide the language for defining invariants: tensorial equations remain true under group actions.</li>
<li>In physics: laws of nature are symmetric under rotations, Lorentz transformations, gauge groups.</li>
<li>In data science: invariance to permutations, rotations, or scalings improves model generalization.</li>
</ul>
</section>
<section id="tensor-invariants" class="level4">
<h4 class="anchored" data-anchor-id="tensor-invariants">Tensor Invariants</h4>
<ul>
<li>A tensor is invariant under a group if <span class="math inline">\(g \cdot T = T\)</span> for all <span class="math inline">\(g \in G\)</span>.</li>
<li>Example: the Euclidean inner product <span class="math inline">\(\langle x,y\rangle\)</span> is invariant under <span class="math inline">\(SO(n)\)</span>.</li>
<li>Determinants, traces, and volume forms are other classical invariants.</li>
</ul>
</section>
<section id="exercises-72" class="level4">
<h4 class="anchored" data-anchor-id="exercises-72">Exercises</h4>
<ol type="1">
<li><p>Vector Rotation: Show that the Euclidean norm <span class="math inline">\(\|x\|^2 = x_1^2 + x_2^2 + x_3^2\)</span> is invariant under <span class="math inline">\(SO(3)\)</span>.</p></li>
<li><p>Permutation Action: Describe how the permutation <span class="math inline">\((12)\)</span> acts on a tensor <span class="math inline">\(T_{ijk}\)</span>.</p></li>
<li><p>GL(n) Action: For a (1,1) tensor <span class="math inline">\(A^i{}_j\)</span>, write its transformation rule under <span class="math inline">\(GL(n)\)</span>.</p></li>
<li><p>Invariant Tensor: Prove that the Kronecker delta <span class="math inline">\(\delta_{ij}\)</span> is invariant under orthogonal transformations.</p></li>
<li><p>Thought Experiment: Why is invariance under certain group actions a guiding principle in formulating physical laws?</p></li>
</ol>
</section>
</section>
<section id="invariant-tensors-and-symmetry" class="level3">
<h3 class="anchored" data-anchor-id="invariant-tensors-and-symmetry">21.2 Invariant Tensors and Symmetry</h3>
<p>Invariant tensors capture quantities that remain unchanged under group actions. They are the backbone of conservation laws in physics, canonical forms in mathematics, and inductive biases in machine learning.</p>
<section id="definition-3" class="level4">
<h4 class="anchored" data-anchor-id="definition-3">Definition</h4>
<p>A tensor <span class="math inline">\(T\)</span> is invariant under a group <span class="math inline">\(G\)</span> if</p>
<p><span class="math display">\[
g \cdot T = T \quad \text{for all } g \in G.
\]</span></p>
<ul>
<li>Example: Under the rotation group <span class="math inline">\(SO(n)\)</span>, the Kronecker delta <span class="math inline">\(\delta_{ij}\)</span> is invariant.</li>
<li>Example: The Levi-Civita symbol <span class="math inline">\(\varepsilon_{ijk}\)</span> is invariant under <span class="math inline">\(SO(3)\)</span> but changes sign under reflections.</li>
</ul>
</section>
<section id="classical-invariant-tensors" class="level4">
<h4 class="anchored" data-anchor-id="classical-invariant-tensors">Classical Invariant Tensors</h4>
<ol type="1">
<li><p>Inner Product: <span class="math inline">\(\langle x, y \rangle = \delta_{ij} x^i y^j\)</span> is invariant under orthogonal transformations.</p></li>
<li><p>Determinant: Expressed with <span class="math inline">\(\varepsilon_{i_1 i_2 \dots i_n}\)</span>, invariant under <span class="math inline">\(SL(n)\)</span> (special linear group).</p></li>
<li><p>Volume Form: Orientation-preserving transformations preserve volume.</p></li>
<li><p>Metric Tensor <span class="math inline">\(g_{ij}\)</span>: Fundamental invariant under coordinate changes, defining distances.</p></li>
</ol>
</section>
<section id="invariant-theory" class="level4">
<h4 class="anchored" data-anchor-id="invariant-theory">Invariant Theory</h4>
<ul>
<li>Studies polynomial functions or tensors that remain unchanged under group actions.</li>
<li>Example: Symmetric polynomials invariant under the permutation group <span class="math inline">\(S_n\)</span>.</li>
<li>Example: Trace and determinant are invariants under conjugation by <span class="math inline">\(GL(n)\)</span>.</li>
</ul>
</section>
<section id="physics-examples" class="level4">
<h4 class="anchored" data-anchor-id="physics-examples">Physics Examples</h4>
<ul>
<li>Conservation of energy and momentum ↔︎ invariance under time and space translations (Noether’s theorem).</li>
<li>Angular momentum ↔︎ invariance under rotations.</li>
<li>Gauge invariants define observable quantities in quantum field theory.</li>
</ul>
</section>
<section id="applications-in-data-science-ml" class="level4">
<h4 class="anchored" data-anchor-id="applications-in-data-science-ml">Applications in Data Science &amp; ML</h4>
<ul>
<li>Designing models invariant to certain transformations (e.g., convolutional nets = translation invariance).</li>
<li>Graph neural networks = invariance under node permutations.</li>
<li>Tensor methods enforce symmetries directly in architecture.</li>
</ul>
</section>
<section id="why-this-matters-58" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-58">Why This Matters</h4>
<ul>
<li>Invariants identify essential quantities independent of coordinates or representation.</li>
<li>Symmetry reduces complexity: from many possible features to a few invariant ones.</li>
<li>This unifies physics, pure math, and modern AI design.</li>
</ul>
</section>
<section id="exercises-73" class="level4">
<h4 class="anchored" data-anchor-id="exercises-73">Exercises</h4>
<ol type="1">
<li><p>Rotation Invariance: Show explicitly that <span class="math inline">\(\delta_{ij}\)</span> is unchanged under rotation matrices <span class="math inline">\(R \in SO(3)\)</span>.</p></li>
<li><p>Levi-Civita: Verify that <span class="math inline">\(\varepsilon_{ijk}\)</span> changes sign under reflection (determinant = –1).</p></li>
<li><p>Determinant: Why is the determinant invariant under <span class="math inline">\(SL(n)\)</span> but not under all of <span class="math inline">\(GL(n)\)</span>?</p></li>
<li><p>Graph Example: Explain why node permutation invariance is crucial in graph neural networks.</p></li>
<li><p>Thought Experiment: Why do invariants often correspond to conserved physical quantities?</p></li>
</ol>
</section>
</section>
<section id="why-invariants-matter-in-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="why-invariants-matter-in-algorithms">21.3 Why Invariants Matter in Algorithms</h3>
<p>Invariants are not just elegant mathematical objects - they are practical tools that make algorithms more robust, efficient, and interpretable. When an algorithm respects the right invariants, it exploits symmetry instead of fighting it.</p>
<section id="invariants-reduce-redundancy" class="level4">
<h4 class="anchored" data-anchor-id="invariants-reduce-redundancy">Invariants Reduce Redundancy</h4>
<ul>
<li>Without invariants, algorithms may waste time learning the same thing in multiple coordinate systems.</li>
<li>Example: PCA uses covariance, which is rotation-invariant, so it doesn’t matter how data is oriented.</li>
<li>Example: Graph algorithms often rely on permutation-invariant structures (degree, adjacency spectrum).</li>
</ul>
</section>
<section id="invariants-improve-robustness" class="level4">
<h4 class="anchored" data-anchor-id="invariants-improve-robustness">Invariants Improve Robustness</h4>
<ul>
<li>If an algorithm outputs the same result under symmetry transformations, results are stable and reproducible.</li>
<li>Example: CNNs are translation-invariant, making them robust to shifts in input images.</li>
<li>Example: Physics simulations rely on energy invariants for numerical stability.</li>
</ul>
</section>
<section id="invariants-simplify-computation" class="level4">
<h4 class="anchored" data-anchor-id="invariants-simplify-computation">Invariants Simplify Computation</h4>
<ul>
<li>Many invariants collapse high-dimensional data into simpler summaries.</li>
<li>Example: Determinant summarizes a matrix’s volume-scaling property.</li>
<li>Example: Tensor contractions with invariant tensors (like <span class="math inline">\(\delta_{ij}\)</span> or <span class="math inline">\(\varepsilon_{ijk}\)</span>) simplify calculations.</li>
</ul>
</section>
<section id="algorithm-design-via-invariants" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-design-via-invariants">Algorithm Design via Invariants</h4>
<ul>
<li>Signal processing: invariant features (moments, cumulants) used for blind source separation.</li>
<li>Machine learning: group-equivariant networks encode invariances (rotations, permutations).</li>
<li>Optimization: invariance-aware preconditioning improves convergence.</li>
</ul>
</section>
<section id="why-this-matters-59" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-59">Why This Matters</h4>
<ul>
<li>Invariants let us design algorithms that generalize better, by focusing only on structure that truly matters.</li>
<li>They connect deep theory (group actions, tensor algebra) with practical implementations (CNNs, GNNs, physics engines).</li>
<li>Understanding invariants is key to the next generation of geometry- and symmetry-aware AI systems.</li>
</ul>
</section>
<section id="exercises-74" class="level4">
<h4 class="anchored" data-anchor-id="exercises-74">Exercises</h4>
<ol type="1">
<li><p>Rotation-Invariant Feature: For a set of 2D points, explain why pairwise distances are invariant under rotations and translations.</p></li>
<li><p>Permutation-Invariance: Show that the sum of node features in a graph is invariant under permutations of node labels.</p></li>
<li><p>Algorithm Stability: Why does enforcing conservation of energy in a simulation improve long-term stability?</p></li>
<li><p>Invariant Contraction: Use <span class="math inline">\(\delta_{ij}\)</span> to show that contracting <span class="math inline">\(A_{ij}\delta_{ij}\)</span> yields the trace of <span class="math inline">\(A\)</span>, an invariant.</p></li>
<li><p>Thought Experiment: If a machine learning model ignores known invariances in the data, what are the risks?</p></li>
</ol>
</section>
</section>
</section>
</section>
<section id="end-matter" class="level1">
<h1>End Matter</h1>
<section id="a.-symbols-and-notation-cheatsheet" class="level2">
<h2 class="anchored" data-anchor-id="a.-symbols-and-notation-cheatsheet">A. Symbols and Notation Cheatsheet</h2>
<p>This appendix gathers the most common symbols used throughout the book. Use it as a quick reference when working through chapters and exercises.</p>
<section id="vector-spaces-and-duals" class="level3">
<h3 class="anchored" data-anchor-id="vector-spaces-and-duals">Vector Spaces and Duals</h3>
<ul>
<li><span class="math inline">\(V, W, U\)</span> - vector spaces</li>
<li><span class="math inline">\(\dim V\)</span> - dimension of <span class="math inline">\(V\)</span></li>
<li><span class="math inline">\(V^*\)</span> - dual space (space of linear functionals on <span class="math inline">\(V\)</span>)</li>
<li><span class="math inline">\(v \in V\)</span> - vector</li>
<li><span class="math inline">\(\alpha \in V^*\)</span> - covector (linear form)</li>
</ul>
</section>
<section id="tensors" class="level3">
<h3 class="anchored" data-anchor-id="tensors">Tensors</h3>
<ul>
<li><span class="math inline">\(T^r_s(V)\)</span> - space of tensors of type (r,s) over <span class="math inline">\(V\)</span></li>
<li><span class="math inline">\(u \otimes v\)</span> - tensor (outer) product of <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span></li>
<li><span class="math inline">\(T_{i_1 \dots i_s}^{j_1 \dots j_r}\)</span> - components of a (r,s) tensor</li>
<li>Contraction - summing over one upper and one lower index</li>
<li>Symmetric / antisymmetric tensors - invariant or alternating under index permutations</li>
</ul>
</section>
<section id="indices-and-summation" class="level3">
<h3 class="anchored" data-anchor-id="indices-and-summation">Indices and Summation</h3>
<ul>
<li><span class="math inline">\(i,j,k,l,m,n\)</span> - generic indices</li>
<li>Einstein summation convention - repeated upper/lower indices are summed over</li>
<li><span class="math inline">\(\delta_{ij}\)</span> - Kronecker delta, identity for index contraction</li>
<li><span class="math inline">\(\varepsilon_{ijk}\)</span> - Levi-Civita symbol (totally antisymmetric, used for cross products, determinants)</li>
</ul>
</section>
<section id="linear-maps-and-operators" class="level3">
<h3 class="anchored" data-anchor-id="linear-maps-and-operators">Linear Maps and Operators</h3>
<ul>
<li><span class="math inline">\(A: V \to W\)</span> - linear map from <span class="math inline">\(V\)</span> to <span class="math inline">\(W\)</span></li>
<li><span class="math inline">\(A^i{}_j\)</span> - components of a (1,1) tensor, i.e., a linear operator</li>
<li><span class="math inline">\(\mathrm{tr}(A)\)</span> - trace of operator <span class="math inline">\(A\)</span></li>
<li><span class="math inline">\(\det(A)\)</span> - determinant of <span class="math inline">\(A\)</span></li>
</ul>
</section>
<section id="tensor-operations" class="level3">
<h3 class="anchored" data-anchor-id="tensor-operations">Tensor Operations</h3>
<ul>
<li><span class="math inline">\(\otimes\)</span> - tensor product</li>
<li><span class="math inline">\(\wedge\)</span> - wedge product (alternating tensor product)</li>
<li><span class="math inline">\(\times_n\)</span> - mode-<span class="math inline">\(n\)</span> product of a tensor with a matrix</li>
<li><span class="math inline">\(\mathrm{vec}(\cdot)\)</span> - vectorization operator (flatten tensor into a vector)</li>
<li><span class="math inline">\(\mathrm{Tr}(\cdot)\)</span> - matrix trace</li>
<li><span class="math inline">\(\nabla\)</span> - gradient / covariant derivative</li>
</ul>
</section>
<section id="special-objects" class="level3">
<h3 class="anchored" data-anchor-id="special-objects">Special Objects</h3>
<ul>
<li><span class="math inline">\(g_{ij}\)</span> - metric tensor</li>
<li><span class="math inline">\(R^i{}_{jkl}\)</span> - Riemann curvature tensor</li>
<li><span class="math inline">\(R_{ij}\)</span> - Ricci tensor</li>
<li><span class="math inline">\(R\)</span> - scalar curvature</li>
<li><span class="math inline">\(|v|\)</span> or <span class="math inline">\(\|v\|\)</span> - norm of a vector</li>
<li><span class="math inline">\(\langle u, v \rangle\)</span> - inner product of <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span></li>
</ul>
</section>
</section>
<section id="appendix-b.-proof-sketches-of-core-theorems" class="level2">
<h2 class="anchored" data-anchor-id="appendix-b.-proof-sketches-of-core-theorems">Appendix B. Proof Sketches of Core Theorems</h2>
<section id="appendix-b.1-universal-property-of-the-tensor-product-proof-sketch" class="level3">
<h3 class="anchored" data-anchor-id="appendix-b.1-universal-property-of-the-tensor-product-proof-sketch">Appendix B.1 Universal Property of the Tensor Product (Proof Sketch)</h3>
<p>The tensor product <span class="math inline">\(V \otimes W\)</span> is not just a convenient construction: it is characterized uniquely by its universal property. This section gives an intuitive sketch of the proof.</p>
<section id="statement-of-the-universal-property" class="level4">
<h4 class="anchored" data-anchor-id="statement-of-the-universal-property">Statement of the Universal Property</h4>
<p>Given vector spaces <span class="math inline">\(V, W\)</span> over a field <span class="math inline">\(\mathbb{F}\)</span>:</p>
<ul>
<li><p>There exists a vector space <span class="math inline">\(V \otimes W\)</span> together with a bilinear map</p>
<p><span class="math display">\[
\otimes : V \times W \to V \otimes W
\]</span></p>
<p>such that:</p></li>
</ul>
<p>For any bilinear map <span class="math inline">\(f : V \times W \to U\)</span> into another vector space <span class="math inline">\(U\)</span>, there exists a unique linear map</p>
<p><span class="math display">\[
\tilde{f}: V \otimes W \to U
\]</span></p>
<p>satisfying</p>
<p><span class="math display">\[
f(v,w) = \tilde{f}(v \otimes w).
\]</span></p>
</section>
<section id="idea-of-the-proof" class="level4">
<h4 class="anchored" data-anchor-id="idea-of-the-proof">Idea of the Proof</h4>
<ol type="1">
<li><p>Build a candidate space:</p>
<ul>
<li><p>Start with the free vector space generated by pairs <span class="math inline">\((v,w)\)</span>.</p></li>
<li><p>Impose relations to enforce bilinearity:</p>
<ul>
<li><span class="math inline">\((v_1+v_2, w) \sim (v_1, w) + (v_2, w)\)</span></li>
<li><span class="math inline">\((v, w_1+w_2) \sim (v, w_1) + (v, w_2)\)</span></li>
<li><span class="math inline">\((\alpha v, w) \sim \alpha (v,w)\)</span>, <span class="math inline">\((v, \alpha w) \sim \alpha (v,w)\)</span>.</li>
</ul></li>
</ul>
<p>The quotient space is defined as <span class="math inline">\(V \otimes W\)</span>.</p></li>
<li><p>Define the canonical bilinear map:</p>
<p><span class="math display">\[
(v,w) \mapsto v \otimes w.
\]</span></p></li>
<li><p>Factorization property:</p>
<ul>
<li><p>For any bilinear map <span class="math inline">\(f: V \times W \to U\)</span>, define <span class="math inline">\(\tilde{f}\)</span> by</p>
<p><span class="math display">\[
\tilde{f}(v \otimes w) = f(v,w).
\]</span></p></li>
<li><p>This is well-defined because the relations in step 1 match the bilinear properties of <span class="math inline">\(f\)</span>.</p></li>
</ul></li>
<li><p>Uniqueness:</p>
<ul>
<li>Any linear map <span class="math inline">\(\tilde{f}\)</span> satisfying the condition must agree on generators <span class="math inline">\(v \otimes w\)</span>.</li>
<li>Since these generate the whole space, <span class="math inline">\(\tilde{f}\)</span> is unique.</li>
</ul></li>
</ol>
</section>
<section id="why-this-matters-60" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-60">Why This Matters</h4>
<ul>
<li>The tensor product is defined *not- by coordinates but by this universal property.</li>
<li>It guarantees that tensors are the natural home for bilinear (and multilinear) maps.</li>
<li>In applications, this explains why changing bases, reshaping, or contracting tensors always behaves consistently.</li>
</ul>
</section>
<section id="exercises-75" class="level4">
<h4 class="anchored" data-anchor-id="exercises-75">Exercises</h4>
<ol type="1">
<li><p>Matrix Multiplication as a Bilinear Map: Show that the bilinear map <span class="math inline">\(f: \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}\)</span>, <span class="math inline">\(f(x,y) = x^\top A y\)</span>, factors through a linear map on <span class="math inline">\(\mathbb{R}^n \otimes \mathbb{R}^m\)</span>.</p></li>
<li><p>Dimension Formula: Using a basis <span class="math inline">\(\{e_i\}\)</span> for <span class="math inline">\(V\)</span> and <span class="math inline">\(\{f_j\}\)</span> for <span class="math inline">\(W\)</span>, prove that <span class="math inline">\(\{e_i \otimes f_j\}\)</span> is a basis of <span class="math inline">\(V \otimes W\)</span>.</p></li>
<li><p>Uniqueness Check: Why can’t two different linear maps <span class="math inline">\(\tilde{f}_1, \tilde{f}_2 : V \otimes W \to U\)</span> both satisfy <span class="math inline">\(f(v,w) = \tilde{f}(v \otimes w)\)</span>?</p></li>
<li><p>Free Vector Space Analogy: Compare the construction of <span class="math inline">\(V \otimes W\)</span> to the way free groups or free vector spaces are defined.</p></li>
<li><p>Thought Experiment: If the tensor product is defined via a universal property, why does this make it “canonical” (independent of choices)?</p></li>
</ol>
</section>
</section>
<section id="appendix-b.2-dimension-formula-for-tensor-products-proof-sketch" class="level3">
<h3 class="anchored" data-anchor-id="appendix-b.2-dimension-formula-for-tensor-products-proof-sketch">Appendix B.2 Dimension Formula for Tensor Products (Proof Sketch)</h3>
<p>The tensor product has a clean relationship between dimensions:</p>
<p><span class="math display">\[
\dim(V \otimes W) = \dim(V) \cdot \dim(W).
\]</span></p>
<p>Here’s a sketch of why this is true.</p>
<section id="step-1.-choose-bases" class="level4">
<h4 class="anchored" data-anchor-id="step-1.-choose-bases">Step 1. Choose Bases</h4>
<ul>
<li>Let <span class="math inline">\(\{e_i\}_{i=1}^m\)</span> be a basis for <span class="math inline">\(V\)</span>.</li>
<li>Let <span class="math inline">\(\{f_j\}_{j=1}^n\)</span> be a basis for <span class="math inline">\(W\)</span>.</li>
</ul>
</section>
<section id="step-2.-construct-tensor-basis" class="level4">
<h4 class="anchored" data-anchor-id="step-2.-construct-tensor-basis">Step 2. Construct Tensor Basis</h4>
<ul>
<li>Consider all simple tensors <span class="math inline">\(e_i \otimes f_j\)</span>, with <span class="math inline">\(1 \leq i \leq m\)</span>, <span class="math inline">\(1 \leq j \leq n\)</span>.</li>
<li>There are exactly <span class="math inline">\(mn\)</span> such tensors.</li>
</ul>
</section>
<section id="step-3.-spanning-argument" class="level4">
<h4 class="anchored" data-anchor-id="step-3.-spanning-argument">Step 3. Spanning Argument</h4>
<ul>
<li><p>Any element of <span class="math inline">\(V \otimes W\)</span> is a linear combination of simple tensors <span class="math inline">\(v \otimes w\)</span>.</p></li>
<li><p>Expanding <span class="math inline">\(v = \sum a_i e_i\)</span>, <span class="math inline">\(w = \sum b_j f_j\)</span>, we get</p>
<p><span class="math display">\[
v \otimes w = \sum_{i,j} a_i b_j \,(e_i \otimes f_j).
\]</span></p></li>
<li><p>Therefore, all tensors are linear combinations of <span class="math inline">\(\{e_i \otimes f_j\}\)</span>.</p></li>
</ul>
</section>
<section id="step-4.-linear-independence" class="level4">
<h4 class="anchored" data-anchor-id="step-4.-linear-independence">Step 4. Linear Independence</h4>
<ul>
<li>Suppose <span class="math inline">\(\sum_{i,j} c_{ij} \, (e_i \otimes f_j) = 0\)</span>.</li>
<li>Apply the universal property with bilinear maps defined by coordinate projections.</li>
<li>One shows all coefficients <span class="math inline">\(c_{ij}\)</span> must vanish.</li>
<li>Hence, <span class="math inline">\(\{e_i \otimes f_j\}\)</span> is linearly independent.</li>
</ul>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<ul>
<li><span class="math inline">\(\{e_i \otimes f_j\}\)</span> is a basis.</li>
<li>Thus, <span class="math inline">\(\dim(V \otimes W) = mn = \dim(V) \cdot \dim(W)\)</span>.</li>
</ul>
</section>
<section id="why-this-matters-61" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-61">Why This Matters</h4>
<ul>
<li>Dimension formula ensures that tensor products don’t “create new degrees of freedom” beyond combinations of existing bases.</li>
<li>Makes tensor products predictable: if <span class="math inline">\(V = \mathbb{R}^m\)</span> and <span class="math inline">\(W = \mathbb{R}^n\)</span>, then <span class="math inline">\(V \otimes W \cong \mathbb{R}^{mn}\)</span>.</li>
<li>Explains why tensor reshaping between matrices, higher arrays, and Kronecker products is consistent.</li>
</ul>
</section>
<section id="exercises-76" class="level4">
<h4 class="anchored" data-anchor-id="exercises-76">Exercises</h4>
<ol type="1">
<li><p>Basis Construction: For <span class="math inline">\(V = \mathbb{R}^2\)</span> with basis <span class="math inline">\(\{e_1,e_2\}\)</span> and <span class="math inline">\(W = \mathbb{R}^3\)</span> with basis <span class="math inline">\(\{f_1,f_2,f_3\}\)</span>, explicitly list the basis of <span class="math inline">\(V \otimes W\)</span>.</p></li>
<li><p>Counting Dimensions: If <span class="math inline">\(\dim(V)=4\)</span> and <span class="math inline">\(\dim(W)=5\)</span>, what is <span class="math inline">\(\dim(V \otimes W)\)</span>?</p></li>
<li><p>Matrix Analogy: Show that <span class="math inline">\(V \otimes W\)</span> with chosen bases is isomorphic to the space of <span class="math inline">\(m \times n\)</span> matrices.</p></li>
<li><p>Independence Check: Prove linear independence of <span class="math inline">\(\{e_i \otimes f_j\}\)</span> by applying a bilinear map <span class="math inline">\(f(e_i,f_j) = \delta_{ii_0}\delta_{jj_0}\)</span>.</p></li>
<li><p>Thought Experiment: How does this dimension formula generalize to <span class="math inline">\(V_1 \otimes V_2 \otimes \cdots \otimes V_k\)</span>?</p></li>
</ol>
</section>
</section>
<section id="appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch" class="level3">
<h3 class="anchored" data-anchor-id="appendix-b.3-decomposition-of-tensors-into-symmetric-and-antisymmetric-parts-proof-sketch">Appendix B.3 Decomposition of Tensors into Symmetric and Antisymmetric Parts (Proof Sketch)</h3>
<p>Any tensor with two indices can be decomposed uniquely into a symmetric and an antisymmetric component. This is a fundamental structural result and often the first place where symmetry in tensors becomes useful.</p>
<section id="statement" class="level4">
<h4 class="anchored" data-anchor-id="statement">Statement</h4>
<p>Let <span class="math inline">\(T \in V \otimes V\)</span> (a bilinear form, or a <span class="math inline">\((0,2)\)</span>-tensor). Then:</p>
<p><span class="math display">\[
T = \tfrac{1}{2}(T + T^\top) \;+\; \tfrac{1}{2}(T - T^\top),
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(S = \tfrac{1}{2}(T + T^\top)\)</span> is symmetric,</li>
<li><span class="math inline">\(A = \tfrac{1}{2}(T - T^\top)\)</span> is antisymmetric, and the decomposition is unique.</li>
</ul>
</section>
<section id="proof-sketch" class="level4">
<h4 class="anchored" data-anchor-id="proof-sketch">Proof Sketch</h4>
<ol type="1">
<li><p>Symmetrization and Antisymmetrization Operators</p>
<ul>
<li><p>Define the symmetrization operator:</p>
<p><span class="math display">\[
\text{Sym}(T)(u,v) = \tfrac{1}{2}[T(u,v) + T(v,u)].
\]</span></p></li>
<li><p>Define the antisymmetrization operator:</p>
<p><span class="math display">\[
\text{Alt}(T)(u,v) = \tfrac{1}{2}[T(u,v) - T(v,u)].
\]</span></p></li>
</ul></li>
<li><p>Linearity and Decomposition</p>
<ul>
<li><p>Both operators are linear.</p></li>
<li><p>For any <span class="math inline">\(u,v\)</span>:</p>
<p><span class="math display">\[
T(u,v) = \text{Sym}(T)(u,v) + \text{Alt}(T)(u,v).
\]</span></p></li>
</ul></li>
<li><p>Uniqueness</p>
<ul>
<li>Suppose <span class="math inline">\(T = S + A\)</span> with <span class="math inline">\(S\)</span> symmetric and <span class="math inline">\(A\)</span> antisymmetric.</li>
<li>Then <span class="math inline">\(S = \text{Sym}(T)\)</span> and <span class="math inline">\(A = \text{Alt}(T)\)</span>.</li>
<li>No other decomposition is possible, proving uniqueness.</li>
</ul></li>
</ol>
</section>
<section id="example-2" class="level4">
<h4 class="anchored" data-anchor-id="example-2">Example</h4>
<p>Matrix form:</p>
<p><span class="math display">\[
T = \begin{bmatrix} 1 &amp; 3 \\ 4 &amp; 2 \end{bmatrix}.
\]</span></p>
<ul>
<li><p>Symmetric part:</p>
<p><span class="math display">\[
S = \tfrac{1}{2}\left(\begin{bmatrix} 1 &amp; 3 \\ 4 &amp; 2 \end{bmatrix} + \begin{bmatrix} 1 &amp; 4 \\ 3 &amp; 2 \end{bmatrix}\right)
= \begin{bmatrix} 1 &amp; 3.5 \\ 3.5 &amp; 2 \end{bmatrix}.
\]</span></p></li>
<li><p>Antisymmetric part:</p>
<p><span class="math display">\[
A = \tfrac{1}{2}\left(\begin{bmatrix} 1 &amp; 3 \\ 4 &amp; 2 \end{bmatrix} - \begin{bmatrix} 1 &amp; 4 \\ 3 &amp; 2 \end{bmatrix}\right)
= \begin{bmatrix} 0 &amp; -0.5 \\ 0.5 &amp; 0 \end{bmatrix}.
\]</span></p></li>
</ul>
<p>So <span class="math inline">\(T = S + A\)</span>.</p>
</section>
<section id="why-this-matters-62" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-62">Why This Matters</h4>
<ul>
<li>Symmetric tensors capture “energy-like” or metric quantities.</li>
<li>Antisymmetric tensors capture orientation, area, and rotational effects.</li>
<li>In physics: stress tensor = symmetric, electromagnetic field tensor = antisymmetric.</li>
</ul>
</section>
<section id="exercises-77" class="level4">
<h4 class="anchored" data-anchor-id="exercises-77">Exercises</h4>
<ol type="1">
<li><p>Compute the Decomposition: Decompose</p>
<p><span class="math display">\[
T = \begin{bmatrix} 0 &amp; 2 \\ -1 &amp; 3 \end{bmatrix}
\]</span></p>
<p>into symmetric and antisymmetric parts.</p></li>
<li><p>Uniqueness Check: Why can’t a nonzero symmetric matrix also be antisymmetric?</p></li>
<li><p>Dimension Argument: Show that in <span class="math inline">\(\mathbb{R}^n\)</span>:</p>
<ul>
<li>Dimension of symmetric 2-tensors = <span class="math inline">\(\tfrac{n(n+1)}{2}\)</span>.</li>
<li>Dimension of antisymmetric 2-tensors = <span class="math inline">\(\tfrac{n(n-1)}{2}\)</span>.</li>
<li>Total adds up to <span class="math inline">\(n^2\)</span>.</li>
</ul></li>
<li><p>Application in Physics: Which parts of the strain tensor (from mechanics) and electromagnetic field tensor (from relativity) correspond to symmetric and antisymmetric parts?</p></li>
<li><p>Thought Experiment: Can you imagine a scenario where the antisymmetric part of a tensor carries more physical information than the symmetric part? ### Appendix B.4 Rank Decomposition for Matrices and Extension to CP Rank (Proof Sketch)</p></li>
</ol>
<p>The concept of rank begins with matrices and extends to higher-order tensors. This appendix sketches the reasoning behind matrix rank decomposition and how it generalizes to the Canonical Polyadic (CP) decomposition for tensors.</p>
</section>
<section id="matrix-rank-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="matrix-rank-decomposition">Matrix Rank Decomposition</h4>
<p>Statement: Any matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> of rank <span class="math inline">\(r\)</span> can be written as a sum of <span class="math inline">\(r\)</span> rank-one matrices.</p>
<p>Formally:</p>
<p><span class="math display">\[
A = \sum_{k=1}^r u^{(k)} (v^{(k)})^\top,
\]</span></p>
<p>with <span class="math inline">\(u^{(k)} \in \mathbb{R}^m\)</span>, <span class="math inline">\(v^{(k)} \in \mathbb{R}^n\)</span>.</p>
<p>Sketch of Proof:</p>
<ol type="1">
<li><p>Column Space Basis:</p>
<ul>
<li>Since <span class="math inline">\(\text{rank}(A) = r\)</span>, there exist <span class="math inline">\(r\)</span> independent columns.</li>
<li>Let <span class="math inline">\(u^{(1)}, \dots, u^{(r)}\)</span> be these basis vectors.</li>
</ul></li>
<li><p>Expansion of Columns:</p>
<ul>
<li>Each column of <span class="math inline">\(A\)</span> is a linear combination of <span class="math inline">\(\{u^{(k)}\}\)</span>.</li>
<li>Thus, <span class="math inline">\(A\)</span> can be written as a sum of outer products between <span class="math inline">\(u^{(k)}\)</span> and suitable coefficient vectors <span class="math inline">\(v^{(k)}\)</span>.</li>
</ul></li>
<li><p>Minimality:</p>
<ul>
<li>No fewer than <span class="math inline">\(r\)</span> terms suffice: otherwise, the column space dimension would drop below <span class="math inline">\(r\)</span>.</li>
</ul></li>
</ol>
<p>This proves the decomposition exists and is minimal.</p>
</section>
<section id="extension-to-tensors-cp-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="extension-to-tensors-cp-decomposition">Extension to Tensors: CP Decomposition</h4>
<p>For a 3rd-order tensor <span class="math inline">\(T \in \mathbb{R}^{I \times J \times K}\)</span>:</p>
<p><span class="math display">\[
T_{ijk} = \sum_{r=1}^R a^{(r)}_i \, b^{(r)}_j \, c^{(r)}_k,
\]</span></p>
<p>or equivalently,</p>
<p><span class="math display">\[
T = \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)}.
\]</span></p>
<ul>
<li><span class="math inline">\(R\)</span> = tensor rank (minimal number of rank-one tensors).</li>
<li>Unlike matrices, computing <span class="math inline">\(R\)</span> is much harder (NP-hard in general).</li>
</ul>
</section>
<section id="key-differences-between-matrix-rank-and-tensor-rank" class="level4">
<h4 class="anchored" data-anchor-id="key-differences-between-matrix-rank-and-tensor-rank">Key Differences Between Matrix Rank and Tensor Rank</h4>
<ul>
<li>Matrices: rank <span class="math inline">\(\leq \min(m,n)\)</span>.</li>
<li>Tensors: rank can exceed dimensions, and uniqueness properties are more subtle.</li>
<li>Matrix SVD: always gives orthogonal decomposition.</li>
<li>Tensor CP: uniqueness requires conditions (e.g., Kruskal’s condition).</li>
</ul>
</section>
<section id="why-this-matters-63" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-63">Why This Matters</h4>
<ul>
<li>Rank decomposition gives the conceptual backbone of low-rank approximation.</li>
<li>CP decomposition underlies applications in data compression, chemometrics, neuroscience, and machine learning.</li>
<li>Shows how a simple linear algebra property grows into a rich multilinear theory.</li>
</ul>
</section>
<section id="exercises-78" class="level4">
<h4 class="anchored" data-anchor-id="exercises-78">Exercises</h4>
<ol type="1">
<li><p>Matrix Rank-One Decomposition: Decompose</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 6 \end{bmatrix}
\]</span></p>
<p>into rank-one matrices.</p></li>
<li><p>Minimality Check: Why can’t the above <span class="math inline">\(A\)</span> be written as a single rank-one matrix?</p></li>
<li><p>Tensor Example: Express the tensor <span class="math inline">\(T_{ijk} = \delta_{ij}\delta_{jk}\)</span> (the identity cube) as a CP decomposition.</p></li>
<li><p>Rank Bound: Show that for <span class="math inline">\(T \in \mathbb{R}^{I \times J \times K}\)</span>, the rank is at most <span class="math inline">\(\min(IJ, JK, IK)\)</span>.</p></li>
<li><p>Thought Experiment: Why might tensor rank decomposition be <em>harder- but also </em>more useful- than matrix rank decomposition in data science? ### Appendix B.5 Identifiability Conditions for CP Decomposition (Proof Sketch)</p></li>
</ol>
<p>One of the most important questions in tensor decomposition is uniqueness: when is a CP (Canonical Polyadic) decomposition determined uniquely (up to permutation and scaling)? This property is called identifiability.</p>
</section>
<section id="statement-informal" class="level4">
<h4 class="anchored" data-anchor-id="statement-informal">Statement (Informal)</h4>
<p>A CP decomposition</p>
<p><span class="math display">\[
T = \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)}
\]</span></p>
<p>is essentially unique (unique up to permutation and scaling of terms) if the factor matrices <span class="math inline">\([a^{(1)} \ \dots \ a^{(R)}]\)</span>, <span class="math inline">\([b^{(1)} \ \dots \ b^{(R)}]\)</span>, <span class="math inline">\([c^{(1)} \ \dots \ c^{(R)}]\)</span> satisfy certain rank conditions.</p>
</section>
<section id="kruskals-theorem-key-result" class="level4">
<h4 class="anchored" data-anchor-id="kruskals-theorem-key-result">Kruskal’s Theorem (Key Result)</h4>
<p>Let <span class="math inline">\(A \in \mathbb{R}^{I \times R}, B \in \mathbb{R}^{J \times R}, C \in \mathbb{R}^{K \times R}\)</span>. Define the Kruskal rank of a matrix <span class="math inline">\(M\)</span>, written <span class="math inline">\(k_M\)</span>, as the maximum number such that every subset of <span class="math inline">\(k_M\)</span> columns is linearly independent.</p>
<p>Kruskal’s condition: If</p>
<p><span class="math display">\[
k_A + k_B + k_C \geq 2R + 2,
\]</span></p>
<p>then the CP decomposition is unique (up to trivial indeterminacies).</p>
</section>
<section id="sketch-of-why-this-works" class="level4">
<h4 class="anchored" data-anchor-id="sketch-of-why-this-works">Sketch of Why This Works</h4>
<ol type="1">
<li><p>Overcompleteness and Mixing:</p>
<ul>
<li>Without conditions, many different sets of factors can represent the same tensor.</li>
</ul></li>
<li><p>Column Independence:</p>
<ul>
<li>Kruskal rank ensures that subsets of columns are independent, ruling out hidden degeneracies.</li>
</ul></li>
<li><p>Balancing the Inequality:</p>
<ul>
<li>The inequality ensures enough “spread” across modes so that factors can’t be swapped or recombined into different decompositions.</li>
</ul></li>
<li><p>Result:</p>
<ul>
<li>Any alternative decomposition must match the original one (up to permutation and scaling).</li>
</ul></li>
</ol>
</section>
<section id="intuition" class="level4">
<h4 class="anchored" data-anchor-id="intuition">Intuition</h4>
<ul>
<li>For matrices, SVD gives uniqueness (up to rotations) because of orthogonality.</li>
<li>For tensors, uniqueness is subtler: CP decomposition is often more unique than matrix decompositions.</li>
<li>This uniqueness makes tensors powerful in applications like blind source separation and latent-variable modeling.</li>
</ul>
</section>
<section id="why-this-matters-64" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-64">Why This Matters</h4>
<ul>
<li>Guarantees that discovered latent factors have meaning.</li>
<li>Critical in chemometrics, neuroscience, psychometrics, and machine learning.</li>
<li>Explains why tensor methods succeed where matrix methods fail: uniqueness despite underdetermined settings.</li>
</ul>
</section>
<section id="exercises-79" class="level4">
<h4 class="anchored" data-anchor-id="exercises-79">Exercises</h4>
<ol type="1">
<li><p>Kruskal Rank: Compute the Kruskal rank of</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
\]</span></p></li>
<li><p>Uniqueness Check: Why is CP decomposition with <span class="math inline">\(R=1\)</span> always unique?</p></li>
<li><p>Matrix vs Tensor: Compare uniqueness of matrix rank decomposition with tensor CP decomposition. Which one is stricter?</p></li>
<li><p>Practical Implication: In blind source separation, why is uniqueness of CP decomposition essential?</p></li>
<li><p>Thought Experiment: Why might identifiability be a <em>blessing- in data analysis but a </em>curse- for numerical algorithms?</p></li>
</ol>
</section>
</section>
</section>
<section id="appendix-d.-identities-cookbook" class="level2">
<h2 class="anchored" data-anchor-id="appendix-d.-identities-cookbook">Appendix D. Identities &amp; “Cookbook”</h2>
<p>This appendix collects the most frequently used identities in multilinear algebra, matrix calculus, and tensor manipulation. They are written in a quick-lookup style - proofs and derivations appear in the main chapters.</p>
<section id="kronecker-product-vec-identities" class="level3">
<h3 class="anchored" data-anchor-id="kronecker-product-vec-identities">1. Kronecker Product &amp; Vec Identities</h3>
<ul>
<li><p>Vectorization of matrix products:</p>
<p><span class="math display">\[
\mathrm{vec}(AXB) = (B^\top \otimes A)\, \mathrm{vec}(X).
\]</span></p></li>
<li><p>Vec of outer product:</p>
<p><span class="math display">\[
\mathrm{vec}(uv^\top) = v \otimes u.
\]</span></p></li>
<li><p>Mixed product property:</p>
<p><span class="math display">\[
(A \otimes B)(C \otimes D) = (AC) \otimes (BD),
\]</span></p>
<p>if dimensions match.</p></li>
</ul>
</section>
<section id="trace-tricks" class="level3">
<h3 class="anchored" data-anchor-id="trace-tricks">2. Trace Tricks</h3>
<ul>
<li><p>Cyclic property:</p>
<p><span class="math display">\[
\mathrm{tr}(AB) = \mathrm{tr}(BA).
\]</span></p></li>
<li><p>Frobenius inner product:</p>
<p><span class="math display">\[
\langle A,B \rangle = \mathrm{tr}(A^\top B).
\]</span></p></li>
<li><p>Trace with Kronecker:</p>
<p><span class="math display">\[
\mathrm{tr}(A \otimes B) = \mathrm{tr}(A)\, \mathrm{tr}(B).
\]</span></p></li>
</ul>
</section>
<section id="tensor-contractions" class="level3">
<h3 class="anchored" data-anchor-id="tensor-contractions">3. Tensor Contractions</h3>
<ul>
<li><p>Inner product of tensors:</p>
<p><span class="math display">\[
\langle T, U \rangle = \sum_{i_1,\dots,i_k} T_{i_1\dots i_k} U_{i_1\dots i_k}.
\]</span></p></li>
<li><p>Contraction with <span class="math inline">\(\delta_{ij}\)</span>:</p>
<p><span class="math display">\[
A_{ij}\,\delta_{ij} = \mathrm{tr}(A).
\]</span></p></li>
<li><p>Contraction with Levi-Civita (<span class="math inline">\(\varepsilon_{ijk}\)</span>):</p>
<p><span class="math display">\[
\varepsilon_{ijk}\, u_j v_k = (u \times v)_i.
\]</span></p></li>
</ul>
</section>
<section id="determinant-volumes" class="level3">
<h3 class="anchored" data-anchor-id="determinant-volumes">4. Determinant &amp; Volumes</h3>
<ul>
<li><p>Determinant via Levi-Civita:</p>
<p><span class="math display">\[
\det(A) = \sum_{i_1,\dots,i_n} \varepsilon_{i_1 \dots i_n}
A_{1,i_1} A_{2,i_2} \cdots A_{n,i_n}.
\]</span></p></li>
<li><p>Volume of parallelepiped (vectors <span class="math inline">\(v_1,\dots,v_n\)</span>):</p>
<p><span class="math display">\[
\text{Vol} = |\det([v_1 \ \dots \ v_n])|.
\]</span></p></li>
</ul>
</section>
<section id="differential-gradient-identities" class="level3">
<h3 class="anchored" data-anchor-id="differential-gradient-identities">5. Differential &amp; Gradient Identities</h3>
<ul>
<li><p>Gradient of quadratic form:</p>
<p><span class="math display">\[
\nabla_x (x^\top A x) = (A + A^\top)x.
\]</span></p></li>
<li><p>Matrix calculus rule:</p>
<p><span class="math display">\[
\nabla_X \,\mathrm{tr}(A^\top X) = A.
\]</span></p></li>
<li><p>Log-det derivative:</p>
<p><span class="math display">\[
\nabla_X \log \det(X) = (X^{-1})^\top.
\]</span></p></li>
</ul>
</section>
<section id="useful-einsum-patterns" class="level3">
<h3 class="anchored" data-anchor-id="useful-einsum-patterns">6. Useful Einsum Patterns</h3>
<ul>
<li><p>Matrix multiplication:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.einsum(<span class="st">'ik,kj-&gt;ij'</span>, A, B)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Tensor contraction:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.einsum(<span class="st">'ijk,k-&gt;ij'</span>, T, x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Inner product:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> np.einsum(<span class="st">'i,i-&gt;'</span>, u, v)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="symmetrization-antisymmetrization" class="level3">
<h3 class="anchored" data-anchor-id="symmetrization-antisymmetrization">7. Symmetrization / Antisymmetrization</h3>
<ul>
<li><p>Symmetrization of 2-tensor:</p>
<p><span class="math display">\[
S_{ij} = \tfrac{1}{2}(T_{ij} + T_{ji}).
\]</span></p></li>
<li><p>Antisymmetrization:</p>
<p><span class="math display">\[
A_{ij} = \tfrac{1}{2}(T_{ij} - T_{ji}).
\]</span></p></li>
<li><p>General <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\text{Sym}(T) = \frac{1}{k!} \sum_{\pi \in S_k} T_{i_{\pi(1)} \dots i_{\pi(k)}}.
\]</span></p></li>
</ul>
</section>
</section>
<section id="appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e.1-mini-project-implement-cp-decomposition-for-small-3-way-tensors">Appendix E.1 Mini-Project: Implement CP Decomposition for Small 3-Way Tensors</h2>
<p>Goal: Learn how to compute a Canonical Polyadic (CP) decomposition of a small 3-way tensor by implementing an algorithm step-by-step. This project combines theory (tensor rank, factorization) with practice (numerical methods).</p>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">1. Background</h3>
<ul>
<li><p>Recall: a 3rd-order tensor <span class="math inline">\(T \in \mathbb{R}^{I \times J \times K}\)</span> has CP form</p>
<p><span class="math display">\[
T \approx \sum_{r=1}^R a^{(r)} \otimes b^{(r)} \otimes c^{(r)},
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is the target rank and <span class="math inline">\(a^{(r)} \in \mathbb{R}^I, b^{(r)} \in \mathbb{R}^J, c^{(r)} \in \mathbb{R}^K\)</span>.</p></li>
<li><p>CP decomposition generalizes SVD to higher-order tensors.</p></li>
<li><p>Unlike SVD, exact decomposition is not guaranteed for arbitrary tensors, so we usually solve an optimization problem.</p></li>
</ul>
</section>
<section id="implementation-plan" class="level3">
<h3 class="anchored" data-anchor-id="implementation-plan">2. Implementation Plan</h3>
<p>Step 1: Generate a Synthetic Tensor</p>
<ul>
<li><p>Choose dimensions, e.g., <span class="math inline">\(I=J=K=4\)</span>.</p></li>
<li><p>Pick random factor matrices <span class="math inline">\(A \in \mathbb{R}^{I \times R}, B \in \mathbb{R}^{J \times R}, C \in \mathbb{R}^{K \times R}\)</span>.</p></li>
<li><p>Form a tensor:</p>
<p><span class="math display">\[
T_{ijk} = \sum_{r=1}^R A_{ir} B_{jr} C_{kr}.
\]</span></p></li>
</ul>
<p>Step 2: Alternating Least Squares (ALS) Algorithm</p>
<ul>
<li><p>Initialize random factor matrices <span class="math inline">\(\hat{A}, \hat{B}, \hat{C}\)</span>.</p></li>
<li><p>Repeat until convergence:</p>
<ol type="1">
<li>Fix <span class="math inline">\(\hat{B}, \hat{C}\)</span>, update <span class="math inline">\(\hat{A}\)</span> by solving least squares.</li>
<li>Fix <span class="math inline">\(\hat{A}, \hat{C}\)</span>, update <span class="math inline">\(\hat{B}\)</span>.</li>
<li>Fix <span class="math inline">\(\hat{A}, \hat{B}\)</span>, update <span class="math inline">\(\hat{C}\)</span>.</li>
</ol></li>
</ul>
<p>Step 3: Evaluate Error</p>
<ul>
<li><p>Reconstruction error:</p>
<p><span class="math display">\[
\text{Error} = \frac{\|T - \hat{T}\|_F}{\|T\|_F}.
\]</span></p></li>
<li><p>Stop when error falls below threshold (e.g., <span class="math inline">\(10^{-6}\)</span>) or max iterations reached.</p></li>
</ul>
</section>
<section id="coding-hints" class="level3">
<h3 class="anchored" data-anchor-id="coding-hints">3. Coding Hints</h3>
<ul>
<li>Use NumPy (or PyTorch/JAX) for efficient tensor operations.</li>
<li>Reshape and unfold tensors along modes for least squares updates.</li>
<li>Use <code>numpy.linalg.lstsq</code> to solve linear systems.</li>
<li>Normalize columns of factors periodically to avoid numerical instability.</li>
</ul>
</section>
<section id="extensions-optional" class="level3">
<h3 class="anchored" data-anchor-id="extensions-optional">4. Extensions (Optional)</h3>
<ul>
<li>Compare convergence for different ranks <span class="math inline">\(R\)</span>.</li>
<li>Add Gaussian noise to the tensor and see how well CP decomposition recovers the factors.</li>
<li>Test uniqueness: permute/scaled versions of factors should still reconstruct the tensor.</li>
<li>Visualize factor matrices as heatmaps.</li>
</ul>
</section>
<section id="deliverables" class="level3">
<h3 class="anchored" data-anchor-id="deliverables">5. Deliverables</h3>
<ul>
<li><p>Working implementation of CP-ALS for 3-way tensors.</p></li>
<li><p>Plots of error vs.&nbsp;iterations.</p></li>
<li><p>Short report (1–2 pages) discussing:</p>
<ul>
<li>Accuracy of decomposition.</li>
<li>Effect of rank choice.</li>
<li>Challenges in convergence.</li>
</ul></li>
</ul>
<p>By finishing this project, you’ll understand CP decomposition at both the conceptual (tensor rank, multilinearity) and practical (numerical algorithms, implementation) levels.</p>
</section>
</section>
<section id="appendix-e.2-mini-project-tucker-decomposition-for-video-compression" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e.2-mini-project-tucker-decomposition-for-video-compression">Appendix E.2 Mini-Project: Tucker Decomposition for Video Compression</h2>
<p>Goal: Explore how Tucker decomposition can compress multi-way data efficiently, using a small grayscale video (or synthetic 3D tensor).</p>
<section id="background-1" class="level3">
<h3 class="anchored" data-anchor-id="background-1">1. Background</h3>
<ul>
<li><p>A video clip with <span class="math inline">\(F\)</span> frames, each of size <span class="math inline">\(H \times W\)</span>, is naturally represented as a 3-way tensor:</p>
<p><span class="math display">\[
X \in \mathbb{R}^{F \times H \times W}.
\]</span></p></li>
<li><p>Tucker decomposition:</p>
<p><span class="math display">\[
X \approx G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)},
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(G\)</span> is a smaller core tensor,</li>
<li><span class="math inline">\(U^{(1)}, U^{(2)}, U^{(3)}\)</span> are factor matrices (orthogonal bases along each mode).</li>
</ul></li>
<li><p>This is a higher-order analogue of SVD (HOSVD).</p></li>
</ul>
</section>
<section id="implementation-plan-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-plan-1">2. Implementation Plan</h3>
<p>Step 1: Data Preparation</p>
<ul>
<li>Option A: Load a small grayscale video (e.g., <span class="math inline">\(30 \times 64 \times 64\)</span>: 30 frames, 64×64 pixels).</li>
<li>Option B: Generate synthetic video data (moving shapes, noise).</li>
</ul>
<p>Step 2: Compute Tucker Decomposition (HOSVD)</p>
<ol type="1">
<li><p>Unfold <span class="math inline">\(X\)</span> along each mode (frame, height, width).</p></li>
<li><p>Compute SVD of each unfolding.</p></li>
<li><p>Select leading <span class="math inline">\(r_1, r_2, r_3\)</span> singular vectors for factor matrices <span class="math inline">\(U^{(1)}, U^{(2)}, U^{(3)}\)</span>.</p></li>
<li><p>Form core tensor:</p>
<p><span class="math display">\[
G = X \times_1 (U^{(1)})^\top \times_2 (U^{(2)})^\top \times_3 (U^{(3)})^\top.
\]</span></p></li>
</ol>
<p>Step 3: Reconstruct Compressed Video</p>
<ul>
<li><p>Approximation:</p>
<p><span class="math display">\[
\hat{X} = G \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}.
\]</span></p></li>
<li><p>Compare reconstruction with original.</p></li>
</ul>
<p>Step 4: Evaluate Compression</p>
<ul>
<li><p>Compression ratio:</p>
<p><span class="math display">\[
\text{CR} = \frac{\text{entries in original tensor}}{\text{entries in core + factors}}.
\]</span></p></li>
<li><p>Reconstruction error:</p>
<p><span class="math display">\[
\text{Error} = \frac{\|X - \hat{X}\|_F}{\|X\|_F}.
\]</span></p></li>
</ul>
</section>
<section id="coding-hints-1" class="level3">
<h3 class="anchored" data-anchor-id="coding-hints-1">3. Coding Hints</h3>
<ul>
<li>Use NumPy or Tensorly (<code>tensorly.decomposition.tucker</code>) to avoid low-level SVD coding.</li>
<li>Visualize reconstruction error by displaying frames before and after compression.</li>
<li>Try varying rank choices <span class="math inline">\((r_1,r_2,r_3)\)</span>.</li>
</ul>
</section>
<section id="extensions-optional-1" class="level3">
<h3 class="anchored" data-anchor-id="extensions-optional-1">4. Extensions (Optional)</h3>
<ul>
<li>Compare Tucker compression with simple PCA on flattened frames.</li>
<li>Add noise to video and check whether Tucker decomposition captures underlying structure better than PCA.</li>
<li>Explore color video: treat as a 4-way tensor <span class="math inline">\(F \times H \times W \times 3\)</span>.</li>
<li>Implement randomized SVD for scalability.</li>
</ul>
</section>
<section id="deliverables-1" class="level3">
<h3 class="anchored" data-anchor-id="deliverables-1">5. Deliverables</h3>
<ul>
<li><p>Code that performs Tucker decomposition on a small video dataset.</p></li>
<li><p>Plots:</p>
<ul>
<li>Error vs.&nbsp;compression ratio.</li>
<li>Example frame (original vs.&nbsp;compressed reconstruction).</li>
</ul></li>
<li><p>Short writeup explaining:</p>
<ul>
<li>How compression works.</li>
<li>Trade-off between rank choice and accuracy.</li>
</ul></li>
</ul>
<p>By completing this project, you’ll see how tensor decompositions reduce storage and preserve structure in real data, and why multilinear methods are superior to naive flattening.</p>
</section>
</section>
<section id="appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e.3-mini-project-strain-tensor-for-a-rotating-plate">Appendix E.3 Mini-Project: Strain Tensor for a Rotating Plate</h2>
<p>Goal: Understand how the strain tensor captures deformation by computing it for a simple rotating square plate. This bridges mechanics (stress/strain) with tensor calculus.</p>
<section id="background-2" class="level3">
<h3 class="anchored" data-anchor-id="background-2">1. Background</h3>
<ul>
<li><p>In continuum mechanics, the strain tensor measures local deformation of a material:</p>
<p><span class="math display">\[
\varepsilon_{ij} = \tfrac{1}{2}\left(\frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i}\right),
\]</span></p>
<p>where <span class="math inline">\(u(x)\)</span> is the displacement field.</p></li>
<li><p>Pure rotation should produce no strain (only rigid-body motion).</p></li>
<li><p>This project demonstrates that principle.</p></li>
</ul>
</section>
<section id="setup-rotating-plate" class="level3">
<h3 class="anchored" data-anchor-id="setup-rotating-plate">2. Setup: Rotating Plate</h3>
<ul>
<li><p>Consider a 2D square plate centered at the origin.</p></li>
<li><p>Apply a small rotation by angle <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Displacement of a point <span class="math inline">\((x,y)\)</span>:</p>
<p><span class="math display">\[
u(x,y) = R(\theta)\begin{bmatrix} x \\ y \end{bmatrix} - \begin{bmatrix} x \\ y \end{bmatrix},
\]</span></p>
<p>where <span class="math inline">\(R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span>.</p></li>
</ul>
</section>
<section id="compute-strain-tensor" class="level3">
<h3 class="anchored" data-anchor-id="compute-strain-tensor">3. Compute Strain Tensor</h3>
<ol type="1">
<li><p>Expand <span class="math inline">\(u(x,y)\)</span>:</p>
<p><span class="math display">\[
u_1(x,y) = (\cos\theta - 1)x - (\sin\theta)y,
\]</span></p>
<p><span class="math display">\[
u_2(x,y) = (\sin\theta)x + (\cos\theta - 1)y.
\]</span></p></li>
<li><p>Compute partial derivatives:</p>
<ul>
<li><span class="math inline">\(\frac{\partial u_1}{\partial x} = \cos\theta - 1,\)</span></li>
<li><span class="math inline">\(\frac{\partial u_1}{\partial y} = -\sin\theta,\)</span></li>
<li><span class="math inline">\(\frac{\partial u_2}{\partial x} = \sin\theta,\)</span></li>
<li><span class="math inline">\(\frac{\partial u_2}{\partial y} = \cos\theta - 1.\)</span></li>
</ul></li>
<li><p>Build strain tensor:</p>
<p><span class="math display">\[
\varepsilon = \tfrac{1}{2}\begin{bmatrix}
2(\cos\theta - 1) &amp; (-\sin\theta + \sin\theta) \\
(\sin\theta - \sin\theta) &amp; 2(\cos\theta - 1)
\end{bmatrix}.
\]</span></p></li>
<li><p>Simplify:</p>
<p><span class="math display">\[
\varepsilon = (\cos\theta - 1) I,
\]</span></p>
<p>which vanishes when <span class="math inline">\(\theta\)</span> is small (pure rotation ≈ no strain).</p></li>
</ol>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">4. Interpretation</h3>
<ul>
<li>For infinitesimal <span class="math inline">\(\theta\)</span>, expand <span class="math inline">\(\cos\theta \approx 1 - \tfrac{1}{2}\theta^2\)</span>.</li>
<li>So strain <span class="math inline">\(\varepsilon \sim -\tfrac{1}{2}\theta^2 I\)</span>, i.e., negligible for small angles.</li>
<li>Confirms physical intuition: rigid rotations produce no first-order strain.</li>
</ul>
</section>
<section id="coding-hints-2" class="level3">
<h3 class="anchored" data-anchor-id="coding-hints-2">5. Coding Hints</h3>
<ul>
<li>Implement in Python/NumPy: define displacement field, compute gradients symbolically (SymPy) or numerically (finite differences).</li>
<li>Visualize deformation arrows for a square grid of points.</li>
<li>Plot strain tensor components as heatmaps.</li>
</ul>
</section>
<section id="extensions-optional-2" class="level3">
<h3 class="anchored" data-anchor-id="extensions-optional-2">6. Extensions (Optional)</h3>
<ul>
<li>Apply non-uniform deformation (e.g., shear or stretching) and compute strain.</li>
<li>Compare symmetric (strain) vs.&nbsp;antisymmetric (rotation) parts of displacement gradient.</li>
<li>Extend to 3D cube rotation.</li>
</ul>
</section>
<section id="deliverables-2" class="level3">
<h3 class="anchored" data-anchor-id="deliverables-2">7. Deliverables</h3>
<ul>
<li>Code computing strain tensor for rotating plate.</li>
<li>Visualization of deformation vs.&nbsp;strain (showing nearly zero strain for pure rotation).</li>
<li>Short explanation connecting math to physical interpretation.</li>
</ul>
<p>This project illustrates how the strain tensor isolates real deformation, distinguishing it from rigid-body motion.</p>
</section>
</section>
<section id="appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e.4-mini-project-parallel-transport-of-a-vector-on-a-sphere">Appendix E.4 Mini-Project: Parallel Transport of a Vector on a Sphere</h2>
<p>Goal: Visualize and compute parallel transport of a vector along a closed path on a sphere, showing how curvature affects vector orientation.</p>
<section id="background-3" class="level3">
<h3 class="anchored" data-anchor-id="background-3">1. Background</h3>
<ul>
<li>On curved manifolds, moving a vector along a path while keeping it “as constant as possible” is called parallel transport.</li>
<li>On a flat plane: parallel transport around any loop returns the vector unchanged.</li>
<li>On a sphere: transporting a vector around a loop can change its orientation, revealing curvature.</li>
</ul>
</section>
<section id="setup-the-sphere-s2" class="level3">
<h3 class="anchored" data-anchor-id="setup-the-sphere-s2">2. Setup: The Sphere <span class="math inline">\(S^2\)</span></h3>
<ul>
<li>Sphere of radius <span class="math inline">\(R=1\)</span>, embedded in <span class="math inline">\(\mathbb{R}^3\)</span>.</li>
<li>Tangent space at a point <span class="math inline">\(p \in S^2\)</span>: plane orthogonal to <span class="math inline">\(p\)</span>.</li>
<li>Path: for simplicity, use a triangle on the sphere (e.g., along the equator and a meridian).</li>
</ul>
</section>
<section id="parallel-transport-along-geodesics" class="level3">
<h3 class="anchored" data-anchor-id="parallel-transport-along-geodesics">3. Parallel Transport along Geodesics</h3>
<ol type="1">
<li><p>Choose Starting Point:</p>
<ul>
<li><span class="math inline">\(p_0 = (0,0,1)\)</span> (north pole).</li>
<li>Tangent vector: <span class="math inline">\(v_0 = (1,0,0)\)</span>.</li>
</ul></li>
<li><p>Transport Path:</p>
<ul>
<li>Move vector along a geodesic (great circle).</li>
<li>Keep it tangent at each point.</li>
<li>Algorithmically: project derivative back into tangent space.</li>
</ul></li>
<li><p>Closed Loop Example:</p>
<ul>
<li>Move from north pole down to equator (longitude 0).</li>
<li>Travel along equator by 90°.</li>
<li>Return to north pole.</li>
</ul></li>
<li><p>Result:</p>
<ul>
<li>Vector rotates relative to original orientation.</li>
<li>Rotation angle equals the spherical excess of the triangle (area on sphere).</li>
</ul></li>
</ol>
</section>
<section id="coding-hints-3" class="level3">
<h3 class="anchored" data-anchor-id="coding-hints-3">4. Coding Hints</h3>
<ul>
<li><p>Use Python + NumPy/Matplotlib.</p></li>
<li><p>Represent path as discrete points on sphere.</p></li>
<li><p>At each step:</p>
<ul>
<li><p>Move point along geodesic.</p></li>
<li><p>Update vector by projecting back into tangent plane:</p>
<p><span class="math display">\[
v \gets v - (v \cdot p)p, \quad \text{then normalize}.
\]</span></p></li>
</ul></li>
<li><p>Visualize trajectory of vector arrows along sphere using 3D plotting (<code>matplotlib.pyplot.quiver</code>).</p></li>
</ul>
</section>
<section id="extensions-optional-3" class="level3">
<h3 class="anchored" data-anchor-id="extensions-optional-3">5. Extensions (Optional)</h3>
<ul>
<li>Experiment with different spherical triangles.</li>
<li>Compute holonomy angle = enclosed area × curvature (for unit sphere, curvature = 1).</li>
<li>Compare parallel transport on sphere vs.&nbsp;flat plane (no change).</li>
<li>Extend to numerical geodesics on other manifolds (torus, hyperbolic surface).</li>
</ul>
</section>
<section id="deliverables-3" class="level3">
<h3 class="anchored" data-anchor-id="deliverables-3">6. Deliverables</h3>
<ul>
<li>Code that simulates parallel transport on a sphere.</li>
<li>3D visualization of vector transport around loop.</li>
<li>Measurement of net rotation angle (compare with theory).</li>
<li>Short reflection: “What does this reveal about curvature?”</li>
</ul>
<p>This project gives tangible insight into how curvature manifests in parallel transport, making an abstract tensorial concept geometrically vivid.</p>
</section>
</section>
<section id="appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data" class="level2">
<h2 class="anchored" data-anchor-id="appendix-e.5-mini-project-pca-vs.-tucker-decomposition-on-real-data">Appendix E.5 Mini-Project: PCA vs.&nbsp;Tucker Decomposition on Real Data</h2>
<p>Goal: Compare Principal Component Analysis (PCA), which is matrix-based, with Tucker decomposition, which is tensor-based, on a real dataset. This project shows why multilinear approaches capture more structure than flattening data.</p>
<section id="background-4" class="level3">
<h3 class="anchored" data-anchor-id="background-4">1. Background</h3>
<ul>
<li>PCA: finds low-rank approximations of matrices (e.g., flatten images or videos into 2D).</li>
<li>Tucker decomposition: generalizes PCA to tensors, preserving multi-way structure.</li>
<li>Key question: Does Tucker give a better representation than PCA when applied to data with natural multi-dimensional structure (e.g., images, videos, EEG signals)?</li>
</ul>
</section>
<section id="dataset-options" class="level3">
<h3 class="anchored" data-anchor-id="dataset-options">2. Dataset Options</h3>
<ul>
<li>Images: MNIST digits (28×28 grayscale images).</li>
<li>Video: small grayscale clip (frames × height × width).</li>
<li>Multichannel signals: EEG (time × channel × trial).</li>
</ul>
</section>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">3. Methodology</h3>
<p>Step 1: Prepare Data</p>
<ul>
<li>For PCA: flatten each sample into a long vector.</li>
<li>For Tucker: keep data as tensor.</li>
</ul>
<p>Step 2: Apply PCA</p>
<ul>
<li>Use <code>scikit-learn</code> PCA.</li>
<li>Choose top <span class="math inline">\(k\)</span> components.</li>
<li>Reconstruct approximations of data.</li>
</ul>
<p>Step 3: Apply Tucker Decomposition</p>
<ul>
<li>Use <code>tensorly.decomposition.tucker</code>.</li>
<li>Choose rank tuple <span class="math inline">\((r_1, r_2, r_3)\)</span>.</li>
<li>Reconstruct approximations.</li>
</ul>
<p>Step 4: Compare Results</p>
<ul>
<li><p>Compute reconstruction error:</p>
<p><span class="math display">\[
\text{Error} = \frac{\|X - \hat{X}\|_F}{\|X\|_F}.
\]</span></p></li>
<li><p>Compare compression ratio (# of parameters stored).</p></li>
<li><p>Visualize original vs reconstructed samples.</p></li>
</ul>
</section>
<section id="coding-hints-4" class="level3">
<h3 class="anchored" data-anchor-id="coding-hints-4">4. Coding Hints</h3>
<ul>
<li>Libraries: <code>numpy</code>, <code>scikit-learn</code>, <code>tensorly</code>.</li>
<li>To compare fairly: match number of parameters used by PCA and Tucker.</li>
<li>For MNIST: display digits reconstructed with 5, 10, 20 components.</li>
<li>For Tucker: vary ranks (e.g., (10,10) vs (5,10,5)).</li>
</ul>
</section>
<section id="extensions-optional-4" class="level3">
<h3 class="anchored" data-anchor-id="extensions-optional-4">5. Extensions (Optional)</h3>
<ul>
<li>Add Gaussian noise to data and test denoising power of PCA vs Tucker.</li>
<li>Compare runtime and scalability.</li>
<li>Try CP decomposition instead of Tucker.</li>
<li>Explore color images as 3-way tensors (height × width × channels).</li>
</ul>
</section>
<section id="deliverables-4" class="level3">
<h3 class="anchored" data-anchor-id="deliverables-4">6. Deliverables</h3>
<ul>
<li><p>Code implementing both PCA and Tucker on chosen dataset.</p></li>
<li><p>Plots:</p>
<ul>
<li>Error vs.&nbsp;compression ratio.</li>
<li>Side-by-side reconstruction images (original, PCA, Tucker).</li>
</ul></li>
<li><p>Short writeup:</p>
<ul>
<li>Which method captures structure better?</li>
<li>Does Tucker handle correlations across modes more effectively?</li>
</ul></li>
</ul>
<p>This project highlights the advantage of tensor methods over flattening approaches, making the case for multilinear models in data science.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>